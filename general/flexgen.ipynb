{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-10-07 11:12:51,539 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpr6rg6mgv\n",
      "2023-10-07 11:12:51,541 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpr6rg6mgv/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from flexgen_utils import logging, Policy\n",
    "from flexgen_init import policy_init\n",
    "from flexgen_test import test_hf_gen \n",
    "from flexgen_forward import flexgen\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 11:12:51,963 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2023-10-07 11:12:52,037 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-07 11:12:52.681132: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-07 11:12:53,650 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
      "2023-10-07 11:12:53,817 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5\n",
      "2023-10-07 11:12:53,818 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7\n",
      "2023-10-07 11:12:53,819 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5\n",
      "2023-10-07 11:12:53,820 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7\n",
      "2023-10-07 11:12:54,882 [flexgen_init.py:40 in policy_init] DEBUG - Got empty CausalLM: 'facebook/opt-125m' on meta device.\n",
      "2023-10-07 11:12:54,895 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400\n",
      "2023-10-07 11:12:54,896 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000\n",
      "2023-10-07 11:12:54,898 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464\n",
      "2023-10-07 11:12:54,899 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592\n",
      "2023-10-07 11:12:54,901 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720\n",
      "2023-10-07 11:12:54,903 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848\n",
      "2023-10-07 11:12:54,905 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976\n",
      "2023-10-07 11:12:54,907 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104\n",
      "2023-10-07 11:12:54,908 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232\n",
      "2023-10-07 11:12:54,910 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360\n",
      "2023-10-07 11:12:54,912 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488\n",
      "2023-10-07 11:12:54,914 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616\n",
      "2023-10-07 11:12:54,916 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744\n",
      "2023-10-07 11:12:54,918 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872\n",
      "2023-10-07 11:12:54,920 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-10-07 11:12:54,921 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-10-07 11:12:54,922 [flexgen_init.py:219 in get_policy_weight_map] INFO - device_map is prepared!\n",
      "2023-10-07 11:12:54,925 [flexgen_init.py:225 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: \n",
      "GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)\n",
      "2023-10-07 11:12:54,967 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-07 11:12:55,102 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-07 11:12:55,239 [flexgen_init.py:72 in policy_init] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'\n",
      "model init: loading by policy...:   0%|          | 0/197 [00:00<?, ?it/s]/home/fsuser/FlexGen/general/flexgen_utils/offload.py:41: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tmp = torch.from_numpy(np_memmap).to(device)\n",
      "model init: loading by policy...: 100%|██████████| 197/197 [00:00<00:00, 2527.14it/s]\n",
      "2023-10-07 11:12:55,324 [flexgen_init.py:84 in policy_init] INFO - model has been loaded by policy.\n",
      "2023-10-07 11:12:55,325 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward\n",
      "2023-10-07 11:12:55,327 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward\n",
      "2023-10-07 11:12:55,328 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward\n",
      "2023-10-07 11:12:55,329 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward\n",
      "2023-10-07 11:12:55,330 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward\n",
      "2023-10-07 11:12:55,331 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward\n",
      "2023-10-07 11:12:55,332 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward\n",
      "2023-10-07 11:12:55,333 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward\n",
      "2023-10-07 11:12:55,334 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward\n",
      "2023-10-07 11:12:55,335 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward\n",
      "2023-10-07 11:12:55,336 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward\n",
      "2023-10-07 11:12:55,336 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward\n",
      "2023-10-07 11:12:55,337 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward\n",
      "2023-10-07 11:12:55,338 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward\n",
      "2023-10-07 11:12:55,341 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward\n",
      "2023-10-07 11:12:55,341 [flexgen_forward.py:38 in to_test_forward] DEBUG - lm_head to test forward\n",
      "2023-10-07 11:12:55,383 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 2, but `max_length` is set to 2. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1535: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "2023-10-07 11:12:55,588 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:12:55,590 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.embed_tokens forward pass:\n",
      "2023-10-07 11:12:55,591 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,593 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:55,594 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.embed_positions forward pass:\n",
      "2023-10-07 11:12:55,596 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,598 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:55,606 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.0 forward pass:\n",
      "2023-10-07 11:12:55,619 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,622 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:55,630 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.1 forward pass:\n",
      "2023-10-07 11:12:55,636 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,639 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:55,647 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.2 forward pass:\n",
      "2023-10-07 11:12:55,655 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,658 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:55,667 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.3 forward pass:\n",
      "2023-10-07 11:12:55,673 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,676 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:55,684 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.4 forward pass:\n",
      "2023-10-07 11:12:55,691 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,693 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:55,701 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.5 forward pass:\n",
      "2023-10-07 11:12:55,707 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,710 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:55,718 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.6 forward pass:\n",
      "2023-10-07 11:12:55,723 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,727 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:55,739 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.7 forward pass:\n",
      "2023-10-07 11:12:55,748 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,752 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:55,764 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.8 forward pass:\n",
      "2023-10-07 11:12:55,770 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,774 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:12:55,784 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.9 forward pass:\n",
      "2023-10-07 11:12:55,795 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,799 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:12:55,811 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.10 forward pass:\n",
      "2023-10-07 11:12:55,822 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,825 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:12:55,833 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.11 forward pass:\n",
      "2023-10-07 11:12:55,838 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:12:55,842 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.final_layer_norm forward pass:\n",
      "2023-10-07 11:12:55,843 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,845 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:12:55,846 [flexgen_forward.py:47 in new_forward] DEBUG - lm_head forward pass:\n",
      "2023-10-07 11:12:55,857 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:12:55,865 [flexgen_test.py:30 in test_hf_gen] INFO - 0\n",
      "\n",
      "2023-10-07 11:12:55,866 [flexgen_test.py:31 in test_hf_gen] INFO - ----------\n",
      "2023-10-07 11:12:55,875 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.embed_tokens from test to old.\n",
      "2023-10-07 11:12:55,876 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.embed_positions from test to old.\n",
      "2023-10-07 11:12:55,878 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.final_layer_norm from test to old.\n",
      "2023-10-07 11:12:55,878 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.0 from test to old.\n",
      "2023-10-07 11:12:55,879 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.1 from test to old.\n",
      "2023-10-07 11:12:55,881 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.2 from test to old.\n",
      "2023-10-07 11:12:55,881 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.3 from test to old.\n",
      "2023-10-07 11:12:55,882 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.4 from test to old.\n",
      "2023-10-07 11:12:55,883 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.5 from test to old.\n",
      "2023-10-07 11:12:55,884 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.6 from test to old.\n",
      "2023-10-07 11:12:55,885 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.7 from test to old.\n",
      "2023-10-07 11:12:55,886 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.8 from test to old.\n",
      "2023-10-07 11:12:55,887 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.9 from test to old.\n",
      "2023-10-07 11:12:55,887 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.10 from test to old.\n",
      "2023-10-07 11:12:55,888 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.11 from test to old.\n",
      "2023-10-07 11:12:55,889 [flexgen_forward.py:27 in to_old_forward] DEBUG - lm_head from test to old.\n",
      "2023-10-07 11:12:55,891 [flexgen_init.py:104 in policy_init] INFO - layer order: ['model.decoder.embed_tokens', 'model.decoder.embed_positions', 'model.decoder.layers.0', 'model.decoder.layers.1', 'model.decoder.layers.2', 'model.decoder.layers.3', 'model.decoder.layers.4', 'model.decoder.layers.5', 'model.decoder.layers.6', 'model.decoder.layers.7', 'model.decoder.layers.8', 'model.decoder.layers.9', 'model.decoder.layers.10', 'model.decoder.layers.11', 'model.decoder.final_layer_norm', 'lm_head']\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"facebook/opt-125m\" # 125m 6.7b 13b 30b\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=2, \n",
    "    num_gpu_batches=4, \n",
    "    weights_gpu_percent=0.0, \n",
    "    weights_cpu_percent=0.3, \n",
    "    cache_gpu_percent=0.0, \n",
    "    cache_cpu_percent=0.2, \n",
    "    act_gpu_percent=0.0, \n",
    "    act_cpu_percent=0.5, \n",
    "    overlap=True, \n",
    "    pin_weight=True,\n",
    ")\n",
    "\n",
    "# model init\n",
    "model = policy_init(checkpoint, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load / offload: module weights, module inputs (hidden states, kv cache, etc.) / outputs (hidden states, attentions)\n",
    "#   FlexGenTensor <-> PyTorch Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 11:12:55,910 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward\n",
      "2023-10-07 11:12:55,911 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward\n",
      "2023-10-07 11:12:55,912 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward\n",
      "2023-10-07 11:12:55,914 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward\n",
      "2023-10-07 11:12:55,914 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward\n",
      "2023-10-07 11:12:55,915 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward\n",
      "2023-10-07 11:12:55,916 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward\n",
      "2023-10-07 11:12:55,917 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward\n",
      "2023-10-07 11:12:55,919 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward\n",
      "2023-10-07 11:12:55,919 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward\n",
      "2023-10-07 11:12:55,920 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward\n",
      "2023-10-07 11:12:55,921 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward\n",
      "2023-10-07 11:12:55,923 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward\n",
      "2023-10-07 11:12:55,924 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward\n",
      "2023-10-07 11:12:55,925 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward\n",
      "2023-10-07 11:12:55,926 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - lm_head to flexgen forward\n",
      "2023-10-07 11:12:56,032 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2023-10-07 11:12:56,208 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:12:56,210 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:56,211 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10]),)\n",
      "2023-10-07 11:12:56,212 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:56,213 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10]),)\n",
      "2023-10-07 11:12:56,213 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:56,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:12:56,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:12:56,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:12:56,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:12:56,220 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 768])\n",
      "2023-10-07 11:12:56,221 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 768])\n",
      "2023-10-07 11:12:56,222 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,224 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:56,225 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:56,236 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10]), 0)\n",
      "2023-10-07 11:12:56,236 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:56,237 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10]), 0)\n",
      "2023-10-07 11:12:56,238 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:56,239 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:12:56,241 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:12:56,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:12:56,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:12:56,245 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 768])\n",
      "2023-10-07 11:12:56,246 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 768])\n",
      "2023-10-07 11:12:56,247 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,250 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:56,260 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:56,268 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,269 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,270 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,271 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:12:56,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:12:56,291 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:12:56,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:12:56,303 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,305 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,306 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,309 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:56,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:56,325 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,326 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,327 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,328 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:12:56,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:12:56,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:12:56,347 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:12:56,361 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,363 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,363 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:56,374 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:56,382 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,383 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,384 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,385 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:12:56,394 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:12:56,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:12:56,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:12:56,411 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,414 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,415 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,418 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:56,430 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:56,443 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,444 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,445 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,446 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:12:56,453 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:12:56,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:12:56,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:12:56,474 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,476 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,477 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,480 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:56,488 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:56,497 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,498 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,499 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,500 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:12:56,509 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:12:56,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:12:56,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:12:56,530 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,532 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,533 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:56,545 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:56,553 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,554 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,554 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,556 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:12:56,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:12:56,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:12:56,576 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:12:56,581 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,584 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,584 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,587 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:56,597 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:56,607 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,609 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,610 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,611 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:12:56,670 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:12:56,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:12:56,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:12:56,690 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,692 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,693 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,696 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:56,704 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:56,713 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,714 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,714 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,716 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,716 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:12:56,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:12:56,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:12:56,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:12:56,737 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,738 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,739 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:56,750 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:12:56,759 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,760 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,761 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,762 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,763 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:12:56,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:12:56,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:12:56,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:12:56,785 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,787 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,788 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,790 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:12:56,798 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:12:56,809 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,810 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,811 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,813 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:12:56,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:12:56,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:12:56,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:12:56,839 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,840 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,841 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,843 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:12:56,851 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:12:56,860 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,860 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,861 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,862 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,863 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:12:56,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:12:56,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:12:56,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:12:56,885 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,886 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,887 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,890 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:12:56,898 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:12:56,900 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,900 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,901 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,903 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:56,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:12:56,911 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:12:56,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:12:56,926 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:12:56,932 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,934 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))\n",
      "2023-10-07 11:12:56,935 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,937 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:12:56,939 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:12:56,941 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,942 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:56,942 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,943 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:56,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:12:56,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:12:56,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:12:56,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:12:56,954 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 768])\n",
      "2023-10-07 11:12:56,955 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 768])\n",
      "2023-10-07 11:12:56,956 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:12:56,958 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:12:56,959 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:12:56,961 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)\n",
      "2023-10-07 11:12:56,961 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:56,962 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)\n",
      "2023-10-07 11:12:56,963 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:56,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:12:56,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:12:56,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:12:57,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:12:57,020 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 50272])\n",
      "2023-10-07 11:12:57,027 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 50272])\n",
      "2023-10-07 11:12:57,028 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,036 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:12:57,038 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:57,040 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:12:57,041 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:57,043 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:12:57,043 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:57,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:12:57,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:12:57,047 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:12:57,048 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:12:57,049 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:57,050 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:57,051 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:57,054 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:57,062 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 11]), 10)\n",
      "2023-10-07 11:12:57,063 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:57,064 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 11]), 10)\n",
      "2023-10-07 11:12:57,065 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:57,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:12:57,068 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:12:57,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:12:57,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:12:57,072 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:57,073 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:57,074 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:57,084 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:57,094 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,096 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,096 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,097 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,098 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:12:57,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:12:57,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:12:57,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:12:57,119 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,121 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,122 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,125 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:57,133 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:57,142 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,143 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,143 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,144 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:12:57,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:12:57,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:12:57,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:12:57,191 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,193 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,194 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,198 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:57,207 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:57,215 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,216 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,218 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,219 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:12:57,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:12:57,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:12:57,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:12:57,239 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,241 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,242 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:57,253 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:57,261 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,262 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,263 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,264 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:12:57,289 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:12:57,293 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:12:57,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:12:57,320 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,323 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,329 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:57,347 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:57,358 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,359 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,359 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,360 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:12:57,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:12:57,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:12:57,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:12:57,395 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,396 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,397 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,400 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:57,409 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:57,418 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,419 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,420 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,422 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:12:57,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:12:57,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:12:57,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:12:57,487 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,489 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,490 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:57,501 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:57,509 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,510 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,511 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,512 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:12:57,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:12:57,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:12:57,542 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:12:57,545 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,546 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,547 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,550 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:57,559 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:57,568 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,569 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,570 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,570 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,572 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:12:57,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:12:57,580 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:12:57,596 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:12:57,610 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,612 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,613 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,616 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:57,624 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:12:57,632 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,633 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,634 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,635 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:12:57,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:12:57,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:12:57,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:12:57,653 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,656 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,657 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,659 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:12:57,667 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:12:57,675 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,676 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,677 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,678 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:12:57,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:12:57,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:12:57,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:12:57,699 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,702 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,703 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,705 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:12:57,713 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:12:57,722 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,723 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,724 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,725 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:12:57,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:12:57,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:12:57,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:12:57,753 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,755 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,756 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,759 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:12:57,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:12:57,768 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,769 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,770 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,771 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:12:57,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:12:57,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:12:57,784 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:12:57,794 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,797 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))\n",
      "2023-10-07 11:12:57,798 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,800 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:12:57,802 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:12:57,804 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,805 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:57,806 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,807 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:57,808 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:12:57,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:12:57,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:12:57,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:12:57,820 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:57,821 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:57,823 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,825 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:12:57,827 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:12:57,829 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,830 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:57,831 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,832 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:57,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:12:57,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:12:57,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:12:57,863 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:12:57,871 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:12:57,874 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:12:57,875 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,883 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:12:57,885 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:57,887 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:12:57,888 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:57,889 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:12:57,890 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:57,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:12:57,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:12:57,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:12:57,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:12:57,895 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:57,896 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:57,897 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,899 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:57,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:57,910 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 12]), 11)\n",
      "2023-10-07 11:12:57,911 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:57,912 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 12]), 11)\n",
      "2023-10-07 11:12:57,913 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:57,914 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:12:57,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:12:57,919 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:12:57,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:12:57,921 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:57,923 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:57,923 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,925 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:57,935 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:57,943 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,944 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,945 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:57,946 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:12:57,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:12:57,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:12:57,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:12:57,976 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:57,977 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:57,978 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:12:57,981 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:57,989 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:57,997 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:57,998 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:57,999 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,000 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,001 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:12:58,008 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:12:58,013 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:12:58,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:12:58,020 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,022 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,023 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,025 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:58,033 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:58,041 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,042 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,043 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,044 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:12:58,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:12:58,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:12:58,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:12:58,067 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,069 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,070 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,072 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:58,079 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:58,087 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,088 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,088 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,089 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,091 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:12:58,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:12:58,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:12:58,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:12:58,129 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,130 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,131 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,134 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:58,142 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:58,150 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,151 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,152 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,153 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:12:58,208 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:12:58,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:12:58,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:12:58,226 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,228 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,229 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,232 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:58,240 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:58,249 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,250 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,251 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,251 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:12:58,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:12:58,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:12:58,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:12:58,269 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,274 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,274 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,277 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:58,285 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:58,293 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,294 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,295 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,296 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,297 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:12:58,305 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:12:58,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:12:58,312 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:12:58,316 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,317 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,318 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,321 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:58,328 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:58,336 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,337 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,338 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,339 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:12:58,350 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:12:58,354 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:12:58,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:12:58,361 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,363 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,364 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:58,373 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:12:58,382 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,383 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,383 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,384 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:12:58,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:12:58,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:12:58,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:12:58,409 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,410 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,411 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,414 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:12:58,421 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:12:58,429 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,430 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,431 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,432 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:12:58,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:12:58,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:12:58,454 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:12:58,458 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,459 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,460 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,462 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:12:58,470 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:12:58,479 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,479 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,480 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,481 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:12:58,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:12:58,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:12:58,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:12:58,508 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,509 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,510 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,513 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:12:58,521 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:12:58,523 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,523 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,524 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,525 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,526 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:12:58,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:12:58,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:12:58,539 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:12:58,544 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,545 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))\n",
      "2023-10-07 11:12:58,546 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,549 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:12:58,551 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:12:58,552 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,553 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:58,554 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,555 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:58,556 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:12:58,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:12:58,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:12:58,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:12:58,567 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:58,569 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:58,570 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,571 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:12:58,573 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:12:58,574 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,575 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:58,576 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,577 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:58,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:12:58,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:12:58,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:12:58,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:12:58,630 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:12:58,635 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:12:58,635 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,654 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:12:58,656 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:58,658 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:12:58,659 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:58,659 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:12:58,660 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:58,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:12:58,662 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:12:58,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:12:58,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:12:58,665 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:58,666 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:58,667 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,668 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:58,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:58,678 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 13]), 12)\n",
      "2023-10-07 11:12:58,678 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:58,680 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 13]), 12)\n",
      "2023-10-07 11:12:58,680 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:58,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:12:58,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:12:58,684 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:12:58,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:12:58,686 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:58,687 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:58,688 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,689 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:58,697 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:58,705 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,706 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,707 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,708 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:12:58,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:12:58,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:12:58,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:12:58,733 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,734 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,735 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,738 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:58,746 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:58,753 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,754 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,755 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,756 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:12:58,763 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:12:58,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:12:58,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:12:58,783 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,784 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,785 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,788 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:58,802 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:58,810 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,812 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,813 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,814 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,815 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:12:58,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:12:58,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:12:58,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:12:58,826 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,828 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,829 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,832 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:58,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:58,848 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,849 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,850 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,851 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:12:58,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:12:58,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:12:58,865 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:12:58,868 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,869 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,870 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,874 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:58,883 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:58,894 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,895 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,896 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,897 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:12:58,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:12:58,914 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:12:58,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:12:58,943 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,944 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,945 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,948 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:58,956 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:58,963 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:58,964 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,965 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:58,965 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:58,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:12:58,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:12:58,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:12:58,989 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:12:58,992 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,994 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:58,995 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:12:58,997 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:59,005 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:59,014 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,015 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,016 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,016 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:12:59,028 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:12:59,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:12:59,041 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:12:59,058 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,060 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,061 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,064 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:59,073 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:59,081 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,082 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,082 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,083 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:12:59,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:12:59,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:12:59,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:12:59,099 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,101 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,102 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,105 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:59,114 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:12:59,124 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,124 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,125 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,126 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,127 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:12:59,132 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:12:59,135 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:12:59,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:12:59,142 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,143 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,144 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,147 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:12:59,155 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:12:59,163 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,164 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,164 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,165 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:12:59,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:12:59,179 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:12:59,182 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:12:59,185 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,187 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,187 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,190 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:12:59,199 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:12:59,207 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,208 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,209 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,210 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,211 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:12:59,220 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:12:59,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:12:59,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:12:59,227 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,229 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,230 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,232 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:12:59,240 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:12:59,242 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,243 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,244 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,245 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:12:59,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:12:59,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:12:59,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:12:59,267 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,269 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))\n",
      "2023-10-07 11:12:59,270 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,278 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:12:59,281 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:12:59,283 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,284 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:59,285 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,286 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:59,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:12:59,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:12:59,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:12:59,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:12:59,300 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:59,301 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:59,302 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,303 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:12:59,305 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:12:59,306 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,307 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:59,308 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,309 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:59,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:12:59,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:12:59,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:12:59,343 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:12:59,354 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:12:59,358 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:12:59,359 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,371 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:12:59,373 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:59,375 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:12:59,376 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:59,377 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:12:59,378 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:59,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:12:59,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:12:59,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:12:59,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:12:59,383 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:59,384 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:59,385 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,386 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:12:59,388 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:59,396 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 14]), 13)\n",
      "2023-10-07 11:12:59,397 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:12:59,398 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 14]), 13)\n",
      "2023-10-07 11:12:59,399 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:12:59,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:12:59,401 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:12:59,402 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:12:59,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:12:59,405 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:12:59,406 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:12:59,406 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,408 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:12:59,415 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:59,424 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,424 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,425 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,426 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,427 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:12:59,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:12:59,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:12:59,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:12:59,543 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,545 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,546 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,550 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:12:59,558 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:59,567 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,568 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,569 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,570 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:12:59,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:12:59,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:12:59,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:12:59,594 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,595 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,596 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,599 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:12:59,607 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:59,615 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,616 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,617 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,618 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:12:59,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:12:59,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:12:59,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:12:59,648 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,650 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,651 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,654 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:12:59,662 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:59,670 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,671 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,672 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,673 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:12:59,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:12:59,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:12:59,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:12:59,693 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,695 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,695 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,699 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:12:59,708 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:59,716 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,717 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,718 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,719 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:12:59,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:12:59,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:12:59,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:12:59,742 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,744 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,745 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,748 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:12:59,761 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:59,776 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,778 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,780 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,782 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,783 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:12:59,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:12:59,796 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:12:59,800 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:12:59,865 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,867 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,868 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,871 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:12:59,879 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:59,889 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,891 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,892 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,893 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:12:59,913 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:12:59,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:12:59,926 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:12:59,929 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,931 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,932 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,935 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:12:59,943 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:59,951 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,952 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,953 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,953 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,955 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:12:59,961 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:12:59,965 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:12:59,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:12:59,972 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,974 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:12:59,975 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:12:59,977 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:12:59,984 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:12:59,993 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:12:59,994 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,995 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:12:59,995 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:12:59,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:00,008 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:00,013 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:00,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:00,023 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:13:00,025 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:13:00,026 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,029 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:00,039 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:00,048 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,050 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,051 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,052 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,052 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:00,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:00,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:00,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:00,071 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:13:00,074 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:13:00,074 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,077 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:00,085 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:00,094 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,097 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,097 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,099 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:00,105 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:00,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:00,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:00,130 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:13:00,131 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:13:00,132 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,135 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:00,143 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:00,145 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,146 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,148 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,149 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,150 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:00,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:00,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:00,170 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:00,174 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))\n",
      "2023-10-07 11:13:00,176 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))\n",
      "2023-10-07 11:13:00,177 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,180 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:00,182 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:00,183 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,184 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:00,185 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,185 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:00,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:00,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:00,192 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:00,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:00,197 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:00,198 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:00,199 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,201 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:00,203 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:00,204 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,205 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:00,206 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,207 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:00,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:00,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:00,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:00,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:00,244 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:00,246 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:00,247 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,255 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:00,256 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:00,258 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:00,259 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:00,260 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:00,261 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:00,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:00,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:00,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:00,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:00,266 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:00,267 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:00,268 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,269 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:00,271 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:00,279 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 15]), 14)\n",
      "2023-10-07 11:13:00,280 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:00,281 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 15]), 14)\n",
      "2023-10-07 11:13:00,281 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:00,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:00,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:00,285 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:00,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:00,287 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:00,288 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:00,289 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,290 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:00,298 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:00,306 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,306 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,307 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,308 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:00,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:00,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:00,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:00,341 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,343 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,344 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,347 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:00,355 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:00,363 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,363 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,364 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,365 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,366 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:00,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:00,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:00,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:00,383 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,384 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,385 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:00,409 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:00,418 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,420 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,421 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,422 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:00,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:00,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:00,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:00,474 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,475 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,476 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,479 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:00,488 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:00,496 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,497 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,498 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,499 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:00,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:00,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:00,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:00,516 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,518 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,519 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:00,530 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:00,538 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,539 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,540 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,541 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,542 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:00,547 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:00,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:00,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:00,563 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,565 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,566 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,568 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:00,576 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:00,584 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,585 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,586 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,587 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:00,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:00,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:00,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:00,614 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,616 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,617 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,619 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:00,627 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:00,636 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,637 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,638 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,639 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:00,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:00,651 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:00,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:00,662 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,663 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,664 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,667 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:00,675 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:00,683 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,684 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,685 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,686 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,686 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:00,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:00,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:00,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:00,716 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,718 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,719 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,721 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:00,730 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:00,738 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,739 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,740 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,741 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:00,748 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:00,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:00,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:00,770 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,771 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,772 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,775 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:00,783 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:00,792 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,793 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,794 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,796 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,796 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:00,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:00,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:00,815 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:00,826 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,828 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,829 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:00,841 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:00,849 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,850 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,852 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,852 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:00,859 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:00,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:00,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:00,878 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,879 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,880 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:00,883 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:00,891 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:00,898 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:00,899 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,900 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:00,905 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:00,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:00,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:00,978 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:00,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:00,994 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,996 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))\n",
      "2023-10-07 11:13:00,997 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,000 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:01,002 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:01,004 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,005 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:01,005 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,006 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:01,007 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:01,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:01,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:01,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:01,019 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:01,020 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:01,021 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,023 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:01,024 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:01,026 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,027 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:01,028 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,029 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:01,030 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:01,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:01,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:01,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:01,071 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:01,074 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:01,075 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,082 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:01,084 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:01,085 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:01,086 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:01,087 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:01,088 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:01,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:01,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:01,091 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:01,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:01,094 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:01,094 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:01,095 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,097 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:01,098 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:01,106 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 16]), 15)\n",
      "2023-10-07 11:13:01,107 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:01,108 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 16]), 15)\n",
      "2023-10-07 11:13:01,109 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:01,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:01,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:01,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:01,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:01,115 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:01,116 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:01,117 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,119 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:01,126 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:01,134 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,135 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,136 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,137 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:01,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:01,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:01,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:01,164 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,166 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,167 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,169 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:01,177 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:01,185 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,186 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,187 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,188 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:01,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:01,198 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:01,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:01,206 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,207 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,208 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,211 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:01,218 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:01,227 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,228 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,229 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,230 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,231 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:01,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:01,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:01,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:01,246 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,247 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,250 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,252 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:01,260 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:01,268 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,269 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,270 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,272 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,273 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:01,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:01,285 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:01,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:01,291 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,293 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,294 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,296 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:01,306 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:01,316 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,317 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,318 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,318 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:01,376 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:01,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:01,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:01,417 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,418 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,419 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,422 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:01,430 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:01,438 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,439 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,440 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,440 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,442 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:01,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:01,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:01,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:01,464 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,465 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,466 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,469 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:01,477 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:01,486 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,487 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,488 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,489 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:01,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:01,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:01,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:01,506 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,507 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,508 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,511 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:01,520 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:01,528 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,528 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,529 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,530 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:01,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:01,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:01,543 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:01,551 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,552 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,553 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,556 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:01,564 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:01,572 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,573 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,574 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,575 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,576 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:01,581 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:01,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:01,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:01,591 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,593 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,594 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,596 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:01,605 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:01,613 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,614 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,615 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,616 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,617 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:01,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:01,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:01,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:01,636 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,637 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,638 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,640 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:01,648 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:01,655 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,656 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,657 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,658 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,659 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:01,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:01,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:01,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:01,682 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,683 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,684 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,686 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:01,694 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:01,697 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,698 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,699 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,699 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:01,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:01,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:01,734 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:01,737 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,738 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))\n",
      "2023-10-07 11:13:01,739 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:01,744 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:01,746 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,747 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:01,748 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,749 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:01,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:01,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:01,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:01,758 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:01,759 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:01,760 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:01,762 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,764 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:01,765 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:01,766 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,767 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:01,768 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,769 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:01,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:01,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:01,795 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:01,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:01,828 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:01,857 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:01,858 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,907 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:01,910 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:01,911 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:01,912 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:01,913 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:01,914 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:01,915 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:01,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:01,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:01,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:01,919 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:01,920 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:01,921 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,923 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:01,924 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:01,932 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 17]), 16)\n",
      "2023-10-07 11:13:01,933 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:01,934 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 17]), 16)\n",
      "2023-10-07 11:13:01,935 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:01,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:01,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:01,938 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:01,940 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:01,941 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:01,942 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:01,943 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,944 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:01,953 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:01,961 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:01,962 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,962 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:01,964 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:01,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:01,970 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:01,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:01,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:01,986 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:01,987 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:01,988 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:01,991 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:01,999 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:02,007 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,007 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,008 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,009 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:02,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:02,021 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:02,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:02,027 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,028 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,029 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,032 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:02,042 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:02,050 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,051 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,052 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,053 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:02,058 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:02,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:02,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:02,067 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,069 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,070 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,073 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:02,080 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:02,088 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,089 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,090 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,091 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:02,098 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:02,101 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:02,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:02,108 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,110 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,111 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,114 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:02,127 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:02,142 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,149 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,150 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,151 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:02,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:02,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:02,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:02,231 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,234 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,235 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,238 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:02,246 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:02,254 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,255 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,256 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,257 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:02,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:02,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:02,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:02,277 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,279 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,280 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,282 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:02,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:02,302 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,303 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,304 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,305 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,306 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:02,321 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:02,326 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:02,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:02,342 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,344 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,345 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,349 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:02,357 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:02,366 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,367 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,369 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,370 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:02,379 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:02,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:02,390 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:02,397 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,399 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,400 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,403 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:02,412 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:02,421 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,422 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,423 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,424 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:02,434 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:02,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:02,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:02,453 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,455 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,456 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,459 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:02,469 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:02,478 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,479 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,480 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,482 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:02,489 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:02,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:02,498 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:02,507 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,509 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,510 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,513 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:02,521 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:02,530 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,531 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,532 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,532 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:02,539 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:02,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:02,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:02,553 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,555 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,556 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,560 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:02,568 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:02,571 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,571 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,572 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,573 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:02,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:02,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:02,592 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:02,597 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,600 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))\n",
      "2023-10-07 11:13:02,601 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,603 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:02,605 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:02,607 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,607 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:02,608 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,609 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:02,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:02,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:02,615 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:02,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:02,618 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:02,619 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:02,620 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,621 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:02,623 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:02,624 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,625 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:02,626 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,627 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:02,628 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:02,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:02,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:02,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:02,671 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:02,673 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:02,674 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,682 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:02,684 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:02,686 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:02,686 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:02,687 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:02,688 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:02,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:02,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:02,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:02,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:02,693 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:02,694 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:02,695 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,696 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:02,698 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:02,706 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 18]), 17)\n",
      "2023-10-07 11:13:02,707 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:02,708 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 18]), 17)\n",
      "2023-10-07 11:13:02,709 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:02,710 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:02,712 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:02,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:02,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:02,715 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:02,716 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:02,717 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,718 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:02,726 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:02,734 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,735 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,736 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,737 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:02,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:02,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:02,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:02,760 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:02,762 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:02,763 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:02,776 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:02,785 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,786 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,797 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,802 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:02,842 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:02,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:02,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:02,888 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:02,890 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:02,892 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,895 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:02,903 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:02,913 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,914 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,915 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,917 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:02,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:02,939 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:02,949 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:02,954 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:02,955 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:02,956 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:02,959 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:02,966 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:02,974 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:02,975 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,977 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:02,978 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:02,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:02,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:02,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:03,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:03,003 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,005 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,006 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,008 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:03,016 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:03,025 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,026 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,027 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,029 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:03,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:03,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:03,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:03,062 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,064 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,065 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,067 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:03,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:03,083 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,084 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,085 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,086 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:03,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:03,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:03,115 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:03,119 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,120 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,121 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,124 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:03,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:03,140 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,141 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,142 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,143 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:03,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:03,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:03,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:03,161 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,162 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,163 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,166 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:03,173 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:03,182 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,183 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,184 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,185 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:03,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:03,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:03,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:03,202 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,204 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,205 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,207 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:03,215 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:03,223 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,224 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,225 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,228 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:03,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:03,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:03,316 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:03,323 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,325 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,326 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,329 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:03,337 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:03,346 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,347 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,348 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,349 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,350 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:03,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:03,361 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:03,364 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:03,368 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,370 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,371 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,373 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:03,381 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:03,390 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,390 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,391 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,393 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,394 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:03,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:03,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:03,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:03,413 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,415 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,415 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,418 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:03,426 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:03,428 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,429 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,430 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,431 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:03,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:03,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:03,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:03,457 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,459 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))\n",
      "2023-10-07 11:13:03,460 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,463 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:03,465 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:03,467 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,467 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:03,468 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,469 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:03,470 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:03,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:03,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:03,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:03,481 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:03,482 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:03,483 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,485 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:03,487 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:03,488 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,489 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:03,489 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,490 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:03,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:03,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:03,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:03,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:03,532 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:03,534 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:03,535 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,542 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:03,544 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:03,546 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:03,547 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:03,547 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:03,548 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:03,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:03,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:03,552 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:03,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:03,554 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:03,555 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:03,556 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,557 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:03,559 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:03,568 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 19]), 18)\n",
      "2023-10-07 11:13:03,569 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:03,570 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 19]), 18)\n",
      "2023-10-07 11:13:03,571 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:03,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:03,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:03,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:03,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:03,577 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:03,577 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:03,578 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,580 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:03,587 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:03,597 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,598 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,599 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,600 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:03,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:03,615 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:03,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:03,624 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,626 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,627 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,630 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:03,639 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:03,649 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,650 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,651 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,652 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:03,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:03,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:03,667 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:03,670 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,671 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,672 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,675 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:03,683 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:03,692 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,693 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,694 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,695 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:03,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:03,710 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:03,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:03,716 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,717 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,718 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,721 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:03,730 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:03,739 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,740 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,741 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,742 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,743 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:03,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:03,758 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:03,762 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:03,765 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,767 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,768 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,770 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:03,779 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:03,787 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,788 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,789 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,790 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:03,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:03,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:03,805 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:03,808 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,810 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,811 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:03,822 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:03,831 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,832 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,833 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,833 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:03,840 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:03,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:03,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:03,856 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,858 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,859 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,861 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:03,869 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:03,877 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:03,878 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,879 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:03,879 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:03,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:03,914 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:03,938 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:03,970 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:03,985 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,987 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:03,988 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:03,990 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:03,999 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:04,008 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,009 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,010 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,011 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:04,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:04,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:04,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:04,047 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:04,048 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:04,049 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:04,062 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:04,070 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,071 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,072 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,073 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:04,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:04,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:04,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:04,095 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:04,096 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:04,098 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:04,109 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:04,117 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,118 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,120 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,121 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:04,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:04,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:04,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:04,145 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:04,147 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:04,148 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,151 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:04,160 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:04,168 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,169 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,170 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,171 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:04,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:04,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:04,183 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:04,188 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:04,189 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:04,190 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,193 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:04,203 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:04,205 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,206 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,207 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,208 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:04,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:04,226 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:04,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:04,236 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))\n",
      "2023-10-07 11:13:04,239 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))\n",
      "2023-10-07 11:13:04,239 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,242 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:04,243 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:04,245 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,246 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:04,247 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,247 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:04,249 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:04,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:04,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:04,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:04,255 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:04,256 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:04,257 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:04,260 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:04,261 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,262 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:04,263 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,264 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:04,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:04,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:04,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:04,313 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:04,324 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:04,327 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:04,328 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,336 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:04,338 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:04,339 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:04,340 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:04,341 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:04,342 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:04,343 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:04,344 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:04,345 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:04,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:04,347 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:04,348 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:04,349 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,350 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:04,352 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:04,360 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 20]), 19)\n",
      "2023-10-07 11:13:04,361 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:04,361 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 20]), 19)\n",
      "2023-10-07 11:13:04,362 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:04,363 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:04,365 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:04,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:04,368 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:04,369 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:04,370 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:04,371 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,372 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:04,380 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:04,389 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,390 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,391 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,392 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:04,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:04,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:04,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:04,427 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,429 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,430 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,433 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:04,445 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:04,459 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,460 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,461 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,461 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:04,470 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:04,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:04,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:04,493 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,495 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,496 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,500 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:04,513 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:04,526 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,527 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,528 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,529 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,530 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:04,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:04,547 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:04,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:04,556 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,558 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,559 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,562 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:04,571 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:04,579 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,580 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,581 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,582 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:04,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:04,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:04,607 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:04,611 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,616 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,617 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,620 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:04,628 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:04,636 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,637 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,638 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,640 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:04,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:04,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:04,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:04,674 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,676 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,677 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:04,694 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:04,707 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,708 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,710 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,711 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,712 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:04,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:04,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:04,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:04,731 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,732 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,733 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,736 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:04,746 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:04,757 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,758 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,759 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,759 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:04,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:04,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:04,783 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:04,789 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,791 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,792 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,795 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:04,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:04,812 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,813 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,814 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,815 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:04,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:04,830 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:04,835 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:04,840 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,842 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,842 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,845 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:04,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:04,861 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,862 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,863 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,864 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,865 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:04,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:04,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:04,899 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:04,903 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,904 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:04,905 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:04,908 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:04,916 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:04,924 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:04,925 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,927 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:04,928 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:04,930 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:04,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:05,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:05,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:05,031 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:05,033 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:05,035 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,037 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:05,047 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:05,056 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,057 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,059 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,060 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:05,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:05,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:05,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:05,091 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:05,093 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:05,094 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,097 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:05,109 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:05,112 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,113 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,114 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,115 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:05,123 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:05,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:05,135 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:05,139 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))\n",
      "2023-10-07 11:13:05,141 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))\n",
      "2023-10-07 11:13:05,142 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,144 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:05,146 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:05,148 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,149 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:05,150 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,151 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:05,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:05,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:05,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:05,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:05,158 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:05,159 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:05,160 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,162 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:05,163 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:05,165 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,165 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:05,166 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,167 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:05,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:05,183 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:05,192 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:05,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:05,210 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:05,213 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:05,214 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,222 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:05,224 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:05,225 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:05,226 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:05,228 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:05,228 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:05,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:05,231 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:05,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:05,233 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:05,234 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:05,235 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:05,236 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,237 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:05,239 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:05,246 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 21]), 20)\n",
      "2023-10-07 11:13:05,247 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:05,248 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 21]), 20)\n",
      "2023-10-07 11:13:05,249 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:05,250 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:05,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:05,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:05,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:05,254 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:05,255 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:05,256 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,257 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:05,266 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:05,274 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,275 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,276 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,276 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,277 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:05,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:05,292 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:05,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:05,303 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,305 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,306 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,309 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:05,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:05,325 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,326 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,327 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,327 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,328 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:05,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:05,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:05,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:05,346 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,347 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,348 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,351 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:05,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:05,369 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,370 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,371 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,372 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:05,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:05,379 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:05,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:05,401 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,403 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,404 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,406 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:05,414 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:05,423 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,425 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,426 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,427 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:05,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:05,454 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:05,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:05,462 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,464 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,465 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,468 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:05,476 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:05,484 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,485 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,486 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,487 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,488 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:05,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:05,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:05,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:05,511 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,513 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,514 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,517 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:05,527 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:05,535 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,536 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,537 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,538 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,539 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:05,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:05,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:05,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:05,557 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,559 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,560 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,562 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:05,571 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:05,580 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,581 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,582 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,583 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,584 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:05,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:05,595 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:05,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:05,603 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,605 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,606 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,609 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:05,617 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:05,625 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,626 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,627 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,628 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:05,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:05,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:05,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:05,653 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,655 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,656 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,658 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:05,667 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:05,675 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,677 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,678 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,679 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:05,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:05,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:05,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:05,751 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,753 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,754 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,757 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:05,765 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:05,782 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,783 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,784 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,785 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:05,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:05,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:05,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:05,849 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,850 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,852 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,854 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:05,863 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:05,871 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,872 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,873 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,874 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:05,881 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:05,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:05,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:05,898 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,900 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,901 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:05,915 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:05,917 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,917 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,919 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,919 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:05,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:05,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:05,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:05,932 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:05,935 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,938 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))\n",
      "2023-10-07 11:13:05,939 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,941 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:05,943 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:05,944 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,945 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:05,946 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,946 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:05,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:05,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:05,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:05,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:05,954 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:05,955 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:05,956 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:05,958 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:05,959 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:05,960 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:05,961 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:05,962 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:05,962 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:05,963 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:05,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:05,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:05,988 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:05,997 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:06,000 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:06,002 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,009 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:06,011 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:06,013 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:06,013 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:06,014 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:06,015 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:06,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:06,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:06,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:06,019 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:06,020 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:06,021 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:06,021 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,023 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:06,024 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:06,032 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 22]), 21)\n",
      "2023-10-07 11:13:06,033 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:06,034 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 22]), 21)\n",
      "2023-10-07 11:13:06,035 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:06,036 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:06,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:06,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:06,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:06,040 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:06,041 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:06,042 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,043 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:06,051 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:06,059 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,060 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,061 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,062 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:06,068 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:06,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:06,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:06,088 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,089 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,090 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,093 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:06,101 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:06,109 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,110 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,113 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,114 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,115 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:06,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:06,135 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:06,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:06,145 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,147 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,148 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,151 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:06,163 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:06,177 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,178 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,179 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,180 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:06,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:06,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:06,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:06,204 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,206 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,207 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,211 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:06,220 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:06,228 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,229 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,230 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,230 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,231 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:06,240 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:06,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:06,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:06,255 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,257 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,258 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,261 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:06,269 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:06,278 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,279 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,280 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,280 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:06,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:06,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:06,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:06,307 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,309 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,309 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,312 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:06,321 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:06,329 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,330 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,331 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,332 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:06,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:06,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:06,368 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:06,372 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,403 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,405 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,407 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:06,415 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:06,424 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,425 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,426 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,427 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:06,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:06,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:06,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:06,448 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,450 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,451 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,455 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:06,463 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:06,471 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,472 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,473 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,474 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:06,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:06,485 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:06,488 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:06,491 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,493 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,493 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,496 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:06,505 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:06,513 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,514 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,516 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,517 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:06,550 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:06,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:06,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:06,604 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,615 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,616 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,619 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:06,627 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:06,635 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,636 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,637 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,638 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:06,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:06,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:06,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:06,657 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,659 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,660 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,662 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:06,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:06,678 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,679 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,680 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,681 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:06,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:06,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:06,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:06,702 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,704 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,705 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,707 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:06,715 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:06,717 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,718 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,719 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,720 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,721 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:06,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:06,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:06,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:06,785 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,818 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))\n",
      "2023-10-07 11:13:06,819 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,822 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:06,824 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:06,826 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,827 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:06,828 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,829 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:06,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:06,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:06,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:06,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:06,835 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:06,837 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:06,838 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,839 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:06,841 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:06,842 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,843 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:06,844 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,845 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:06,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:06,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:06,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:06,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:06,884 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:06,892 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:06,893 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,905 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:06,907 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:06,909 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:06,910 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:06,911 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:06,911 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:06,913 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:06,914 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:06,915 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:06,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:06,918 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:06,919 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:06,921 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,922 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:06,923 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:06,931 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 23]), 22)\n",
      "2023-10-07 11:13:06,932 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:06,933 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 23]), 22)\n",
      "2023-10-07 11:13:06,934 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:06,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:06,936 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:06,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:06,938 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:06,939 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:06,940 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:06,941 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,942 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:06,950 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:06,958 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,959 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,960 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:06,961 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:06,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:06,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:06,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:06,976 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:06,978 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:06,979 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:06,982 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:06,989 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:06,998 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:06,998 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:06,999 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,000 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,001 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:07,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:07,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:07,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:07,058 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,075 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,077 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,079 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:07,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:07,095 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,096 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,097 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,098 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:07,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:07,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:07,109 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:07,112 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,117 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,118 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,120 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:07,128 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:07,137 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,138 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,139 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,140 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:07,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:07,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:07,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:07,162 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,164 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,164 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,167 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:07,176 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:07,185 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,186 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,187 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,188 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:07,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:07,203 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:07,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:07,214 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,216 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,217 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,219 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:07,228 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:07,236 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,237 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,238 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,239 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,240 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:07,245 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:07,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:07,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:07,273 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,275 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,276 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,278 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:07,286 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:07,295 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,296 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,297 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,298 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:07,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:07,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:07,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:07,391 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,393 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,394 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,396 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:07,403 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:07,411 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,412 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,413 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,414 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:07,421 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:07,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:07,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:07,438 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,440 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,441 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,443 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:07,451 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:07,459 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,460 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,461 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,462 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:07,469 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:07,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:07,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:07,483 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,485 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,486 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,489 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:07,498 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:07,506 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,507 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,508 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,509 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:07,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:07,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:07,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:07,535 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,537 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,538 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,541 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:07,549 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:07,558 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,559 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,560 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,561 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:07,567 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:07,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:07,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:07,580 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,581 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,582 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,585 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:07,594 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:07,595 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,596 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,597 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,598 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:07,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:07,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:07,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:07,619 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,621 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))\n",
      "2023-10-07 11:13:07,622 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:07,627 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:07,629 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,629 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:07,630 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,631 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:07,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:07,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:07,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:07,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:07,646 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:07,647 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:07,648 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,649 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:07,651 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:07,652 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,653 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:07,654 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,655 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:07,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:07,667 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:07,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:07,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:07,699 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:07,701 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:07,702 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,710 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:07,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:07,713 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:07,714 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:07,715 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:07,716 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:07,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:07,718 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:07,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:07,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:07,722 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:07,723 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:07,724 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,725 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:07,727 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:07,740 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 24]), 23)\n",
      "2023-10-07 11:13:07,742 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:07,745 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 24]), 23)\n",
      "2023-10-07 11:13:07,746 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:07,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:07,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:07,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:07,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:07,755 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:07,756 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:07,758 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,761 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:07,770 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:07,779 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,780 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,781 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,781 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:07,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:07,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:07,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:07,814 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:07,816 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:07,817 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,820 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:07,829 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:07,837 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,838 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,839 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,840 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:07,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:07,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:07,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:07,856 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:07,858 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:07,859 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:07,870 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:07,878 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,879 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,880 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,881 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,882 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:07,886 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:07,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:07,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:07,897 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:07,900 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:07,901 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,905 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:07,918 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:07,929 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,930 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,931 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,932 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:07,941 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:07,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:07,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:07,959 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:07,961 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:07,962 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:07,965 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:07,973 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:07,982 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:07,983 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,984 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:07,985 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:07,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:07,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:07,995 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:08,036 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:08,063 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,068 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,069 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,071 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:08,080 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:08,089 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,090 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,092 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,093 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:08,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:08,109 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:08,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:08,116 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,119 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,120 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,122 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:08,131 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:08,140 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,141 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,142 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,143 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:08,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:08,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:08,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:08,174 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,176 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,177 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,179 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:08,188 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:08,197 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,198 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,199 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,199 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:08,208 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:08,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:08,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:08,221 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,223 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,224 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,227 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:08,235 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:08,244 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,245 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,246 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,247 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:08,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:08,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:08,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:08,275 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,279 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,280 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,283 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:08,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:08,300 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,301 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,302 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,303 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:08,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:08,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:08,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:08,338 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,341 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,342 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,344 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:08,353 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:08,362 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,363 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,364 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,365 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,365 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:08,370 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:08,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:08,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:08,388 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,390 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,391 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:08,403 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:08,406 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,407 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,409 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,410 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:08,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:08,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:08,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:08,498 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,500 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))\n",
      "2023-10-07 11:13:08,501 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,503 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:08,505 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:08,507 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,508 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:08,509 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,510 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:08,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:08,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:08,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:08,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:08,517 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:08,518 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:08,519 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,520 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:08,521 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:08,523 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,524 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:08,525 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,526 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:08,528 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:08,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:08,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:08,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:08,567 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:08,573 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:08,573 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:08,583 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:08,585 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:08,586 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:08,587 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:08,588 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:08,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:08,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:08,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:08,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:08,594 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:08,595 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:08,596 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,598 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:08,599 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:08,608 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 25]), 24)\n",
      "2023-10-07 11:13:08,609 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:08,610 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 25]), 24)\n",
      "2023-10-07 11:13:08,611 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:08,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:08,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:08,615 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:08,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:08,617 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:08,619 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:08,619 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,621 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:08,632 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:08,640 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,641 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,642 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,643 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:08,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:08,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:08,734 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:08,738 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:08,740 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:08,741 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,744 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:08,753 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:08,762 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,763 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,763 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,764 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,765 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:08,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:08,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:08,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:08,786 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:08,787 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:08,788 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,791 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:08,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:08,812 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,813 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,814 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,815 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:08,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:08,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:08,838 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:08,849 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:08,851 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:08,852 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,855 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:08,863 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:08,871 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,873 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,874 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,875 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:08,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:08,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:08,886 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:08,892 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:08,902 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:08,904 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,907 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:08,915 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:08,929 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,930 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,931 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,932 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:08,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:08,951 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:08,958 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:08,963 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:08,965 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:08,966 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:08,968 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:08,977 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:08,986 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:08,986 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,987 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:08,989 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:08,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:08,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:09,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:09,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:09,016 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,018 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,020 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,023 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:09,031 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:09,039 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,040 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,041 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,042 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:09,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:09,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:09,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:09,084 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,086 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,087 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,090 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:09,101 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:09,112 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,113 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,114 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,115 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:09,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:09,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:09,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:09,135 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,137 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,137 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,140 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:09,148 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:09,156 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,157 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,158 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,159 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,160 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:09,165 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:09,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:09,171 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:09,180 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,181 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,182 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,186 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:09,194 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:09,203 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,204 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,205 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,206 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:09,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:09,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:09,222 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:09,230 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,235 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,236 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,238 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:09,247 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:09,255 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,256 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,257 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,258 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:09,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:09,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:09,325 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:09,327 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,333 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,335 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,337 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:09,345 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:09,347 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,348 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,349 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,350 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:09,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:09,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:09,364 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:09,367 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,368 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))\n",
      "2023-10-07 11:13:09,369 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,372 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:09,374 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:09,375 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,376 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:09,377 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,378 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:09,379 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:09,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:09,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:09,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:09,390 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:09,391 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:09,392 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,393 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:09,395 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:09,396 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,397 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:09,398 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,398 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:09,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:09,410 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:09,420 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:09,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:09,438 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:09,440 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:09,441 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,466 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:09,468 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:09,469 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:09,470 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:09,470 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:09,472 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:09,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:09,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:09,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:09,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:09,476 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:09,477 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:09,478 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,480 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:09,482 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:09,490 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 26]), 25)\n",
      "2023-10-07 11:13:09,490 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:09,491 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 26]), 25)\n",
      "2023-10-07 11:13:09,492 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:09,493 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:09,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:09,496 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:09,498 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:09,499 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:09,500 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:09,501 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,502 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:09,511 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:09,519 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,520 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,521 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,522 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:09,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:09,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:09,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:09,546 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,548 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,549 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,552 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:09,561 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:09,570 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,571 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,572 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,574 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:09,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:09,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:09,592 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:09,602 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,604 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,605 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,608 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:09,616 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:09,624 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,625 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,626 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,627 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,628 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:09,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:09,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:09,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:09,648 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,649 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,650 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,653 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:09,662 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:09,671 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,672 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,673 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,675 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:09,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:09,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:09,701 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:09,705 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,707 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,708 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:09,718 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:09,727 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,728 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,729 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,730 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:09,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:09,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:09,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:09,748 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,753 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,755 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,757 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:09,765 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:09,773 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,774 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,775 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,776 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:09,785 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:09,789 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:09,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:09,797 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,799 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,801 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:09,811 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:09,819 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,820 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,821 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,822 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:09,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:09,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:09,836 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:09,839 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,841 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,841 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,844 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:09,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:09,860 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,861 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,862 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,863 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:09,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:09,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:09,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:09,881 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,883 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,884 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,886 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:09,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:09,902 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,903 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,904 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,905 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:09,913 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:09,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:09,921 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:09,924 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,926 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:09,927 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:09,929 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:09,938 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:09,946 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:09,947 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,948 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:09,949 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:09,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:09,958 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:09,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:09,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:09,998 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:10,000 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:10,001 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,003 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:10,011 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:10,019 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,020 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,021 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,023 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:10,035 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:10,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:10,048 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:10,051 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:10,053 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:10,054 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,057 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:10,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:10,067 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,068 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,069 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,070 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:10,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:10,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:10,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:10,088 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))\n",
      "2023-10-07 11:13:10,097 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))\n",
      "2023-10-07 11:13:10,099 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,101 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:10,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:10,105 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,106 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:10,106 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,107 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:10,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:10,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:10,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:10,115 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:10,120 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:10,121 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:10,123 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,124 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:10,125 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:10,127 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,128 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:10,129 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,129 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:10,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:10,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:10,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:10,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:10,177 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:10,181 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:10,182 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,190 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:10,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:10,194 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:10,195 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:10,196 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:10,196 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:10,197 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:10,198 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:10,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:10,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:10,201 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:10,202 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:10,203 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,204 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:10,206 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:10,214 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 27]), 26)\n",
      "2023-10-07 11:13:10,215 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:10,216 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 27]), 26)\n",
      "2023-10-07 11:13:10,216 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:10,217 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:10,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:10,220 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:10,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:10,223 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:10,223 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:10,224 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,226 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:10,233 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:10,241 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,242 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,243 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,244 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,245 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:10,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:10,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:10,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:10,297 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,328 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,330 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:10,342 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:10,351 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,352 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,352 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,354 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,354 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:10,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:10,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:10,388 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:10,392 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,394 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,395 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,397 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:10,405 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:10,413 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,414 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,415 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,416 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:10,424 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:10,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:10,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:10,467 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,470 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,471 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,474 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:10,481 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:10,493 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,494 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,495 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,496 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,498 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:10,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:10,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:10,516 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:10,520 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,525 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,526 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,529 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:10,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:10,546 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,547 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,548 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,549 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,550 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:10,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:10,572 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:10,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:10,588 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,596 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,598 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,601 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:10,609 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:10,619 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,620 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,621 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,623 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,624 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:10,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:10,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:10,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:10,645 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,647 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,648 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,650 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:10,658 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:10,666 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,667 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,668 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,668 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,670 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:10,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:10,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:10,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:10,686 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,688 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,689 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,692 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:10,700 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:10,708 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,709 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,709 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,710 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,712 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:10,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:10,724 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:10,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:10,733 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,736 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,737 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,739 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:10,747 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:10,756 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,757 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,758 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,759 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:10,779 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:10,783 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:10,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:10,829 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,858 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,860 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:10,871 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:10,880 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,881 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,882 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,883 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:10,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:10,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:10,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:10,908 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,910 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,911 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,913 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:10,920 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:10,929 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,930 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,931 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,931 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,932 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:10,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:10,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:10,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:10,948 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,957 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,959 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:10,961 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:10,970 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:10,972 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:10,973 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,974 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:10,975 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:10,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:10,984 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:10,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:10,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:10,994 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,996 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))\n",
      "2023-10-07 11:13:10,997 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,000 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:11,002 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:11,003 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,004 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:11,005 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,005 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:11,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:11,008 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:11,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:11,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:11,015 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:11,016 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:11,017 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,018 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:11,020 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:11,021 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,022 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:11,023 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,024 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:11,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:11,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:11,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:11,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:11,091 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:11,107 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:11,108 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,122 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:11,124 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:11,126 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:11,127 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:11,128 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:11,129 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:11,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:11,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:11,132 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:11,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:11,134 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:11,135 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:11,136 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,137 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:11,139 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:11,147 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 28]), 27)\n",
      "2023-10-07 11:13:11,147 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:11,148 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 28]), 27)\n",
      "2023-10-07 11:13:11,150 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:11,150 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:11,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:11,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:11,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:11,155 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:11,156 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:11,157 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,158 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:11,166 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:11,174 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,175 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,175 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,176 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:11,184 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:11,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:11,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:11,206 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,212 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,214 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,217 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:11,225 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:11,233 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,234 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,235 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,236 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:11,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:11,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:11,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:11,261 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,263 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,264 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,266 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:11,274 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:11,283 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,283 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,285 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,286 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:11,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:11,298 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:11,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:11,313 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,364 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,365 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,368 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:11,376 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:11,387 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,388 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,389 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,391 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:11,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:11,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:11,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:11,416 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,419 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,419 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,423 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:11,431 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:11,439 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,440 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,441 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,442 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,442 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:11,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:11,462 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:11,465 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:11,471 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,473 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,474 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,477 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:11,485 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:11,493 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,494 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,495 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,496 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:11,506 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:11,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:11,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:11,528 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,530 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,530 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,535 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:11,544 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:11,552 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,553 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,554 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,554 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:11,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:11,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:11,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:11,576 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,578 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,579 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:11,590 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:11,598 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,599 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,600 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,600 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:11,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:11,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:11,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:11,615 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,620 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,621 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,623 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:11,632 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:11,640 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,641 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,641 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,642 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:11,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:11,656 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:11,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:11,666 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,669 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,669 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,672 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:11,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:11,688 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,688 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,690 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,690 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:11,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:11,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:11,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:11,715 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,717 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,718 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,720 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:11,727 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:11,736 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,736 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,737 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,738 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,739 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:11,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:11,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:11,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:11,755 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,757 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,757 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,760 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:11,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:11,769 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,770 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,771 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,772 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:11,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:11,783 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:11,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:11,796 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,798 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))\n",
      "2023-10-07 11:13:11,799 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,801 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:11,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:11,804 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,805 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:11,806 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,807 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:11,808 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:11,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:11,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:11,815 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:11,816 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:11,817 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:11,818 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,820 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:11,821 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:11,823 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,823 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:11,824 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,825 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:11,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:11,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:11,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:11,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:11,865 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:11,872 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:11,873 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,880 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:11,882 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:11,884 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:11,885 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:11,886 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:11,887 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:11,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:11,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:11,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:11,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:11,892 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:11,893 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:11,894 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,895 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:11,896 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:11,904 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 29]), 28)\n",
      "2023-10-07 11:13:11,905 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:11,906 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 29]), 28)\n",
      "2023-10-07 11:13:11,907 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:11,908 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:11,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:11,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:11,911 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:11,913 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:11,913 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:11,914 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:11,920 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:11,928 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:11,936 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:11,937 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,938 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:11,939 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:11,940 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:11,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:12,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:12,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:12,009 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,014 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,014 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,017 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:12,027 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:12,035 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,036 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,037 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,037 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:12,044 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:12,048 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:12,052 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:12,056 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,058 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,059 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,061 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:12,069 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:12,077 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,079 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,080 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,080 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:12,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:12,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:12,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:12,144 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,168 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,170 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,172 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:12,180 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:12,188 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,189 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,190 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,190 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:12,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:12,203 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:12,208 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:12,211 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,213 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,214 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,217 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:12,224 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:12,233 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,234 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,235 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,236 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:12,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:12,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:12,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:12,267 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,269 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,270 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,273 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:12,286 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:12,294 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,295 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,296 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,297 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,298 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:12,316 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:12,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:12,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:12,353 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,356 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,357 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:12,369 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:12,378 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,378 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,380 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,381 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:12,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:12,390 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:12,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:12,395 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,397 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,398 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,400 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:12,408 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:12,415 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,416 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,417 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,419 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,420 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:12,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:12,429 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:12,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:12,434 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,436 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,437 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,439 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:12,446 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:12,455 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,456 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,457 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,458 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:12,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:12,470 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:12,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:12,483 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,484 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,485 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,488 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:12,495 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:12,504 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,504 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,505 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,506 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:12,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:12,516 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:12,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:12,528 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,530 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,531 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,534 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:12,542 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:12,550 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,551 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,552 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,552 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:12,560 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:12,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:12,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:12,572 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,573 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,574 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,576 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:12,584 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:12,586 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,587 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,588 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,589 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:12,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:12,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:12,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:12,614 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,616 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))\n",
      "2023-10-07 11:13:12,617 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,619 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:12,621 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:12,622 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,623 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:12,624 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,626 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:12,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:12,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:12,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:12,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:12,635 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:12,638 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:12,638 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,640 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:12,642 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:12,643 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,644 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:12,645 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,645 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:12,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:12,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:12,668 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:12,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:12,686 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:12,688 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:12,689 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,701 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:12,703 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:12,705 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:12,705 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:12,706 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:12,707 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:12,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:12,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:12,710 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:12,711 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:12,712 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:12,713 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:12,714 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,716 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:12,717 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:12,725 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 30]), 29)\n",
      "2023-10-07 11:13:12,726 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:12,727 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 30]), 29)\n",
      "2023-10-07 11:13:12,728 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:12,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:12,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:12,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:12,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:12,734 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:12,735 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:12,736 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,737 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:12,745 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:12,753 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,754 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,755 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,756 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:12,768 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:12,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:12,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:12,798 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:12,803 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:12,804 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,807 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:12,815 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:12,823 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,824 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,825 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,826 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:12,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:12,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:12,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:12,847 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:12,848 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:12,849 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:12,860 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:12,869 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,870 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,871 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,872 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,872 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:12,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:12,885 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:12,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:12,891 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:12,893 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:12,894 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:12,896 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:12,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:12,921 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:12,923 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,925 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:12,927 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:12,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:12,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:12,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:13,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:13,010 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,012 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,013 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,015 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:13,023 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:13,032 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,033 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,034 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,034 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,035 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:13,040 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:13,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:13,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:13,055 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,057 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,058 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,060 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:13,073 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:13,085 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,086 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,087 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,088 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:13,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:13,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:13,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:13,115 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,117 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,118 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,120 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:13,129 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:13,137 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,138 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,139 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,140 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:13,146 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:13,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:13,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:13,155 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,163 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,164 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,166 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:13,175 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:13,183 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,184 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,185 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,186 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:13,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:13,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:13,205 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:13,210 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,214 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,215 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,217 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:13,225 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:13,234 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,234 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,236 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,236 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:13,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:13,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:13,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:13,266 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,267 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,268 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,271 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:13,279 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:13,287 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,288 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,289 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,290 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,291 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:13,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:13,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:13,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:13,307 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,308 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,309 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,312 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:13,320 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:13,328 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,329 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,330 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,331 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:13,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:13,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:13,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:13,351 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,353 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,353 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,356 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:13,364 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:13,366 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,367 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,368 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,368 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,369 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:13,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:13,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:13,450 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:13,453 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,455 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))\n",
      "2023-10-07 11:13:13,456 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,458 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:13,461 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:13,463 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,464 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:13,466 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,467 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:13,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:13,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:13,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:13,473 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:13,474 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:13,475 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:13,476 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,478 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:13,479 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:13,481 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,482 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:13,483 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,484 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:13,485 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:13,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:13,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:13,516 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:13,525 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:13,528 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:13,529 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:13,539 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:13,541 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:13,542 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:13,543 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:13,544 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:13,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:13,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:13,547 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:13,548 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:13,549 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:13,551 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:13,551 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,553 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:13,555 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:13,563 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 31]), 30)\n",
      "2023-10-07 11:13:13,564 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:13,565 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 31]), 30)\n",
      "2023-10-07 11:13:13,565 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:13,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:13,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:13,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:13,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:13,571 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:13,572 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:13,573 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,575 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:13,582 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:13,590 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,591 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,592 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,593 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:13,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:13,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:13,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:13,615 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:13,616 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:13,617 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,620 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:13,627 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:13,635 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,636 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,637 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,638 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:13,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:13,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:13,657 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:13,663 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:13,667 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:13,668 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:13,679 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:13,687 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,688 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,688 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,689 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:13,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:13,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:13,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:13,706 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:13,708 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:13,709 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:13,719 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:13,727 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,728 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,729 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,730 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:13,735 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:13,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:13,741 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:13,743 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:13,748 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:13,749 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,752 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:13,759 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:13,768 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,769 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,770 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,771 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,772 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:13,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:13,885 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:13,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:13,962 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:13,964 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:13,965 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:13,967 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:13,975 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:13,983 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:13,984 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,986 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:13,986 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:13,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:13,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:13,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:13,999 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:14,002 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,025 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,027 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,030 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:14,037 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:14,046 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,047 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,048 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,049 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:14,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:14,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:14,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:14,062 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,065 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,066 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,069 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:14,076 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:14,084 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,086 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,087 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,088 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:14,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:14,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:14,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:14,127 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,128 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,129 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:14,140 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:14,148 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,149 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,150 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,151 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:14,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:14,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:14,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:14,231 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,233 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,234 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,237 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:14,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:14,253 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,254 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,256 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,257 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:14,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:14,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:14,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:14,276 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,278 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,279 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,281 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:14,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:14,298 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,298 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,299 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,300 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:14,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:14,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:14,312 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:14,315 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,317 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,318 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,320 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:14,328 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:14,330 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,331 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,332 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,333 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,334 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:14,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:14,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:14,350 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:14,354 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,356 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))\n",
      "2023-10-07 11:13:14,357 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,360 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:14,362 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:14,364 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,365 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:14,366 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,367 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:14,368 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:14,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:14,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:14,376 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:14,378 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:14,379 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:14,380 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,381 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:14,383 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:14,384 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,385 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:14,386 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,387 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:14,388 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:14,402 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:14,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:14,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:14,426 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:14,428 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:14,429 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,437 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:14,439 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:14,440 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:14,441 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:14,442 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:14,443 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:14,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:14,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:14,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:14,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:14,448 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:14,449 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:14,450 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,451 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:14,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:14,461 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 32]), 31)\n",
      "2023-10-07 11:13:14,462 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:14,463 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 32]), 31)\n",
      "2023-10-07 11:13:14,463 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:14,464 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:14,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:14,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:14,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:14,469 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:14,470 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:14,471 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,472 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:14,480 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:14,488 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,489 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,490 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,491 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,492 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:14,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:14,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:14,517 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:14,521 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,523 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,524 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,527 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:14,535 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:14,543 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,544 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,545 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,546 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,547 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:14,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:14,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:14,567 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:14,570 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,571 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,572 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,574 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:14,582 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:14,591 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,601 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,603 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,603 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:14,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:14,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:14,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:14,644 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,646 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,647 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,649 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:14,658 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:14,666 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,667 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,669 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,670 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,670 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:14,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:14,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:14,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:14,683 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,702 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,704 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,707 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:14,715 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:14,724 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,725 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,726 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,727 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:14,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:14,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:14,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:14,822 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,825 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,826 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,828 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:14,836 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:14,844 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,845 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,847 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,847 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:14,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:14,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:14,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:14,870 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,872 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,873 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,876 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:14,884 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:14,892 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,893 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,894 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,894 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:14,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:14,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:14,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:14,916 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,918 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,918 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,921 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:14,929 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:14,937 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,938 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,939 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,940 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,941 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:14,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:14,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:14,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:14,957 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,959 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:14,960 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:14,962 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:14,970 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:14,979 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:14,980 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,981 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:14,982 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:14,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:15,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:15,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:15,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:15,080 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:15,082 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:15,083 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,085 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:15,093 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:15,102 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,103 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,104 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,105 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:15,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:15,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:15,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:15,126 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:15,128 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:15,128 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:15,140 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:15,149 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,150 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,151 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,152 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:15,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:15,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:15,165 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:15,169 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:15,171 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:15,171 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,174 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:15,182 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:15,184 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,185 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,186 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,187 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:15,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:15,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:15,208 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:15,214 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))\n",
      "2023-10-07 11:13:15,216 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))\n",
      "2023-10-07 11:13:15,217 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,219 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:15,222 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:15,223 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,224 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:15,225 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,226 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:15,226 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:15,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:15,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:15,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:15,231 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:15,232 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:15,234 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,236 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:15,237 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:15,238 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,240 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:15,241 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,252 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:15,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:15,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:15,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:15,313 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:15,325 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:15,342 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:15,345 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,367 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:15,369 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:15,371 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:15,371 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:15,372 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:15,373 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:15,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:15,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:15,376 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:15,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:15,378 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:15,379 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:15,380 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,381 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:15,383 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:15,391 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 33]), 32)\n",
      "2023-10-07 11:13:15,392 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:15,393 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 33]), 32)\n",
      "2023-10-07 11:13:15,394 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:15,395 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:15,396 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:15,398 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:15,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:15,400 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:15,401 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:15,402 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,403 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:15,411 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:15,419 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,420 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,421 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,422 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:15,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:15,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:15,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:15,452 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,454 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,456 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,458 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:15,466 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:15,474 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,476 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,477 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,478 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:15,492 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:15,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:15,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:15,511 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,518 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,519 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:15,530 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:15,539 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,540 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,541 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,542 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,543 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:15,550 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:15,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:15,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:15,565 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,567 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,568 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,571 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:15,579 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:15,587 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,588 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,590 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,590 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:15,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:15,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:15,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:15,608 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,610 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,612 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,615 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:15,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:15,635 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,635 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,637 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,638 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:15,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:15,651 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:15,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:15,658 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,660 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,661 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,663 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:15,672 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:15,681 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,681 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,682 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,683 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,684 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:15,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:15,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:15,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:15,710 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,711 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,712 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,714 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:15,722 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:15,731 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,731 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,733 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,733 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,734 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:15,739 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:15,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:15,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:15,753 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,767 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,768 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,771 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:15,779 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:15,787 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,788 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,789 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,790 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:15,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:15,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:15,804 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:15,808 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,809 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,810 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,813 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:15,821 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:15,830 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,831 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,832 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,833 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:15,840 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:15,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:15,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:15,852 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,853 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,854 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,857 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:15,865 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:15,876 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,877 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,879 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,879 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,881 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:15,885 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:15,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:15,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:15,899 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,901 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,902 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:15,912 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:15,920 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,921 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,922 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,923 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:15,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:15,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:15,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:15,954 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,956 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:15,957 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:15,962 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:15,972 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:15,974 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:15,975 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,976 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:15,977 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:15,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:15,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:15,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:16,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:16,016 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))\n",
      "2023-10-07 11:13:16,017 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))\n",
      "2023-10-07 11:13:16,018 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,020 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:16,022 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:16,024 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,024 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:16,025 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,027 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:16,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:16,029 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:16,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:16,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:16,034 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:16,036 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:16,036 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,039 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:16,040 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:16,042 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,042 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:16,044 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,044 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:16,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:16,058 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:16,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:16,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:16,088 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:16,092 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:16,093 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,102 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:16,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:16,105 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:16,106 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:16,107 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:16,108 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:16,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:16,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:16,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:16,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:16,113 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:16,114 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:16,115 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,116 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:16,118 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:16,126 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 34]), 33)\n",
      "2023-10-07 11:13:16,127 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:16,128 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 34]), 33)\n",
      "2023-10-07 11:13:16,129 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:16,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:16,132 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:16,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:16,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:16,135 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:16,136 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:16,137 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,139 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:16,147 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:16,155 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,156 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,157 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,158 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:16,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:16,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:16,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:16,241 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,243 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,244 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,247 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:16,255 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:16,263 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,264 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,265 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,266 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:16,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:16,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:16,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:16,286 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,293 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,295 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,298 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:16,307 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:16,316 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,317 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,318 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,319 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:16,327 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:16,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:16,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:16,335 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,336 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,337 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,340 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:16,348 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:16,355 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,356 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,357 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,358 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:16,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:16,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:16,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:16,433 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,435 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,436 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,439 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:16,447 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:16,456 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,457 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,458 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,459 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:16,464 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:16,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:16,469 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:16,472 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,475 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,476 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,479 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:16,487 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:16,496 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,497 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,498 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,499 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:16,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:16,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:16,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:16,516 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,523 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,524 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,526 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:16,535 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:16,544 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,544 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,545 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,546 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,547 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:16,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:16,607 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:16,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:16,662 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,664 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,665 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,668 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:16,676 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:16,690 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,692 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,694 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,695 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:16,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:16,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:16,765 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:16,770 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,782 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,784 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,788 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:16,800 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:16,810 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,811 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,812 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,812 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:16,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:16,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:16,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:16,836 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,838 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,839 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,841 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:16,849 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:16,857 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,858 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,859 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,860 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:16,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:16,874 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:16,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:16,894 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,896 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,897 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,899 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:16,907 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:16,914 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,915 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,916 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,917 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:16,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:16,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:16,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:16,940 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,942 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,942 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,945 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:16,953 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:16,955 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,956 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,957 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,958 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:16,959 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:16,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:16,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:16,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:16,979 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,982 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))\n",
      "2023-10-07 11:13:16,983 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:16,986 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:16,988 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:16,989 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:16,990 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:16,991 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:16,992 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:16,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:16,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:16,997 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:16,998 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:16,999 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:17,000 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:17,001 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,003 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:17,004 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:17,006 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,007 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:17,008 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,009 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:17,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:17,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:17,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:17,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:17,072 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:17,084 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:17,087 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,133 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:17,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:17,138 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:17,139 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:17,140 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:17,142 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:17,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:17,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:17,146 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:17,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:17,148 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:17,149 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:17,150 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,152 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:17,153 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:17,162 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 35]), 34)\n",
      "2023-10-07 11:13:17,162 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:17,163 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 35]), 34)\n",
      "2023-10-07 11:13:17,164 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:17,165 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:17,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:17,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:17,170 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:17,171 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:17,172 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:17,173 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,175 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:17,182 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:17,191 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,192 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,193 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,193 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:17,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:17,208 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:17,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:17,218 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,221 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,222 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,227 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:17,236 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:17,250 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,251 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,252 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,254 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:17,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:17,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:17,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:17,306 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,309 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,310 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,313 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:17,322 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:17,331 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,332 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,333 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,334 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:17,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:17,397 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:17,401 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:17,413 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,415 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,416 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,418 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:17,426 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:17,435 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,436 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,437 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,438 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:17,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:17,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:17,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:17,452 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,456 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,457 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,460 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:17,468 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:17,478 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,479 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,479 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,480 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:17,489 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:17,492 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:17,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:17,500 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,502 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,503 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,506 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:17,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:17,522 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,523 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,523 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,524 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,525 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:17,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:17,535 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:17,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:17,544 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,569 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,577 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,579 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:17,588 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:17,597 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,598 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,599 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,599 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:17,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:17,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:17,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:17,656 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,657 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,659 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,662 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:17,671 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:17,679 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,680 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,681 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,682 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:17,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:17,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:17,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:17,702 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,707 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,708 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,710 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:17,718 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:17,727 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,727 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,728 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,729 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:17,735 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:17,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:17,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:17,750 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,752 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,753 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,756 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:17,763 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:17,770 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,771 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,772 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,773 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:17,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:17,783 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:17,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:17,790 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,793 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,794 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,796 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:17,804 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:17,813 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,814 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,815 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,815 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:17,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:17,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:17,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:17,830 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,831 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,832 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,835 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:17,842 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:17,844 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,845 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,846 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,847 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:17,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:17,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:17,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:17,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:17,864 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,874 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))\n",
      "2023-10-07 11:13:17,875 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,878 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:17,880 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:17,881 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,882 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:17,883 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,884 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:17,885 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:17,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:17,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:17,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:17,897 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:17,899 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:17,899 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:17,902 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:17,904 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:17,905 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:17,905 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:17,906 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:17,907 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:17,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:17,926 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:17,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:17,947 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:17,950 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:17,951 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,959 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:17,961 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:17,962 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:17,963 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:17,964 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:17,965 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:17,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:17,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:17,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:17,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:17,970 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:17,971 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:17,972 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,974 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:17,975 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:17,984 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 36]), 35)\n",
      "2023-10-07 11:13:17,985 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:17,985 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 36]), 35)\n",
      "2023-10-07 11:13:17,987 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:17,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:17,989 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:17,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:17,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:17,993 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:17,994 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:17,994 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:17,996 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:18,005 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:18,013 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,014 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,015 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,017 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:18,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:18,028 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:18,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:18,060 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,062 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,063 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,067 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:18,076 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:18,084 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,085 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,087 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,088 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:18,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:18,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:18,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:18,116 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,118 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,119 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,122 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:18,130 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:18,138 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,139 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,140 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,141 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,142 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:18,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:18,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:18,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:18,165 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,167 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,168 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,170 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:18,179 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:18,193 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,194 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,196 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,197 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,198 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:18,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:18,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:18,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:18,236 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,242 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,245 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,248 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:18,256 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:18,266 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,268 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,269 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,270 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:18,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:18,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:18,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:18,294 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,300 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,302 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,304 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:18,312 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:18,320 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,321 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,322 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,323 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:18,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:18,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:18,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:18,355 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,381 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,383 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,386 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:18,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:18,404 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,405 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,406 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,407 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:18,440 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:18,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:18,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:18,476 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,482 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,483 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,486 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:18,494 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:18,502 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,503 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,504 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,505 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:18,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:18,550 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:18,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:18,557 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,559 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,560 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,562 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:18,571 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:18,581 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,583 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,584 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,585 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:18,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:18,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:18,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:18,607 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,610 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,611 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,614 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:18,622 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:18,631 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,632 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,633 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,634 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:18,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:18,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:18,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:18,654 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,667 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,668 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,671 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:18,679 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:18,688 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,689 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,690 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,691 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:18,699 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:18,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:18,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:18,717 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,719 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,720 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,723 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:18,731 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:18,733 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,734 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,735 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,736 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:18,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:18,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:18,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:18,762 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,764 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))\n",
      "2023-10-07 11:13:18,765 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:18,769 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:18,771 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,772 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:18,773 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,775 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:18,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:18,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:18,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:18,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:18,789 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:18,790 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:18,791 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,793 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:18,795 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:18,797 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,798 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:18,799 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,800 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:18,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:18,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:18,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:18,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:18,845 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:18,848 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:18,849 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,857 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:18,859 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:18,861 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:18,861 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:18,862 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:18,863 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:18,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:18,865 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:18,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:18,868 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:18,869 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:18,870 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:18,871 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,872 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:18,874 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:18,882 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 37]), 36)\n",
      "2023-10-07 11:13:18,883 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:18,884 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 37]), 36)\n",
      "2023-10-07 11:13:18,885 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:18,886 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:18,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:18,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:18,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:18,891 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:18,892 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:18,893 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:18,902 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:18,911 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,912 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,913 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,914 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,915 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:18,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:18,939 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:18,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:18,955 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:18,958 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:18,960 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:18,964 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:18,971 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:18,980 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:18,980 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,981 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:18,982 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:18,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:18,988 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:18,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:18,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:19,003 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,005 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,006 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,009 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:19,017 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:19,029 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,030 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,031 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,032 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:19,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:19,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:19,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:19,114 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,116 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,117 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,120 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:19,128 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:19,136 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,137 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,138 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,140 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:19,150 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:19,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:19,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:19,257 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,260 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,261 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,265 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:19,273 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:19,282 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,283 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,284 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,285 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:19,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:19,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:19,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:19,308 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,310 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,311 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,315 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:19,324 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:19,332 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,333 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,334 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,335 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:19,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:19,345 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:19,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:19,354 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,356 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,357 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:19,367 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:19,376 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,376 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,377 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,378 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:19,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:19,398 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:19,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:19,407 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,409 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,410 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,412 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:19,421 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:19,429 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,430 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,431 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,432 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:19,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:19,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:19,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:19,465 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,467 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,468 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,471 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:19,481 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:19,495 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,497 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,505 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,506 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:19,520 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:19,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:19,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:19,603 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,605 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,606 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,609 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:19,617 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:19,626 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,627 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,627 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,628 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:19,636 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:19,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:19,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:19,656 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,658 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,659 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,662 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:19,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:19,679 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,680 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,681 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,681 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:19,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:19,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:19,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:19,699 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,701 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,703 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,705 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:19,714 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:19,715 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,716 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,717 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,718 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:19,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:19,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:19,735 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:19,742 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,745 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))\n",
      "2023-10-07 11:13:19,747 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,749 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:19,751 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:19,753 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,754 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:19,754 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,755 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:19,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:19,759 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:19,762 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:19,767 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:19,772 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:19,773 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:19,774 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,776 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:19,777 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:19,779 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,780 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:19,781 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,782 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:19,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:19,794 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:19,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:19,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:19,822 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:19,827 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:19,829 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:19,848 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:19,850 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:19,851 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:19,852 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:19,853 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:19,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:19,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:19,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:19,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:19,858 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:19,858 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:19,859 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,861 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:19,863 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:19,872 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 38]), 37)\n",
      "2023-10-07 11:13:19,872 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:19,873 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 38]), 37)\n",
      "2023-10-07 11:13:19,874 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:19,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:19,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:19,877 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:19,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:19,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:19,881 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:19,881 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,883 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:19,892 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:19,901 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,902 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,903 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,903 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,904 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:19,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:19,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:19,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:19,934 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:19,936 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:19,937 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,940 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:19,949 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:19,957 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:19,958 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,959 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:19,960 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:19,961 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:19,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:19,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:19,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:19,990 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:19,992 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:19,993 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:19,997 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:20,005 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:20,013 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,014 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,015 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,017 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:20,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:20,105 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:20,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:20,122 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,128 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,129 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:20,140 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:20,149 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,150 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,151 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,152 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:20,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:20,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:20,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:20,172 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,174 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,175 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,177 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:20,185 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:20,194 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,195 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,196 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,197 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,198 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:20,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:20,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:20,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:20,217 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,219 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,220 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,224 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:20,233 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:20,241 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,242 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,243 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,244 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,245 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:20,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:20,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:20,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:20,264 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,309 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,312 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,314 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:20,322 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:20,331 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,332 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,333 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,333 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,334 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:20,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:20,344 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:20,363 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:20,373 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,375 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,376 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,379 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:20,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:20,395 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,396 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,397 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,398 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:20,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:20,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:20,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:20,491 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,494 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,495 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,498 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:20,505 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:20,514 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,515 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,516 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,517 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:20,530 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:20,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:20,543 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:20,547 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,549 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,550 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,553 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:20,561 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:20,570 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,571 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,572 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,572 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:20,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:20,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:20,596 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:20,601 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,604 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,605 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,608 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:20,619 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:20,629 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,630 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,631 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,632 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:20,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:20,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:20,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:20,706 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,727 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,730 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,733 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:20,741 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:20,743 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,744 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,746 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,747 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,748 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:20,758 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:20,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:20,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:20,787 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,797 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))\n",
      "2023-10-07 11:13:20,799 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:20,807 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:20,808 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,809 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:20,810 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,810 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:20,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:20,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:20,820 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:20,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:20,827 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:20,828 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:20,830 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:20,833 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:20,834 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,835 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:20,836 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,837 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:20,838 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:20,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:20,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:20,881 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:20,894 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:20,905 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:20,907 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,919 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:20,922 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:20,924 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)\n",
      "2023-10-07 11:13:20,925 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:20,926 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)\n",
      "2023-10-07 11:13:20,927 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:20,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-07 11:13:20,930 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-07 11:13:20,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-07 11:13:20,932 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-07 11:13:20,933 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:20,933 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:20,934 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,936 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-07 11:13:20,938 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:20,948 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 39]), 38)\n",
      "2023-10-07 11:13:20,949 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:20,950 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 39]), 38)\n",
      "2023-10-07 11:13:20,951 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:20,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-07 11:13:20,954 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-07 11:13:20,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-07 11:13:20,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-07 11:13:20,959 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:20,960 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:20,961 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-07 11:13:20,963 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-07 11:13:20,971 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:20,981 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:20,982 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,983 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:20,984 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:20,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n",
      "2023-10-07 11:13:20,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1\n",
      "2023-10-07 11:13:21,008 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2\n",
      "2023-10-07 11:13:21,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3\n",
      "2023-10-07 11:13:21,022 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,031 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,033 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,036 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-07 11:13:21,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:21,053 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,054 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,055 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,056 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0\n",
      "2023-10-07 11:13:21,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1\n",
      "2023-10-07 11:13:21,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2\n",
      "2023-10-07 11:13:21,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3\n",
      "2023-10-07 11:13:21,088 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,089 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,091 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,094 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-07 11:13:21,102 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:21,111 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,113 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,114 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,115 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0\n",
      "2023-10-07 11:13:21,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1\n",
      "2023-10-07 11:13:21,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2\n",
      "2023-10-07 11:13:21,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3\n",
      "2023-10-07 11:13:21,150 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,153 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,154 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,157 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-07 11:13:21,166 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:21,174 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,175 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,176 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,177 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0\n",
      "2023-10-07 11:13:21,182 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1\n",
      "2023-10-07 11:13:21,185 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2\n",
      "2023-10-07 11:13:21,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3\n",
      "2023-10-07 11:13:21,229 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,231 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,232 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,235 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-07 11:13:21,243 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:21,251 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,252 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,253 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,254 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0\n",
      "2023-10-07 11:13:21,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1\n",
      "2023-10-07 11:13:21,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2\n",
      "2023-10-07 11:13:21,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3\n",
      "2023-10-07 11:13:21,274 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,335 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,336 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,339 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-07 11:13:21,347 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:21,357 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,358 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,358 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,359 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0\n",
      "2023-10-07 11:13:21,366 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1\n",
      "2023-10-07 11:13:21,369 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2\n",
      "2023-10-07 11:13:21,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3\n",
      "2023-10-07 11:13:21,380 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,382 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,383 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,386 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-07 11:13:21,393 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:21,401 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,402 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,403 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,403 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0\n",
      "2023-10-07 11:13:21,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1\n",
      "2023-10-07 11:13:21,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2\n",
      "2023-10-07 11:13:21,421 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3\n",
      "2023-10-07 11:13:21,424 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,432 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,433 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,436 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-07 11:13:21,444 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:21,453 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,453 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,454 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,456 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0\n",
      "2023-10-07 11:13:21,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1\n",
      "2023-10-07 11:13:21,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2\n",
      "2023-10-07 11:13:21,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3\n",
      "2023-10-07 11:13:21,478 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,480 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,482 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,484 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-07 11:13:21,491 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:21,500 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,501 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,503 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,504 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0\n",
      "2023-10-07 11:13:21,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1\n",
      "2023-10-07 11:13:21,525 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2\n",
      "2023-10-07 11:13:21,567 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3\n",
      "2023-10-07 11:13:21,573 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,575 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,576 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,579 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-07 11:13:21,593 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:21,604 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,605 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,606 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,607 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0\n",
      "2023-10-07 11:13:21,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1\n",
      "2023-10-07 11:13:21,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2\n",
      "2023-10-07 11:13:21,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3\n",
      "2023-10-07 11:13:21,663 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,665 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,666 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,669 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-07 11:13:21,677 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:21,686 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,687 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,688 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,689 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0\n",
      "2023-10-07 11:13:21,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1\n",
      "2023-10-07 11:13:21,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2\n",
      "2023-10-07 11:13:21,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3\n",
      "2023-10-07 11:13:21,714 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,716 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,717 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,720 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-07 11:13:21,728 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:21,730 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,731 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,732 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,732 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}\n",
      "2023-10-07 11:13:21,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0\n",
      "2023-10-07 11:13:21,739 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1\n",
      "2023-10-07 11:13:21,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2\n",
      "2023-10-07 11:13:21,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3\n",
      "2023-10-07 11:13:21,748 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,756 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))\n",
      "2023-10-07 11:13:21,757 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,760 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-07 11:13:21,762 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:21,763 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,764 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:21,765 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,766 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:21,767 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0\n",
      "2023-10-07 11:13:21,769 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1\n",
      "2023-10-07 11:13:21,772 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2\n",
      "2023-10-07 11:13:21,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3\n",
      "2023-10-07 11:13:21,776 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])\n",
      "2023-10-07 11:13:21,777 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])\n",
      "2023-10-07 11:13:21,777 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,779 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-07 11:13:21,781 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-07 11:13:21,782 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)\n",
      "2023-10-07 11:13:21,783 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-07 11:13:21,784 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)\n",
      "2023-10-07 11:13:21,785 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}\n",
      "2023-10-07 11:13:21,785 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0\n",
      "2023-10-07 11:13:21,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1\n",
      "2023-10-07 11:13:21,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2\n",
      "2023-10-07 11:13:21,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3\n",
      "2023-10-07 11:13:21,825 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])\n",
      "2023-10-07 11:13:21,828 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])\n",
      "2023-10-07 11:13:21,829 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-07 11:13:21,839 [flexgen_test.py:30 in test_hf_gen] INFO - Who are you? Are you conscious? _Who_ are you?\n",
      "You know exactly what you are, the person who you *hate* who's trying to make you go insane by\n",
      "2023-10-07 11:13:21,839 [flexgen_test.py:31 in test_hf_gen] INFO - ----------\n",
      "2023-10-07 11:13:21,840 [flexgen_test.py:30 in test_hf_gen] INFO - Where is Deutschland?\n",
      "Wien.\n",
      "2023-10-07 11:13:21,841 [flexgen_test.py:31 in test_hf_gen] INFO - ----------\n",
      "2023-10-07 11:13:21,842 [flexgen_test.py:30 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?�️\n",
      "Huawei has revealed its latest device, the Huawei Mate 60 Pro series, in an official website and the announcement is quite good. You\n",
      "2023-10-07 11:13:21,843 [flexgen_test.py:31 in test_hf_gen] INFO - ----------\n",
      "2023-10-07 11:13:21,844 [flexgen_test.py:30 in test_hf_gen] INFO - Who are you? Are you conscious?\n",
      "I'm actually not aware at all that I am at all.\n",
      "2023-10-07 11:13:21,845 [flexgen_test.py:31 in test_hf_gen] INFO - ----------\n",
      "2023-10-07 11:13:21,845 [flexgen_test.py:30 in test_hf_gen] INFO - Where is Deutschland?\n",
      "I'd assume it is in Central Europe, they have the largest population of any country.\n",
      "2023-10-07 11:13:21,846 [flexgen_test.py:31 in test_hf_gen] INFO - ----------\n",
      "2023-10-07 11:13:21,847 [flexgen_test.py:30 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?: Everything you need to know\n",
      "Huawei is rolling out its new Mate 60 Pro soon.\n",
      "The US company’s flagship smartphone is known\n",
      "2023-10-07 11:13:21,848 [flexgen_test.py:31 in test_hf_gen] INFO - ----------\n",
      "2023-10-07 11:13:21,849 [flexgen_test.py:30 in test_hf_gen] INFO - Who are you? Are you conscious?�\n",
      "[Woman: Well, we're not. There's nothing you're going to remember.]\n",
      "[Mason: I'm just tired.\n",
      "2023-10-07 11:13:21,850 [flexgen_test.py:31 in test_hf_gen] INFO - ----------\n",
      "2023-10-07 11:13:21,851 [flexgen_test.py:30 in test_hf_gen] INFO - Where is Deutschland? \"Deutschland\"\n",
      "Das war ein paar Jahre ausgebracht nach der Tockevanufti\n",
      "2023-10-07 11:13:21,851 [flexgen_test.py:31 in test_hf_gen] INFO - ----------\n",
      "2023-10-07 11:13:21,860 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.\n",
      "2023-10-07 11:13:21,861 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.embed_positions from flexgen to old.\n",
      "2023-10-07 11:13:21,862 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.0 from flexgen to old.\n",
      "2023-10-07 11:13:21,863 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.1 from flexgen to old.\n",
      "2023-10-07 11:13:21,864 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.2 from flexgen to old.\n",
      "2023-10-07 11:13:21,865 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.3 from flexgen to old.\n",
      "2023-10-07 11:13:21,865 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.4 from flexgen to old.\n",
      "2023-10-07 11:13:21,866 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.5 from flexgen to old.\n",
      "2023-10-07 11:13:21,867 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.6 from flexgen to old.\n",
      "2023-10-07 11:13:21,869 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.7 from flexgen to old.\n",
      "2023-10-07 11:13:21,870 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.8 from flexgen to old.\n",
      "2023-10-07 11:13:21,871 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.9 from flexgen to old.\n",
      "2023-10-07 11:13:21,873 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.10 from flexgen to old.\n",
      "2023-10-07 11:13:21,874 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.11 from flexgen to old.\n",
      "2023-10-07 11:13:21,876 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.\n",
      "2023-10-07 11:13:21,877 [flexgen_forward.py:22 in to_old_forward] DEBUG - lm_head from flexgen to old.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with flexgen(model, policy):\n",
    "    test_hf_gen(model.checkpoint, model.model, policy.gpu_batch_size, policy.num_gpu_batches)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
