{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 07:43:04,047 [657238177.py:6 in <module>] DEBUG - Importing...\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-10-03 07:43:07.072552: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from flexgen_utils import logging, Policy, AttrDict\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.debug('Importing...')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import find_tied_parameters, named_module_tensors, set_module_tensor_to_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"facebook/opt-125m\" # 1.3b 6.7b 13b 30b 66b \n",
    "offload_folder = 'flexgen_offload/' + checkpoint.replace('/', '.')\n",
    "\n",
    "# empty model\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lm_head.weight', 'model.decoder.embed_tokens.weight']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_tied_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 07:29:25,550 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400\n",
      "2023-10-03 07:29:25,552 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000\n",
      "2023-10-03 07:29:25,555 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464\n",
      "2023-10-03 07:29:25,557 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592\n",
      "2023-10-03 07:29:25,559 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720\n",
      "2023-10-03 07:29:25,561 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848\n",
      "2023-10-03 07:29:25,563 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976\n",
      "2023-10-03 07:29:25,565 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104\n",
      "2023-10-03 07:29:25,567 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232\n",
      "2023-10-03 07:29:25,570 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360\n",
      "2023-10-03 07:29:25,572 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488\n",
      "2023-10-03 07:29:25,573 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616\n",
      "2023-10-03 07:29:25,575 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744\n",
      "2023-10-03 07:29:25,578 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872\n",
      "2023-10-03 07:29:25,580 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-10-03 07:29:25,581 [4043888472.py:116 in get_policy_weight_map] INFO - lm_head, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-10-03 07:29:25,582 [4043888472.py:120 in get_policy_weight_map] INFO - device_map is prepared!\n",
      "2023-10-03 07:29:25,585 [4043888472.py:126 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: \n",
      "GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=16, \n",
    "    num_gpu_batches=8, \n",
    "    weights_gpu_percent=0.0, \n",
    "    weights_cpu_percent=0.3, \n",
    "    cache_gpu_percent=0.0, \n",
    "    cache_cpu_percent=0.2, \n",
    "    act_gpu_percent=0.0, \n",
    "    act_cpu_percent=0.5, \n",
    "    overlap=True, \n",
    "    pin_weight=True,\n",
    ")\n",
    "\n",
    "def get_layers_dict(lm_model: Module, prefix: str='') -> dict:\n",
    "    # return a dict of {layer_name : layer_module ('meta')} with only leaf nodes & transformer layers\n",
    "    layers_dict = {}\n",
    "    for name, module in lm_model.named_children():\n",
    "        # leaf nodes\n",
    "        if len(list(module.named_children())) == 0:\n",
    "            layers_dict[prefix+name] = module\n",
    "        # ModuleList: transformer  \n",
    "        elif isinstance(module, ModuleList):\n",
    "            for block_name, block_module in module.named_children():\n",
    "                layers_dict[prefix+name+'.'+block_name] = block_module\n",
    "        else:\n",
    "            layers_dict.update(get_layers_dict(module, prefix+name+'.'))\n",
    "    return layers_dict\n",
    "\n",
    "def get_device(cur_percent, percents, choices):\n",
    "    # choose a device (gpu / cpu / disk) for a weight tensor by its percent of size\n",
    "    percents = np.cumsum(percents)\n",
    "    assert np.abs(percents[-1] - 1.0) < 1e-5, f'{percents}'\n",
    "\n",
    "    for i in range(len(percents)):\n",
    "        if cur_percent < percents[i]:\n",
    "            return choices[i]\n",
    "    return choices[-1]\n",
    "\n",
    "def get_policy_weight_map(model: PreTrainedModel, policy: Policy):\n",
    "    \"\"\"{module_name: device}\"\"\"\n",
    "    assert model.device == torch.device('meta'), 'model is not on device meta.'\n",
    "    \n",
    "    # to ensure the tied params are allocated to the same device in the weight_map\n",
    "    model.tie_weights()\n",
    "    tied_params = find_tied_parameters(model)\n",
    "\n",
    "    # layers to be scheduled\n",
    "    layers_dict = get_layers_dict(model)\n",
    "\n",
    "    # device assignment for each tensor in the model\n",
    "    weight_assign_dict = {}\n",
    "    devices = ['cuda', 'cpu', 'disk']\n",
    "    percents_target = np.array([\n",
    "        policy.weights_gpu_percent, \n",
    "        policy.weights_cpu_percent, \n",
    "        policy.weights_disk_percent\n",
    "    ])\n",
    "    \n",
    "    # model size (parameters + buffers), here we do not repeatly sum the tied paramters \n",
    "    size_total = sum(np.prod(tensor.shape) for _, tensor in named_module_tensors(model, include_buffers=True, recurse=True))\n",
    "    size_done, size_todo = 0, size_total\n",
    "    percents_done, percents_todo = 0 * percents_target, percents_target  \n",
    "\n",
    "    for layer_name, layer_module in layers_dict.items():\n",
    "        # current layer\n",
    "        tensor_sizes = [np.prod(tensor.shape) for _, tensor in named_module_tensors(layer_module, include_buffers=True, recurse=True)]\n",
    "        tensor_sizes_cumsum = np.cumsum(tensor_sizes)\n",
    "\n",
    "        device_allo_size_dict = {device: 0 for device in devices} # to balance the percents\n",
    "        for i, (tensor_name, tensor) in enumerate(named_module_tensors(layer_module, include_buffers=True, recurse=True)):\n",
    "            abs_tensor_name = layer_name + '.' + tensor_name\n",
    "\n",
    "            def find_processed_tied(abs_tensor_name, tied_params, weight_assign_dict):\n",
    "                # find the processed parameter (in weight_assign_dict) of the tied parameters.\n",
    "                for tp in tied_params:\n",
    "                    if abs_tensor_name in tp:\n",
    "                        for p in tp:\n",
    "                            if p in weight_assign_dict:\n",
    "                                return p, tuple(tp)\n",
    "                return None\n",
    "            \n",
    "            processed_tied = find_processed_tied(abs_tensor_name, tied_params, weight_assign_dict) \n",
    "            if processed_tied: # this tensor is tied and processed.\n",
    "                p, tp = processed_tied\n",
    "                weight_assign_dict[abs_tensor_name] = {\n",
    "                    # 'shape':  tensor.shape,\n",
    "                    'assigned_device': weight_assign_dict[p]['assigned_device'],\n",
    "                    'tied': tp\n",
    "                }\n",
    "            else:\n",
    "                mid_percent = (tensor_sizes_cumsum[i] - tensor_sizes[i] / 2) / tensor_sizes_cumsum[-1] # tensor mid size percent \n",
    "                device = get_device(mid_percent, percents_todo, devices)\n",
    "                weight_assign_dict[abs_tensor_name] = {\n",
    "                    'shape':  tensor.shape,\n",
    "                    'assigned_device': device\n",
    "                }\n",
    "                \n",
    "                device_allo_size_dict[device] += tensor_sizes[i]\n",
    "\n",
    "        # update percents_todo\n",
    "        size_layer = sum(device_allo_size_dict.values())\n",
    "        if size_layer > 0:\n",
    "            device_allo_percents = np.array([device_allo_size_dict[device] * 1. for device in devices]) / size_layer\n",
    "            percents_done = (percents_done * size_done + device_allo_percents * size_layer) / (size_done + size_layer)      \n",
    "        size_done += size_layer\n",
    "        size_todo -= size_layer\n",
    "        if size_todo > 0:\n",
    "            percents_todo = (size_total * percents_target - size_done * percents_done) / size_todo \n",
    "        \n",
    "        logging.info(f'{layer_name}, {percents_done}, size_todo: {size_todo}')\n",
    "\n",
    "\n",
    "    device_map = {k:v['assigned_device'] for k, v in weight_assign_dict.items()}\n",
    "    logging.info('device_map is prepared!')\n",
    "\n",
    "    mem_g = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if 'cuda' in v['assigned_device'] and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem_c = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if v['assigned_device'] == 'cpu' and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem_d = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if v['assigned_device'] == 'disk' and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem = mem_d + mem_c + mem_g\n",
    "    logging.info(f'CausalLM {checkpoint} is to be loaded on: ' \n",
    "                 f'\\nGPU Mem {mem_g:.2f} GiB ({mem_g / mem:.2%}), ' \n",
    "                 f'CPU Mem {mem_c:.2f} GiB ({mem_c / mem:.2%}), '\n",
    "                 f'Disk Mem {mem_d:.2f} Gib ({mem_d / mem:.2%})')\n",
    "    \n",
    "    # prepare output\n",
    "    output = {\n",
    "        'model': model,\n",
    "        'tied_params': tied_params,\n",
    "        'layers_dict': layers_dict,\n",
    "        'weight_assign_dict': weight_assign_dict,\n",
    "        'device_map': device_map\n",
    "    }\n",
    "    output = AttrDict(output)\n",
    "    return output\n",
    "\n",
    "output = get_policy_weight_map(model, policy)\n",
    "policy_device_map = output.device_map\n",
    "flexgen_layers = output.layers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 07:29:25,861 [1111749824.py:28 in <module>] INFO - The whole model has been downloaded an processed to offload_folder: 'offload/facebook.opt-125m'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 07:29:25,985 [1111749824.py:40 in <module>] INFO - Got empty CausalLM: 'facebook/opt-125m' on meta device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lm_head.weight', 'model.decoder.embed_tokens.weight']]\n"
     ]
    }
   ],
   "source": [
    "def check_disk(checkpoint, offload_folder):\n",
    "    if not os.path.isdir(offload_folder):\n",
    "        return False \n",
    "    \n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "    model.tie_weights()\n",
    "    tensor_names = [n for n, _ in named_module_tensors(model, include_buffers=True, recurse=True)]\n",
    "    dat_file_names = [file[:-4] for file in os.listdir(offload_folder) if file.endswith('.dat')]\n",
    "    # logging.info(set(tensor_names) - set(dat_file_names), set(dat_file_names) - set(tensor_names))\n",
    "    return len(set(tensor_names) - set(dat_file_names)) == 0\n",
    "\n",
    "if not check_disk(checkpoint, offload_folder):\n",
    "    # download and process to .dat files\n",
    "    disk_weight_map = {name:'disk' for name in policy_device_map}\n",
    "    try:\n",
    "        AutoModelForCausalLM.from_pretrained(\n",
    "            checkpoint, \n",
    "            device_map=disk_weight_map, \n",
    "            offload_folder=offload_folder, \n",
    "            offload_state_dict=True\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if check_disk(checkpoint, offload_folder):\n",
    "    logging.info(f'The whole model has been downloaded an processed to offload_folder: \\'{offload_folder}\\'')\n",
    "else:\n",
    "    err_msg = 'Mismatch between offload folder and model'\n",
    "    logging.error(err_msg)\n",
    "    raise RuntimeError(err_msg)\n",
    "\n",
    "# get empty model\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "model.tie_weights()\n",
    "model.eval()\n",
    "logging.info(f'Got empty CausalLM: \\'{checkpoint}\\' on meta device.')\n",
    "\n",
    "tied_params = find_tied_parameters(model)\n",
    "print(tied_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_from_name(lm_model, name):\n",
    "    splits = name.split('.')\n",
    "    module = lm_model\n",
    "    for split in splits:\n",
    "        if split == '': \n",
    "            continue \n",
    "\n",
    "        new_module = getattr(module, split)\n",
    "        if new_module is None:\n",
    "            raise ValueError(f\"{module} has no attribute {split}.\")\n",
    "        module = new_module\n",
    "    return module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model init: loading by policy...:   0%|          | 0/197 [00:00<?, ?it/s]/tmp/ipykernel_2241/3593259108.py:45: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tmp = torch.from_numpy(np_memmap).to(device)\n",
      "model init: loading by policy...: 100%|██████████| 197/197 [00:00<00:00, 3828.92it/s]\n",
      "2023-10-03 07:29:26,083 [3593259108.py:60 in policy_init] INFO - model has been loaded by policy.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm \n",
    "import gc \n",
    "\n",
    "dat_files = [f for f in os.listdir(offload_folder) if f.endswith('.dat')]\n",
    "with open(os.path.join(offload_folder, 'index.json'), 'r') as f:\n",
    "    index = json.load(f) # {name: {dtype, shape}}\n",
    "\n",
    "def get_tied_target(tensor_name):\n",
    "    # if tensor_name is tied and without a .dat file, if it is not tied, return itself\n",
    "    for group in tied_params:\n",
    "        if tensor_name in group:\n",
    "            for name in group:\n",
    "                if name + '.dat' in dat_files:\n",
    "                    return name \n",
    "    return tensor_name\n",
    "\n",
    "def flexgen_load_module_tensor(model, tensor_name, device):\n",
    "    tensor = get_obj_from_name(model, tensor_name)\n",
    "    if tensor.device == device:\n",
    "        return \n",
    "    \n",
    "    # else\n",
    "    old_tensor_name = tensor_name\n",
    "    \n",
    "    tensor_name = get_tied_target(tensor_name) \n",
    "    metadata = index[tensor_name]\n",
    "\n",
    "    # copied from accelerate.utils.offload\n",
    "    shape = tuple(metadata[\"shape\"])\n",
    "    if shape == ():\n",
    "        # NumPy memory-mapped arrays can't have 0 dims so it was saved as 1d tensor\n",
    "        shape = (1,)\n",
    "\n",
    "    dtype = metadata[\"dtype\"]\n",
    "    if dtype == \"bfloat16\":\n",
    "        # NumPy does not support bfloat16 so this was saved as a int16\n",
    "        dtype = \"int16\"\n",
    "    \n",
    "    # load .dat file\n",
    "    save_path = os.path.join(offload_folder, tensor_name + '.dat')\n",
    "\n",
    "    # to device \n",
    "    np_memmap = np.memmap(save_path, dtype=dtype, shape=shape, mode='r') \n",
    "    tmp = torch.from_numpy(np_memmap).to(device) \n",
    "    set_module_tensor_to_device(model, old_tensor_name, device, tmp)\n",
    "\n",
    "def flexgen_offload_module_tensor(model, tensor_name):\n",
    "    tensor = get_obj_from_name(model, tensor_name)\n",
    "    device = policy_device_map[tensor_name]\n",
    "    device = device if device != 'disk' else 'meta' \n",
    "    if tensor.device != device:\n",
    "        set_module_tensor_to_device(model, tensor_name, device, tensor) # gtoc, ctog\n",
    "\n",
    "def policy_init(model, policy_device_map):\n",
    "    for tensor_name, device in tqdm(policy_device_map.items(), desc='model init: loading by policy...'):\n",
    "        if device != 'disk':\n",
    "            flexgen_load_module_tensor(model, tensor_name, device) \n",
    "\n",
    "    logging.info('model has been loaded by policy.')        \n",
    "\n",
    "policy_init(model, policy_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(obj): # recursive\n",
    "\n",
    "    if isinstance(obj, tuple):\n",
    "        return tuple(get_info(o) for o in obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k:get_info(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj.size()\n",
    "    else:\n",
    "        return type(obj)\n",
    "              \n",
    "num_gpu_batches = 6\n",
    "\n",
    "def get_kth_batch_inputs(inputs, k): # for both args, kwargs\n",
    "    if isinstance(inputs, tuple):\n",
    "        return tuple(get_kth_batch_inputs(inp, k) for inp in inputs)\n",
    "    elif isinstance(inputs, dict):\n",
    "        return {k:get_kth_batch_inputs(v, k) for k, v in inputs.items()}\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        block_size = inputs.size(0)\n",
    "        assert block_size % num_gpu_batches == 0\n",
    "\n",
    "        gpu_batch_size = block_size // num_gpu_batches\n",
    "        return inputs[k * gpu_batch_size:(k + 1) * gpu_batch_size]\n",
    "    else: # int \n",
    "        return inputs\n",
    "\n",
    "def concat_outputs(outputs, ): # concat K outputs to one output\n",
    "    ans = []\n",
    "    for elem in zip(*outputs):\n",
    "        if isinstance(elem[0], torch.Tensor):\n",
    "            ans.append(torch.cat(elem, dim=0))\n",
    "        elif isinstance(elem[0], tuple):\n",
    "            ans.append(tuple(concat_outputs(elem)))\n",
    "        else: # all the same\n",
    "            ans.append(elem[0])\n",
    "    return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aei', 'bfj', ['cgk', ['dhl']]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = ['a', 'b', ['c', ['d']]]\n",
    "l2 = ['e', 'f', ['g', ['h']]]\n",
    "l3 = ['i', 'j', ['k', ['l']]]\n",
    "ls = [l1, l2, l3]\n",
    "def f(ls):\n",
    "    ans = []\n",
    "    for elem in zip(*ls):\n",
    "        if isinstance(elem[0], str):\n",
    "            ans.append(''.join(elem))\n",
    "        elif isinstance(elem[0], list):\n",
    "            ans.append(f(elem))\n",
    "        # print(elem)\n",
    "    return ans\n",
    "f(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.hooks import (\n",
    "    ModelHook, \n",
    "    SequentialHook, \n",
    "    add_hook_to_module, \n",
    "    remove_hook_from_module\n",
    ")\n",
    "# TODO: add_minibatch_hook, remove_minibatch_hook\n",
    "\n",
    "from accelerate.utils import (\n",
    "    find_device,\n",
    "    named_module_tensors,\n",
    "    send_to_device,\n",
    "    set_module_tensor_to_device,\n",
    ")\n",
    "\n",
    "# global buffers: {layer_name: value_holder}\n",
    "# weight_home = {}\n",
    "# weight_load_buf = {}\n",
    "\n",
    "act_home = {}\n",
    "act_load_buf = {}\n",
    "act_store_buf = {}\n",
    "\n",
    "kv_home = {} \n",
    "kv_load_buf = {}\n",
    "kv_store_buf = {}\n",
    "\n",
    "# TODO: cuda streams / cpu threads (?)\n",
    "\n",
    "\n",
    "from typing import Optional, Union, Mapping\n",
    "class LayerWeightHook(ModelHook):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model, \n",
    "        layer_name,\n",
    "        next_layer_name,\n",
    "        compute_device,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name\n",
    "        self.next_layer_name = next_layer_name\n",
    "        self.compute_device = compute_device\n",
    "\n",
    "        # get weight names\n",
    "        layer_module = get_obj_from_name(model, layer_name)\n",
    "        self.weight_names = [layer_name + '.' + name for name, _ in named_module_tensors(layer_module, True, True)]\n",
    "        dat_files = [os.path.join(offload_folder, get_tied_target(w) + '.dat') for w in self.weight_names]\n",
    "        assert all([self.check_dat(f) for f in dat_files]), f'dat file error, {dat_files}'\n",
    "        \n",
    "        if next_layer_name:\n",
    "            next_layer_module = get_obj_from_name(model, next_layer_name)\n",
    "            self.next_weight_names = [next_layer_name + '.' + name for name, _ in named_module_tensors(next_layer_module, True, True)]\n",
    "            dat_files = [os.path.join(offload_folder, get_tied_target(w) + '.dat') for w in self.next_weight_names]\n",
    "            assert all([self.check_dat(f) for f in dat_files]), f'dat file error, {dat_files}'\n",
    "\n",
    "        \n",
    "    def check_dat(self, dat_file):\n",
    "        return os.path.isfile(dat_file)\n",
    "\n",
    "    def init_hook(self, module):\n",
    "        return module \n",
    "\n",
    "    def load_layer(self, weight_names):\n",
    "        for w in weight_names:\n",
    "            flexgen_load_module_tensor(self.model, w, self.compute_device)\n",
    "\n",
    "    def offload_layer(self, weight_names):\n",
    "        for w in weight_names:\n",
    "            flexgen_offload_module_tensor(self.model, w)\n",
    "    \n",
    "    def pre_forward(self, module: Module, *args, **kwargs):\n",
    "        logging.info(f'pre {self.layer_name} forward,'\n",
    "                     f'\\n args:{get_info(args)},'\n",
    "                     f'\\n kwargs: {get_info(kwargs)}')\n",
    "\n",
    "\n",
    "        self.load_layer(self.weight_names) \n",
    "        if self.next_layer_name:\n",
    "            self.load_layer(self.next_weight_names) \n",
    "\n",
    "        return args, kwargs\n",
    "    \n",
    "    def post_forward(self, module, output):\n",
    "        \n",
    "        logging.info(f'post {self.layer_name} forward, \\noutput: {get_info(output)}\\n\\n')\n",
    "\n",
    "        self.offload_layer(self.weight_names)\n",
    "        return output\n",
    "    \n",
    "    def detach_hook(self, module):\n",
    "        return module \n",
    "\n",
    "import functools\n",
    "\n",
    "def add_minibatch_hook(module, num_gpu_batches):\n",
    "    if hasattr(module, \"_hf_hook\") and hasattr(module, \"_old_forward\"):\n",
    "        # If we already put some hook on this module, we replace it with the new one.\n",
    "        old_forward = module._old_forward\n",
    "    else:\n",
    "        old_forward = module.forward\n",
    "        module._old_forward = old_forward\n",
    "\n",
    "    @functools.wraps(old_forward)\n",
    "    def new_forward(*args, **kwargs):\n",
    "        args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs) # weights\n",
    "        if module._hf_hook.no_grad:\n",
    "            with torch.no_grad():\n",
    "                # output = old_forward(*args, **kwargs)\n",
    "                outputs = []\n",
    "                for k in range(num_gpu_batches):\n",
    "                    args_k = get_kth_batch_inputs(args, k)\n",
    "                    kwargs_k = get_kth_batch_inputs(kwargs, k)\n",
    "                    output = old_forward(*args_k, **kwargs_k)\n",
    "                    outputs.append(output) \n",
    "                \n",
    "                output = concat_outputs(outputs)\n",
    "                \n",
    "        else:\n",
    "            # output = old_forward(*args, **kwargs)\n",
    "            outputs = []\n",
    "            for k in range(num_gpu_batches):\n",
    "                args_k = get_kth_batch_inputs(args, k)\n",
    "                kwargs_k = get_kth_batch_inputs(kwargs, k)\n",
    "                output = old_forward(*args_k, **kwargs_k)\n",
    "                outputs.append(output) \n",
    "            \n",
    "            output = concat_outputs(outputs)\n",
    "\n",
    "        return module._hf_hook.post_forward(module, output) # offload weights\n",
    "\n",
    "    module.forward = new_forward\n",
    "    return module  \n",
    "\n",
    "def remove_minibatch_hook(module, recurse):\n",
    "    if hasattr(module, \"_hf_hook\"):\n",
    "        module._hf_hook.detach_hook(module)\n",
    "        delattr(module, \"_hf_hook\")\n",
    "\n",
    "    if hasattr(module, \"_old_forward\"):\n",
    "        module.forward = module._old_forward\n",
    "        delattr(module, \"_old_forward\")\n",
    "\n",
    "    if recurse:\n",
    "        for child in module.children():\n",
    "            remove_hook_from_module(child, recurse)\n",
    "\n",
    "    return module\n",
    "\n",
    "class LayerActHook(ModelHook): pass \n",
    "class LayerKVCacheHook(ModelHook): pass \n",
    "\n",
    "def clear_hooks(model):\n",
    "    remove_hook_from_module(model, recurse=True)\n",
    "\n",
    "\n",
    "clear_hooks(model)\n",
    "\n",
    "compute_device = 'cpu' \n",
    "\n",
    "layer_names = list(flexgen_layers)\n",
    "for i, layer_name in enumerate(layer_names): # layer names\n",
    "    layer_module = get_obj_from_name(model, layer_name)\n",
    "    next_layer_name = layer_names[i + 1] if i < len(layer_names) - 1 else None \n",
    "\n",
    "    layer_weight_hook = LayerWeightHook(\n",
    "        model=model, layer_name=layer_name, next_layer_name=next_layer_name, compute_device=compute_device)\n",
    "    add_hook_to_module(layer_module, layer_weight_hook, append=True)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1535: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "2023-10-03 07:29:26,492 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 10]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:26,504 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 10, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:26,519 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 10]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:26,528 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 10, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:26,531 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:26,766 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:26,771 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:26,911 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:26,916 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:27,059 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:27,063 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:27,187 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:27,191 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:27,308 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:27,312 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:27,432 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:27,436 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:27,549 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:27,555 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:27,670 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:27,674 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:27,792 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:27,796 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:27,913 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:27,917 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,056 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,060 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,179 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,182 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:28,190 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 10, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,192 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:28,379 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 10, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,447 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:28,450 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,452 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 11]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:28,454 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,455 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,473 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,478 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,494 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,500 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,513 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,517 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,530 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,534 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,549 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,554 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,569 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,574 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,589 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,593 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,605 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,608 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,620 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,625 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,636 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,640 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,652 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,656 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,665 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,668 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:28,676 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,677 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:28,700 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,723 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:28,726 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,727 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 12]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:28,729 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,731 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,746 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,750 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,763 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,767 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,780 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,784 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,800 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,803 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,817 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,821 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,833 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,839 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,882 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,887 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,901 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,905 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,933 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,938 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,956 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,960 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:28,987 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:28,991 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,000 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,004 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,011 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,013 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,030 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,042 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,045 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,047 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 13]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,049 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,050 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,064 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,068 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,081 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,085 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,099 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,103 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,118 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,122 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,137 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,141 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,153 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,156 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,170 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,174 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,186 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,192 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,214 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,219 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,239 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,245 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,267 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,270 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,280 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,283 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,289 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,291 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,311 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,326 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,328 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,330 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 14]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,332 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,333 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,346 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,350 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,363 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,367 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,383 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,386 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,399 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,403 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,416 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,420 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,432 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,435 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,447 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,450 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,464 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,467 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,479 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,482 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,496 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,500 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,515 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,521 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,531 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,536 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,543 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,544 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,563 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,576 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,579 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,580 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 15]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,583 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,584 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,599 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,603 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,620 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,623 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,636 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,641 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,658 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,662 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,681 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,685 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,699 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,704 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,722 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,726 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,738 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,741 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,754 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,758 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,771 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,774 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,786 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,790 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,800 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,803 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,809 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,811 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,828 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,840 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,842 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,844 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 16]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:29,845 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,847 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,860 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,865 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,879 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,882 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,894 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,898 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,909 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,912 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,924 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,928 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,940 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,944 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,958 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,962 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,982 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:29,986 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:29,999 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,003 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,020 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,023 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,036 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,039 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,048 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,051 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,057 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,058 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,072 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,082 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,085 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,086 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 17]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,088 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,089 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,101 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,105 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,119 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,122 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,134 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,139 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,151 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,154 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,167 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,170 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,182 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,185 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,200 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,204 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,216 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,220 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,232 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,235 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,249 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,252 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,264 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,267 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,278 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,281 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,287 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,289 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,305 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,319 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,321 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,323 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 18]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,325 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,326 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,342 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,346 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,360 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,363 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,375 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,379 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,391 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,396 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,409 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,413 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,426 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,432 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,462 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,466 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,480 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,483 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,497 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,501 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,514 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,518 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,530 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,534 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,547 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,552 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,557 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,559 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,570 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,635 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,637 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,639 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 19]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,642 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,643 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,663 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,667 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,681 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,685 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,698 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,702 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,715 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,719 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,733 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,737 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,749 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,752 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,766 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,771 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,785 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,789 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,803 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,806 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,820 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,824 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,837 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,841 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,850 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,854 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,860 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,862 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,882 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,893 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,896 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,898 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 20]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:30,899 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,901 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,915 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,918 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,931 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,934 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,947 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,951 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,963 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,967 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,980 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:30,983 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:30,997 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,000 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,013 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,017 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,030 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,034 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,049 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,053 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,066 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,069 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,082 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,085 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,094 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,098 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,104 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,106 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,126 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,141 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,143 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,145 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 21]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,147 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,149 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,165 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,169 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,182 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,186 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,201 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,205 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,218 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,223 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,238 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,241 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,253 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,256 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,268 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,272 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,339 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,344 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,363 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,367 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,380 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,383 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,396 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,400 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,410 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,414 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,421 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,424 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,442 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,455 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,457 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,459 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 22]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,461 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,462 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,475 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,479 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,492 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,496 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,509 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,513 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,525 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,529 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,543 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,547 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,563 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,568 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,582 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,586 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,602 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,605 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,619 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,623 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,637 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,641 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,654 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,658 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,668 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,671 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,677 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,678 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,692 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,703 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,706 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,707 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 23]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,709 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,711 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,725 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,728 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,743 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,747 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,759 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,762 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,774 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,778 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,790 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,795 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,809 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,813 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,827 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,830 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,843 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,846 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,876 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,879 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,894 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,897 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,910 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,914 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,922 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,925 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,931 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,933 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,945 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,958 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,961 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,963 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 24]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:31,965 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,966 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,980 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,983 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:31,996 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:31,999 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,015 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,018 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,032 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,035 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,050 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,054 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,069 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,073 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,087 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,091 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,110 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,114 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,128 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,132 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,147 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,150 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,164 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,167 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,179 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,183 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,189 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,192 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,209 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,220 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,222 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,224 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 25]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,227 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,228 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,241 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,248 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,263 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,267 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,279 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,284 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,297 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,300 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,314 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,318 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,371 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,376 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,403 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,407 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,430 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,433 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,447 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,451 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,464 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,469 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,483 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,486 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,496 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,501 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,508 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,510 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,529 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,539 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,542 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,543 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 26]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,545 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,547 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,558 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,561 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,573 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,577 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,590 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,594 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,607 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,611 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,637 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,640 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,656 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,660 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,677 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,681 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,697 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,700 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,714 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,717 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,736 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,740 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,754 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,757 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,766 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,770 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,776 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,778 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,799 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,876 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,878 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,880 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 27]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:32,882 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,883 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,897 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,901 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,915 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,920 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,934 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,938 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,952 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,955 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,968 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,972 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:32,988 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:32,992 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,005 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,009 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,021 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,024 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,037 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,041 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,053 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,057 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,070 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,073 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,082 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,085 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,091 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,093 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,109 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,132 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,135 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,136 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 28]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,138 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,140 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,154 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,157 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,170 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,174 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,187 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,192 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,210 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,215 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,231 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,235 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,248 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,251 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,268 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,273 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,288 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,291 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,305 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,310 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,330 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,333 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,348 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,352 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,361 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,364 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,372 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,375 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,386 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,408 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,411 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,412 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 29]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,414 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,416 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,430 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,434 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,448 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,452 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,465 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,468 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,481 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,485 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,497 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,501 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,517 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,522 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,536 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,540 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,558 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,564 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,578 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,581 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,594 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,598 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,616 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,619 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,635 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,638 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,644 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,646 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,668 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,679 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,681 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,683 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 30]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,685 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,686 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,699 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,704 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,716 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,721 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,734 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,737 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,750 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,753 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,768 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,771 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,787 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,791 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,805 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,809 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,821 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,824 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,841 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,844 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,857 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,861 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,874 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,878 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,894 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,898 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,904 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,907 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,919 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,928 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,930 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,933 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 31]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:33,935 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,936 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,948 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,952 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,965 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,968 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,981 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:33,984 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:33,997 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,001 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,014 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,018 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,031 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,034 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,048 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,051 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,062 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,065 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,085 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,088 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,100 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,102 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,118 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,122 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,141 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,146 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,152 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,153 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,168 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,182 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,184 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,186 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 32]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,188 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,190 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,203 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,207 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,268 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,272 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,284 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,288 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,301 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,304 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,317 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,321 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,335 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,338 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,351 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,355 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,375 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,379 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,392 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,395 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,412 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,415 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,428 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,431 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,441 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,444 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,450 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,451 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,472 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,482 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,484 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,485 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 33]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,487 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,489 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,500 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,503 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,516 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,520 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,535 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,539 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,564 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,567 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,583 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,587 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,601 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,604 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,618 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,621 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,636 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,640 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,723 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,728 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,742 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,745 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,757 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,760 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,777 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,780 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,786 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,788 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,804 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,823 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,826 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,827 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 34]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:34,830 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,831 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,844 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,848 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,861 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,865 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,877 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,881 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,893 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,896 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,909 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,911 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,923 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,926 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,941 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,944 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,957 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,961 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,975 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,978 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:34,993 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:34,996 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,009 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,013 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,023 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,027 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,032 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,034 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,051 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,065 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,068 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,070 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 35]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,072 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,073 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,089 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,093 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,106 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,110 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,124 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,128 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,141 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,146 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,163 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,167 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,184 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,188 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,212 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,216 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,302 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,305 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,318 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,322 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,336 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,340 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,354 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,357 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,367 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,370 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,376 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,377 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,393 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,406 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,409 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,410 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 36]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,412 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,414 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,426 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,430 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,444 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,448 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,461 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,464 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,476 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,480 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,493 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,497 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,511 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,514 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,537 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,542 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,559 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,563 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,575 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,579 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,597 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,601 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,619 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,623 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,633 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,637 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,643 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,644 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,658 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,667 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,669 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,671 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 37]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,673 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,674 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,687 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,690 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,703 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,707 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,721 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,725 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,741 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,745 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,762 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,765 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,778 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,781 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,871 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,876 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,889 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,892 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,906 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,910 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,923 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,926 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,940 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,944 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:35,955 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,958 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,964 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,965 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,980 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,994 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:35,997 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:35,999 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 38]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:36,001 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,003 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,018 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,023 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,038 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,042 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,064 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,067 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,081 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,084 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,098 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,101 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,114 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,117 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,130 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,133 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,148 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,151 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,164 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,168 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,180 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,184 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,197 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,200 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,210 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,214 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:36,221 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,223 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:36,243 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,253 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:36,255 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,257 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 39]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:36,259 [1685238530.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,260 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,272 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,277 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,292 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,295 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,308 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,311 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,323 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,327 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,340 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,344 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,359 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,362 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,373 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,376 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,391 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,395 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,480 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,484 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,496 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,499 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,511 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,514 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-10-03 07:29:36,539 [1685238530.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,542 [1685238530.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:36,548 [1685238530.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,550 [1685238530.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-10-03 07:29:36,578 [1685238530.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-10-03 07:29:36,623 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?1. What is your name? I am\n",
      "\n",
      "2. The day we died? We do not know if you were conscious and therefore not\n",
      "\n",
      "2023-10-03 07:29:36,624 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,625 [2210540570.py:26 in <module>] INFO - Where is Deutschland?  No mention of us, I mean all of it!\n",
      "Führen immer hoch, die wirklich nicht soll\n",
      "2023-10-03 07:29:36,627 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,627 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro??\n",
      "Huawei Mate 60 Pro (TigerView) is a new smartphone made by Huawei. The company has made a smartphone in India for over\n",
      "2023-10-03 07:29:36,628 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,629 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?ognchik.jpg \"Binh mi, toh mi me hace que haga, eres muy amor, ya est\n",
      "2023-10-03 07:29:36,630 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,631 [2210540570.py:26 in <module>] INFO - Where is Deutschland?\n",
      "In Germany.\n",
      "2023-10-03 07:29:36,632 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,633 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro?ANALYSIS BY TIMES\n",
      "The Huawei Mate 60 Pro series is now available for pre-order in China with more details to come. This\n",
      "2023-10-03 07:29:36,634 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,635 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?>  Who are you? Are you conscious?>  Who are you?\n",
      "I'm not aware of who you are\n",
      "Just curious.\n",
      "\n",
      "2023-10-03 07:29:36,636 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,637 [2210540570.py:26 in <module>] INFO - Where is Deutschland?””\n",
      "In the heart of the country.\n",
      "2023-10-03 07:29:36,638 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,639 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro?\n",
      "\n",
      "At first I thought that the Huawei Mate 60 Pro was a joke when compared to the rest of the Huawei Mate 20 line. Although I found\n",
      "2023-10-03 07:29:36,639 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,641 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?:\n",
      "Yes, I am. I am conscious at least with most humans.\n",
      "There are sooo many of us though!\n",
      "2023-10-03 07:29:36,642 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,643 [2210540570.py:26 in <module>] INFO - Where is Deutschland?\n",
      "Berlin in Germany.  I'm a really hard rock drummer and I don't know if it's just a German thing or if it's\n",
      "2023-10-03 07:29:36,644 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,645 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro?’s battery life is better than Mate 30 Pro? We’ll know more soon.\n",
      "Huawei’s new Mate 60 Pro\n",
      "2023-10-03 07:29:36,646 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,647 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?\n",
      "I don't even know what's in your head...\n",
      "You. I don't know what I know. I don't use computers anymore.\n",
      "2023-10-03 07:29:36,648 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,649 [2210540570.py:26 in <module>] INFO - Where is Deutschland? (Aberdeen)\n",
      "DUBLIN, the main province in Austria\n",
      "2023-10-03 07:29:36,650 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,651 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro? | iPhone 13\n",
      "Here we compared two midlevel Huawei Mate 60 Pro smartphones: the 6.21-inch Huawei Mate30 Pro with Qualcomm Snapdragon 625\n",
      "2023-10-03 07:29:36,652 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,653 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?3. Why do you wear blackface (or even if you do) and why do you wear glasses?4. Why do you use a large\n",
      "2023-10-03 07:29:36,654 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,655 [2210540570.py:26 in <module>] INFO - Where is Deutschland?: Berlin\n",
      "Germans in Berlin are usually around here, but they seem to be more of a north american accent.   Where is\n",
      "2023-10-03 07:29:36,656 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-10-03 07:29:36,658 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro?.  The cheapest Huawei Mate 60 Pro is $119.90, which is also a significant upgrade from what i have been seeing in my area and\n",
      "2023-10-03 07:29:36,658 [2210540570.py:27 in <module>] INFO - ----------\n"
     ]
    }
   ],
   "source": [
    "# generate test\n",
    "\n",
    "prompts = [\n",
    "    'Who are you? Are you conscious?',\n",
    "    'Where is Deutschland?',\n",
    "    'How is Huawei Mate 60 Pro?'\n",
    "] * 6\n",
    "\n",
    "prompt_len = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompts, padding=\"max_length\", max_length=prompt_len, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    max_length=30 + prompt_len,\n",
    "    # num_beams=2,\n",
    "    # num_beam_groups=2,\n",
    "    # diversity_penalty=0.1,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "output_texts = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "for output_text in output_texts:\n",
    "    logging.info(output_text)\n",
    "    logging.info('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args_type_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m args_type_set, kwargs_type_set, output_type_set\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args_type_set' is not defined"
     ]
    }
   ],
   "source": [
    "args_type_set, kwargs_type_set, output_type_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LayerWeightHook at 0x7ff14e56fb80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_module._hf_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load / offload\n",
    "#     module object, .dat file path\n",
    "#     layer: pre / post forward hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_activation_assignment(num_layers, offload_config: Policy):\n",
    "#     logging.debug(f\"<compute_activation_assignment> enter\")\n",
    "#     gpu_batch_limit = int(offload_config.num_gpu_batches * offload_config.act_gpu_percent)\n",
    "#     cpu_batch_limit = int(offload_config.num_gpu_batches * (offload_config.act_gpu_percent + offload_config.act_cpu_percent))\n",
    "#     logging.debug(f\"<compute_activation_assignment> gpu_batch_limit: {gpu_batch_limit}, cpu_batch_limit: {cpu_batch_limit}\")\n",
    "    \n",
    "#     act_assign_dict = {}\n",
    "#     for l in range(num_layers):\n",
    "#         for i in range(offload_config.num_gpu_batches):\n",
    "#             act_key = f\"layer.{l}_index.{i}\"\n",
    "#             if i < gpu_batch_limit:\n",
    "#                 device = 'cuda'\n",
    "#             elif i < cpu_batch_limit:\n",
    "#                 device = 'cpu'\n",
    "#             else:\n",
    "#                 device = 'disk'\n",
    "#             act_assign_dict[act_key]= {'assigned_device': device}\n",
    "#     return act_assign_dict\n",
    "\n",
    "\n",
    "# def compute_kv_cache_assignment(num_layers, offload_config: OffloadConfig):\n",
    "#     logging.debug(f\"<compute_kv_cache_assignment> enter\")\n",
    "#     gpu_batch_limit = int(offload_config.num_gpu_batches * offload_config.cache_gpu_percent)\n",
    "#     cpu_batch_limit = int(offload_config.num_gpu_batches * (offload_config.cache_gpu_percent + offload_config.cache_cpu_percent))\n",
    "#     logging.debug(f\"<compute_kv_cache_assignment> gpu_batch_limit: {gpu_batch_limit}, cpu_batch_limit: {cpu_batch_limit}\")\n",
    "    \n",
    "#     act_assign_dict = {}\n",
    "#     for l in range(num_layers):\n",
    "#         for i in range(offload_config.num_gpu_batches):\n",
    "#             key_cache_key = f\"key_layer.{l}_index.{i}\"\n",
    "#             value_cache_key = f\"key_layer.{l}_index.{i}\"\n",
    "#             if i < gpu_batch_limit:\n",
    "#                 device = 'cuda'\n",
    "#             elif i < cpu_batch_limit:\n",
    "#                 device = 'cpu'\n",
    "#             else:\n",
    "#                 device = 'disk'\n",
    "#             act_assign_dict[key_cache_key] = {'assigned_device': device}\n",
    "#             act_assign_dict[value_cache_key] = {'assigned_device': device}\n",
    "#     return act_assign_dict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
