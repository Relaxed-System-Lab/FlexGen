{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Policy, logging\n",
    "# from forward import flexgen\n",
    "from test import test_hf_gen\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "checkpoint = \"facebook/opt-125m\" # 125m 6.7b 13b 30b\n",
    "# checkpoint = \"Salesforce/codegen-350M-mono\"\n",
    "# checkpoint = 'bigscience/bloom-560m'\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=2, \n",
    "    num_gpu_batches=4, \n",
    "    weights_gpu_percent=0.0, \n",
    "    weights_cpu_percent=0.3, \n",
    "    cache_gpu_percent=0.0, \n",
    "    cache_cpu_percent=0.2, \n",
    "    act_gpu_percent=0.0, \n",
    "    act_cpu_percent=0.5, \n",
    "    overlap=True, \n",
    "    pin_weight=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-10-11 10:07:54,036 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp8tuytnca\n",
      "2023-10-11 10:07:54,038 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp8tuytnca/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "# forward.py: rewrite layer forward function\n",
    "\n",
    "import torch\n",
    "import functools \n",
    "import contextlib\n",
    "\n",
    "from minibatch import get_size_info, load_kth_batch_inputs, concat_outputs\n",
    "from utils import get_module_from_name\n",
    "\n",
    "\n",
    "def reset_forward(model, layer_name):        \n",
    "    layer = get_module_from_name(model, layer_name) \n",
    "\n",
    "    if hasattr(layer, \"_flexgen_old_forward\"):\n",
    "        layer.forward = layer._flexgen_old_forward\n",
    "        delattr(layer, \"_flexgen_old_forward\")\n",
    "        logger.debug(f'{layer_name} from flexgen to old.')\n",
    "\n",
    "    if hasattr(layer, \"_test_old_forward\"):\n",
    "        layer.forward = layer._test_old_forward\n",
    "        delattr(layer, \"_test_old_forward\")\n",
    "        logger.debug(f'{layer_name} from test to old.')\n",
    "\n",
    "def to_test_forward(mpl, layer_name, call_layer_log):\n",
    "    layer = get_module_from_name(mpl.model, layer_name) \n",
    "    compute_device = 'cpu' \n",
    "    layer._test_old_forward = old_forward = layer.forward \n",
    "\n",
    "    @functools.wraps(old_forward)\n",
    "    def new_forward(*args, **kwargs):\n",
    "        mpl.load_layer_weights(layer_name, compute_device) \n",
    "\n",
    "        call_layer_log.append(layer_name)  # \n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = old_forward(*args, **kwargs)\n",
    "\n",
    "        mpl.offload_layer_weights(layer_name)\n",
    "        return output\n",
    "\n",
    "    layer.forward = new_forward\n",
    "    logger.debug(f'{layer_name} to test forward') \n",
    "\n",
    "@contextlib.contextmanager\n",
    "def test(mpl, call_layer_log):\n",
    "    model = mpl.model\n",
    "    layer_names = mpl.layer_names\n",
    "\n",
    "    # test run to get layer calling order\n",
    "    for layer_name in layer_names:\n",
    "        to_test_forward(mpl, layer_name, call_layer_log)\n",
    "    yield \n",
    "    for layer_name in layer_names:\n",
    "        reset_forward(model, layer_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = torch.tensor([[[1],[2],[3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1]), tensor([2, 3])]\n",
      "[tensor([1]), tensor([2, 3])]\n",
      "[tensor([2]), tensor([4, 6])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Mapping, Tuple\n",
    "import numpy as np \n",
    "import os \n",
    "from math import floor\n",
    "\n",
    "class MixTensor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        mix_data: Tuple, \n",
    "        split_dim: int, \n",
    "        device: torch.device, \n",
    "        shape: torch.Size,\n",
    "        percents: Mapping[str, float],\n",
    "        file_path: str\n",
    "    ):\n",
    "        self.mix_data = mix_data\n",
    "        self.split_dim = split_dim \n",
    "        self.device = device \n",
    "        self.shape = shape \n",
    "        self.percents = percents\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def size(self):\n",
    "        return self.shape \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_split_dim(tensor):\n",
    "        dim_sizes = tensor.size()\n",
    "        max_dim, max_size = -1, -1\n",
    "        for dim, size in enumerate(dim_sizes):\n",
    "            if size > max_size:\n",
    "                max_size = size\n",
    "                max_dim = dim \n",
    "        return max_dim \n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_dim_slice(tensor, dim, dim_slice):\n",
    "        return tensor[(dim if dim >= 0 else dim + tensor.dim()) * (slice(None), ) + (dim_slice, )]\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_tensor(tensor, dim, percents):\n",
    "        dim_size = tensor.size(dim)\n",
    "        g_per, c_per, _ = [percents[dev] for dev in ['cuda', 'cpu', 'disk']]\n",
    "        \n",
    "        g_cut = floor(dim_size * g_per)\n",
    "        c_cut = floor(dim_size * (g_per + c_per))\n",
    "\n",
    "        g_data = MixTensor.tensor_dim_slice(tensor, dim, slice(0, g_cut))\n",
    "        c_data = MixTensor.tensor_dim_slice(tensor, dim, slice(g_cut, c_cut))\n",
    "        d_data = MixTensor.tensor_dim_slice(tensor, dim, slice(c_cut, dim_size))\n",
    "        return g_data, c_data, d_data \n",
    "\n",
    "    @classmethod\n",
    "    def from_tensor(\n",
    "        cls, \n",
    "        tensor: torch.Tensor, \n",
    "        percents: Mapping[str, float],\n",
    "        file_path: str \n",
    "    ):\n",
    "        split_dim = cls.get_split_dim(tensor) \n",
    "        device = tensor.device \n",
    "        shape = tensor.shape\n",
    "        \n",
    "        g_data, c_data, d_data = cls.split_tensor(tensor, split_dim, percents) \n",
    "        \n",
    "        g_data = g_data.to('cuda' if torch.cuda.is_available() else 'cpu') if g_data.numel() else None\n",
    "        c_data = c_data.to('cpu') if c_data.numel() else None\n",
    "        if d_data.numel():\n",
    "            d_data = d_data.cpu().numpy()\n",
    "            shape = d_data.shape\n",
    "            dtype = d_data.dtype \n",
    "\n",
    "            fp = np.memmap(file_path, mode=\"w+\", shape=shape, dtype=dtype)\n",
    "            fp[:] = d_data[:]\n",
    "            d_data = (shape, dtype)\n",
    "        else:\n",
    "            d_data = None \n",
    "        mix_data = (g_data, c_data, d_data)\n",
    "\n",
    "        return cls(\n",
    "            mix_data=mix_data,\n",
    "            split_dim=split_dim,\n",
    "            device=device,\n",
    "            shape=shape,\n",
    "            percents=percents,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    def to_tensor(self):\n",
    "        g_data, c_data, d_data = self.mix_data \n",
    "        compute_device = self.device \n",
    "\n",
    "        tensor = []\n",
    "        if g_data is not None:\n",
    "            if g_data.device != torch.device(compute_device):\n",
    "                g_data = g_data.to(compute_device) \n",
    "            tensor.append(g_data)\n",
    "        if c_data is not None:\n",
    "            if c_data.device != torch.device(compute_device):\n",
    "                c_data = c_data.to(compute_device) \n",
    "            tensor.append(c_data)\n",
    "        if d_data is not None:\n",
    "            (shape, dtype) = d_data \n",
    "            d_data = np.memmap(self.file_path, shape=shape, dtype=dtype, mode='r')\n",
    "            d_data = torch.from_numpy(d_data).to(compute_device)\n",
    "            tensor.append(d_data)\n",
    "            \n",
    "        tensor = torch.cat(tensor, dim=self.split_dim) \n",
    "        return tensor \n",
    "            \n",
    "\n",
    "    def __add__(self, mix_tensor):\n",
    "        assert self.shape == mix_tensor.shape and type(self) == type(mix_tensor) # is same shape mix tensor\n",
    "        res = self.to_tensor() + mix_tensor.to_tensor() \n",
    "        return self.from_tensor(res, self.percents, self.file_path)\n",
    "\n",
    "\n",
    "x = torch.tensor([1,2,3])\n",
    "m = MixTensor.from_tensor(x, percents={'cuda':0, 'cpu':0.5, 'disk':0.5}, file_path='test/m.dat')\n",
    "m2 = MixTensor.from_tensor(x, percents={'cuda':0, 'cpu':0.5, 'disk':0.5}, file_path='test/m2.dat')\n",
    "(m+m2).to_tensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def to_flexgen_forward(mpl, j, compute_device):\n",
    "    # rewrite the j-th layer's forward\n",
    "    layer_name = mpl.layer_names[j]\n",
    "    next_layer_name = mpl.layer_names[(j + 1) % len(mpl.layer_names)]\n",
    "\n",
    "    ngb = mpl.policy.num_gpu_batches\n",
    "\n",
    "    layer = get_module_from_name(mpl.model, layer_name)  \n",
    "    if hasattr(layer, \"_flexgen_old_forward\"): return  \n",
    "    \n",
    "    layer._flexgen_old_forward = old_forward = layer.forward \n",
    "\n",
    "    @functools.wraps(old_forward)\n",
    "    def new_forward(*args, **kwargs):\n",
    "        # pre fwd: load curr & next weights, TODO: cuda stream\n",
    "        mpl.load_layer_weights(layer_name, compute_device) \n",
    "        mpl.load_layer_weights(next_layer_name, compute_device) \n",
    "        \n",
    "        # loop forward pass of K minibatches, TODO: cuda stream\n",
    "        with torch.no_grad():\n",
    "            logger.debug(f'args: {get_size_info(args)}')\n",
    "            logger.debug(f'kwargs: {get_size_info(kwargs)}')\n",
    "            \n",
    "            outputs = []\n",
    "            for k in range(ngb):\n",
    "                logger.debug(f'layer: {layer_name}, batch: {k}')\n",
    "\n",
    "                # 'pre' fwd: load curr & next inputs (activations, KV cache), store & offload prev \n",
    "                args_k = load_kth_batch_inputs(args, k, ngb)\n",
    "                kwargs_k = load_kth_batch_inputs(kwargs, k, ngb)\n",
    "\n",
    "                # TODO: load args, kwargs to compute device\n",
    "\n",
    "                # the k-th fwd pass\n",
    "                output = old_forward(*args_k, **kwargs_k)\n",
    "\n",
    "                # TODO: 1) output: to mix, 2) args_k, kwargs_k: free\n",
    "\n",
    "\n",
    "                outputs.append(output) \n",
    "\n",
    "            output = concat_outputs(outputs)\n",
    "            logger.debug(f'outputs after concat: {get_size_info(output)}')                \n",
    "\n",
    "        # post fwd: free curr weights\n",
    "        mpl.offload_layer_weights(layer_name)\n",
    "        return output\n",
    "\n",
    "    layer.forward = new_forward\n",
    "    logger.debug(f'{layer_name} to flexgen forward')\n",
    "\n",
    "@contextlib.contextmanager \n",
    "def flexgen(checkpoint, policy):\n",
    "    # init model \n",
    "    from model import ModelPolicyLoader\n",
    "    mpl = ModelPolicyLoader(checkpoint, policy)\n",
    "    mpl.init_all_weights() # init \n",
    "\n",
    "    # test run, get layer order\n",
    "    call_layer_log = []\n",
    "    with test(mpl, call_layer_log):\n",
    "        from test import test_hf_gen\n",
    "        test_hf_gen(mpl.checkpoint, mpl.model, 1,1, prompts=['0'])\n",
    "\n",
    "    assert len(call_layer_log) == len(mpl.layer_names) and set(call_layer_log) == set(mpl.layer_names)\n",
    "    mpl.layer_names = call_layer_log\n",
    "\n",
    "    # rewrite layer forward\n",
    "    for j, _ in enumerate(mpl.layer_names):\n",
    "        compute_device = 'cpu'\n",
    "        to_flexgen_forward(mpl, j, compute_device)\n",
    "    yield mpl.model \n",
    "    for layer_name in mpl.layer_names:\n",
    "        reset_forward(mpl.model, layer_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with flexgen(checkpoint, policy) as model:\n",
    "    num_prompts = policy.gpu_batch_size * policy.num_gpu_batches\n",
    "    test_hf_gen(checkpoint, model, num_prompts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
