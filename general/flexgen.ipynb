{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-10-04 07:08:01,020 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmphh60a97l\n",
      "2023-10-04 07:08:01,022 [instantiator.py:76 in _write] INFO - Writing /tmp/tmphh60a97l/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module \n",
    "import functools \n",
    "\n",
    "from flexgen_utils import logging, Policy, get_module_from_name\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "from flexgen_init import policy_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 07:08:02,015 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2023-10-04 07:08:02,074 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-04 07:08:02.753724: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-04 07:08:03,729 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
      "2023-10-04 07:08:03,910 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5\n",
      "2023-10-04 07:08:03,912 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7\n",
      "2023-10-04 07:08:03,913 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5\n",
      "2023-10-04 07:08:03,913 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7\n",
      "2023-10-04 07:08:04,989 [flexgen_init.py:201 in get_policy_weight_map] INFO - device_map is prepared!\n",
      "2023-10-04 07:08:04,993 [flexgen_init.py:207 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: \n",
      "GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)\n",
      "2023-10-04 07:08:05,027 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-04 07:08:05,162 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-04 07:08:05,269 [flexgen_init.py:67 in policy_init] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'\n",
      "model init: loading by policy...:   0%|          | 0/197 [00:00<?, ?it/s]/home/fsuser/FlexGen/general/flexgen_utils/offload.py:41: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tmp = torch.from_numpy(np_memmap).to(device)\n",
      "model init: loading by policy...: 100%|██████████| 197/197 [00:00<00:00, 2644.03it/s]\n",
      "2023-10-04 07:08:05,353 [flexgen_init.py:79 in policy_init] INFO - model has been loaded by policy.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"facebook/opt-125m\"\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=8, \n",
    "    num_gpu_batches=4, \n",
    "    weights_gpu_percent=0.0, \n",
    "    weights_cpu_percent=0.3, \n",
    "    cache_gpu_percent=0.0, \n",
    "    cache_cpu_percent=0.2, \n",
    "    act_gpu_percent=0.0, \n",
    "    act_cpu_percent=0.5, \n",
    "    overlap=True, \n",
    "    pin_weight=True,\n",
    ")\n",
    "\n",
    "# for test\n",
    "gbs = policy.gpu_batch_size\n",
    "ngb = policy.num_gpu_batches\n",
    "num_prompts = ngb * gbs \n",
    "\n",
    "# model init\n",
    "output = policy_init(checkpoint, policy)\n",
    "\n",
    "model = output.model\n",
    "weight_map = output.weight_map\n",
    "layer_names = output.layer_names\n",
    "index = output.index\n",
    "dat_files = output.dat_files\n",
    "tied_params = output.tied_params\n",
    "offload_folder = output.offload_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from accelerate.utils import named_module_tensors \n",
    "from flexgen_utils import get_tied_target\n",
    "from flexgen_utils import flexgen_load_module_tensor, flexgen_offload_module_tensor\n",
    "\n",
    "def load_layer_weights(model, layer_name, compute_device, offload_folder, dat_files):\n",
    "    logger.debug(f'load_layer_weights: {layer_name} to {compute_device}')\n",
    "    layer_module = get_module_from_name(model, layer_name)\n",
    "    weight_names = [layer_name + '.' + name for name, _ in named_module_tensors(layer_module, True, True)]\n",
    "    layer_dat_files = [os.path.join(offload_folder, get_tied_target(w, tied_params, dat_files) + '.dat') for w in weight_names]\n",
    "    assert all([os.path.isfile(f) for f in layer_dat_files]), f'dat file error, {dat_files}'\n",
    "    \n",
    "    for w in weight_names:\n",
    "        flexgen_load_module_tensor(model, w, compute_device, index, offload_folder, tied_params)\n",
    "\n",
    "\n",
    "def offload_layer_weights(model, layer_name, weight_map):\n",
    "    logger.debug(f'offload_layer_weights: {layer_name}')\n",
    "    layer_module = get_module_from_name(model, layer_name)\n",
    "    weight_names = [layer_name + '.' + name for name, _ in named_module_tensors(layer_module, True, True)]\n",
    "    for w in weight_names:\n",
    "        flexgen_offload_module_tensor(model, w, weight_map) \n",
    "\n",
    "def get_info(obj): # recursive\n",
    "    if isinstance(obj, tuple):\n",
    "        return tuple(get_info(o) for o in obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k:get_info(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj.size()\n",
    "    else:\n",
    "        return type(obj)\n",
    "\n",
    "def get_kth_batch_inputs(inputs, k, gpu_batch_size): # for both args, kwargs\n",
    "    if isinstance(inputs, tuple):\n",
    "        return tuple(get_kth_batch_inputs(inp, k, gpu_batch_size) for inp in inputs)\n",
    "    elif isinstance(inputs, dict):\n",
    "        return {k:get_kth_batch_inputs(v, k, gpu_batch_size) for k, v in inputs.items()}\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        return inputs[k * gpu_batch_size:(k + 1) * gpu_batch_size]\n",
    "    else: # int \n",
    "        return inputs\n",
    "\n",
    "def concat_outputs(outputs, ): # concat K outputs to one output\n",
    "    ans = []\n",
    "    for elem in zip(*outputs):\n",
    "        if isinstance(elem[0], torch.Tensor):\n",
    "            ans.append(torch.cat(elem, dim=0))\n",
    "        elif isinstance(elem[0], tuple):\n",
    "            ans.append(tuple(concat_outputs(elem)))\n",
    "        else: # all the same\n",
    "            ans.append(elem[0])\n",
    "    return ans \n",
    "\n",
    "def to_flexgen_forward(model, layer_names, j, compute_device, weight_map, offload_folder):\n",
    "    # rewrite the j-th layer's forward\n",
    "    \n",
    "    layer_name = layer_names[j]\n",
    "    next_layer_name = layer_names[(j + 1) % len(layer_names)]\n",
    "\n",
    "    layer = get_module_from_name(model, layer_name)  \n",
    "    if hasattr(layer, \"_flexgen_old_forward\"): # has been rewriten\n",
    "        return layer \n",
    "    \n",
    "    logger.debug(f'{layer_name} to flexgen forward')\n",
    "    layer._flexgen_old_forward = old_forward = layer.forward \n",
    "\n",
    "    @functools.wraps(old_forward)\n",
    "    def new_forward(*args, **kwargs):\n",
    "        # pre fwd: load curr & next weights\n",
    "        load_layer_weights(model, layer_name, compute_device, offload_folder, dat_files)\n",
    "        load_layer_weights(model, next_layer_name, compute_device, offload_folder, dat_files)\n",
    "        \n",
    "        # loop forward pass of K minibatches\n",
    "        with torch.no_grad():\n",
    "            outputs = []\n",
    "            for k in range(ngb):\n",
    "                logger.debug(f'layer: {layer_name}, {k}-th gpu batch')\n",
    "                args_k = get_kth_batch_inputs(args, k, gbs)\n",
    "                kwargs_k = get_kth_batch_inputs(kwargs, k, gbs)\n",
    "                output = old_forward(*args_k, **kwargs_k)\n",
    "                outputs.append(output) \n",
    "            \n",
    "            output = concat_outputs(outputs)\n",
    "                \n",
    "\n",
    "        # post fwd: free curr weights\n",
    "        offload_layer_weights(model, layer_name, weight_map)\n",
    "        return output\n",
    "\n",
    "    layer.forward = new_forward\n",
    "    return layer\n",
    "\n",
    "def to_old_forward(model, layer_name):\n",
    "    layer = get_module_from_name(model, layer_name) \n",
    "\n",
    "    if hasattr(layer, \"_flexgen_old_forward\"):\n",
    "        layer.forward = layer._flexgen_old_forward\n",
    "        delattr(layer, \"_flexgen_old_forward\")\n",
    "    logger.debug(f'{layer_name} to old forward')\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 07:21:16,140 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen version forward\n",
      "2023-10-04 07:21:16,143 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen version forward\n",
      "2023-10-04 07:21:16,144 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen version forward\n",
      "2023-10-04 07:21:16,145 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen version forward\n",
      "2023-10-04 07:21:16,147 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen version forward\n",
      "2023-10-04 07:21:16,148 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen version forward\n",
      "2023-10-04 07:21:16,150 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen version forward\n",
      "2023-10-04 07:21:16,151 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen version forward\n",
      "2023-10-04 07:21:16,153 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen version forward\n",
      "2023-10-04 07:21:16,154 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen version forward\n",
      "2023-10-04 07:21:16,155 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen version forward\n",
      "2023-10-04 07:21:16,157 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen version forward\n",
      "2023-10-04 07:21:16,158 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen version forward\n",
      "2023-10-04 07:21:16,160 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen version forward\n",
      "2023-10-04 07:21:16,160 [3075400243.py:65 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen version forward\n",
      "2023-10-04 07:21:16,161 [3075400243.py:65 in to_flexgen_forward] DEBUG - lm_head to flexgen version forward\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# rewrite layers' forward\n",
    "layer_nums = len(layer_names)\n",
    "for j in range(layer_nums):\n",
    "    compute_device = 'cpu'\n",
    "    to_flexgen_forward(model, layer_names, j, compute_device, weight_map, offload_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 07:21:13,227 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.embed_tokens to old forward\n",
      "2023-10-04 07:21:13,229 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.embed_positions to old forward\n",
      "2023-10-04 07:21:13,230 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.final_layer_norm to old forward\n",
      "2023-10-04 07:21:13,231 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.0 to old forward\n",
      "2023-10-04 07:21:13,232 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.1 to old forward\n",
      "2023-10-04 07:21:13,233 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.2 to old forward\n",
      "2023-10-04 07:21:13,234 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.3 to old forward\n",
      "2023-10-04 07:21:13,235 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.4 to old forward\n",
      "2023-10-04 07:21:13,236 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.5 to old forward\n",
      "2023-10-04 07:21:13,237 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.6 to old forward\n",
      "2023-10-04 07:21:13,237 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.7 to old forward\n",
      "2023-10-04 07:21:13,238 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.8 to old forward\n",
      "2023-10-04 07:21:13,239 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.9 to old forward\n",
      "2023-10-04 07:21:13,240 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.10 to old forward\n",
      "2023-10-04 07:21:13,240 [3075400243.py:100 in to_old_forward] DEBUG - model.decoder.layers.11 to old forward\n",
      "2023-10-04 07:21:13,241 [3075400243.py:100 in to_old_forward] DEBUG - lm_head to old forward\n"
     ]
    }
   ],
   "source": [
    "for j in range(layer_nums):\n",
    "    to_old_forward(model, layer_names[j])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 07:21:32,146 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2023-10-04 07:21:32,352 [3075400243.py:7 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-04 07:21:32,355 [3075400243.py:7 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-04 07:21:32,356 [3075400243.py:78 in new_forward] DEBUG - layer: model.decoder.embed_tokens, 0-th gpu batch\n",
      "2023-10-04 07:21:32,359 [3075400243.py:78 in new_forward] DEBUG - layer: model.decoder.embed_tokens, 1-th gpu batch\n",
      "2023-10-04 07:21:32,363 [3075400243.py:78 in new_forward] DEBUG - layer: model.decoder.embed_tokens, 2-th gpu batch\n",
      "2023-10-04 07:21:32,366 [3075400243.py:78 in new_forward] DEBUG - layer: model.decoder.embed_tokens, 3-th gpu batch\n",
      "2023-10-04 07:21:32,370 [3075400243.py:18 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(prompts, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39mprompt_len, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Generate\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m generate_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     18\u001b[0m     inputs\u001b[39m.\u001b[39;49minput_ids, \n\u001b[1;32m     19\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m \u001b[39m+\u001b[39;49m prompt_len,\n\u001b[1;32m     20\u001b[0m     \u001b[39m# num_beams=2,\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39m# num_beam_groups=2,\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m     \u001b[39m# diversity_penalty=0.1,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m output_texts \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(generate_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m output_text \u001b[39min\u001b[39;00m output_texts:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1641\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1642\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1643\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1644\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1645\u001b[0m     )\n\u001b[1;32m   1647\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1649\u001b[0m         input_ids,\n\u001b[1;32m   1650\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1651\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1652\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1653\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1654\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1655\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1656\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1657\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1658\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1659\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1660\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1663\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1665\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1666\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1672\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:2730\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2727\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2729\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2730\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2731\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2732\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2733\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2734\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2735\u001b[0m )\n\u001b[1;32m   2737\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2738\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:944\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    941\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    943\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 944\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m    945\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    946\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    947\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    948\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    949\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    950\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    951\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    952\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    953\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    954\u001b[0m )\n\u001b[1;32m    956\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    958\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:650\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[39melif\u001b[39;00m attention_mask\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m mask_seq_length:\n\u001b[1;32m    646\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    647\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe provided attention mask has length \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but its length should be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmask_seq_length\u001b[39m}\u001b[39;00m\u001b[39m (sum of the lengths of current and past inputs)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m     )\n\u001b[0;32m--> 650\u001b[0m causal_attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_decoder_attention_mask(\n\u001b[1;32m    651\u001b[0m     attention_mask, input_shape, inputs_embeds, past_key_values_length\n\u001b[1;32m    652\u001b[0m )\n\u001b[1;32m    653\u001b[0m pos_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_positions(attention_mask, past_key_values_length)\n\u001b[1;32m    655\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject_in \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:540\u001b[0m, in \u001b[0;36mOPTDecoder._prepare_decoder_attention_mask\u001b[0;34m(self, attention_mask, input_shape, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    536\u001b[0m combined_attention_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[39mif\u001b[39;00m input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    538\u001b[0m     combined_attention_mask \u001b[39m=\u001b[39m _make_causal_mask(\n\u001b[1;32m    539\u001b[0m         input_shape,\n\u001b[0;32m--> 540\u001b[0m         inputs_embeds\u001b[39m.\u001b[39;49mdtype,\n\u001b[1;32m    541\u001b[0m         device\u001b[39m=\u001b[39minputs_embeds\u001b[39m.\u001b[39mdevice,\n\u001b[1;32m    542\u001b[0m         past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    543\u001b[0m     )\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m     \u001b[39m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[1;32m    547\u001b[0m     expanded_attn_mask \u001b[39m=\u001b[39m _expand_mask(attention_mask, inputs_embeds\u001b[39m.\u001b[39mdtype, tgt_len\u001b[39m=\u001b[39minput_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(\n\u001b[1;32m    548\u001b[0m         inputs_embeds\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    549\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "# generate test\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "prompts = [\n",
    "    'Who are you? Are you conscious?',\n",
    "    'Where is Deutschland?',\n",
    "    'How is Huawei Mate 60 Pro?'\n",
    "] \n",
    "prompts = prompts * (gbs * ngb // len(prompts)) + prompts[:(gbs * ngb % len(prompts))]\n",
    "\n",
    "prompt_len = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompts, padding=\"max_length\", max_length=prompt_len, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    max_length=30 + prompt_len,\n",
    "    # num_beams=2,\n",
    "    # num_beam_groups=2,\n",
    "    # diversity_penalty=0.1,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "output_texts = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "for output_text in output_texts:\n",
    "    logging.info(output_text)\n",
    "    logging.info('-' * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
