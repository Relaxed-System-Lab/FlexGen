{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Policy, logging\n",
    "# from forward import flexgen\n",
    "from test import test_hf_gen\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "checkpoint = \"facebook/opt-125m\" # 125m 6.7b 13b 30b\n",
    "# checkpoint = \"Salesforce/codegen-350M-mono\"\n",
    "# checkpoint = 'bigscience/bloom-560m'\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=2, \n",
    "    num_gpu_batches=4, \n",
    "    weights_gpu_percent=0.0, \n",
    "    weights_cpu_percent=0.3, \n",
    "    cache_gpu_percent=0.0, \n",
    "    cache_cpu_percent=0.2, \n",
    "    act_gpu_percent=0.0, \n",
    "    act_cpu_percent=0.5, \n",
    "    overlap=True, \n",
    "    pin_weight=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward.py: rewrite layer forward function\n",
    "\n",
    "import torch\n",
    "import functools \n",
    "import contextlib\n",
    "\n",
    "# from minibatch import get_size_info, load_kth_batch_inputs, concat_outputs\n",
    "from utils import get_module_from_name\n",
    "\n",
    "\n",
    "def reset_forward(model, layer_name):        \n",
    "    layer = get_module_from_name(model, layer_name) \n",
    "\n",
    "    if hasattr(layer, \"_flexgen_old_forward\"):\n",
    "        layer.forward = layer._flexgen_old_forward\n",
    "        delattr(layer, \"_flexgen_old_forward\")\n",
    "        logger.debug(f'{layer_name} from flexgen to old.')\n",
    "\n",
    "    if hasattr(layer, \"_test_old_forward\"):\n",
    "        layer.forward = layer._test_old_forward\n",
    "        delattr(layer, \"_test_old_forward\")\n",
    "        logger.debug(f'{layer_name} from test to old.')\n",
    "\n",
    "def to_test_forward(mpl, layer_name, call_layer_log):\n",
    "    layer = get_module_from_name(mpl.model, layer_name) \n",
    "    compute_device = 'cpu' \n",
    "    layer._test_old_forward = old_forward = layer.forward \n",
    "\n",
    "    @functools.wraps(old_forward)\n",
    "    def new_forward(*args, **kwargs):\n",
    "        mpl.load_layer_weights(layer_name, compute_device) \n",
    "\n",
    "        call_layer_log.append(layer_name)  # \n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = old_forward(*args, **kwargs)\n",
    "\n",
    "        mpl.offload_layer_weights(layer_name)\n",
    "        return output\n",
    "\n",
    "    layer.forward = new_forward\n",
    "    logger.debug(f'{layer_name} to test forward') \n",
    "\n",
    "@contextlib.contextmanager\n",
    "def test(mpl, call_layer_log):\n",
    "    model = mpl.model\n",
    "    layer_names = mpl.layer_names\n",
    "\n",
    "    # test run to get layer calling order\n",
    "    for layer_name in layer_names:\n",
    "        to_test_forward(mpl, layer_name, call_layer_log)\n",
    "    yield \n",
    "    for layer_name in layer_names:\n",
    "        reset_forward(model, layer_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "from typing import Mapping, Tuple\n",
    "import numpy as np \n",
    "import os \n",
    "import shutil \n",
    "import torch\n",
    "from math import floor\n",
    "\n",
    "class MixTensor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        mix_data: Tuple, \n",
    "        split_dim: int, \n",
    "        device: torch.device, \n",
    "        shape: torch.Size,\n",
    "        percents: Mapping[str, float],\n",
    "        file_path: str,\n",
    "        dtype\n",
    "    ):\n",
    "        self.mix_data = mix_data\n",
    "        self.split_dim = split_dim \n",
    "        self.device = device \n",
    "        self.shape = shape \n",
    "        self.percents = percents\n",
    "        self.file_path = file_path\n",
    "        self.dtype = dtype\n",
    "    \n",
    "    def size(self):\n",
    "        return self.shape \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_split_dim(tensor):\n",
    "        dim_sizes = tensor.size()\n",
    "        max_dim, max_size = -1, -1\n",
    "        for dim, size in enumerate(dim_sizes):\n",
    "            if size > max_size:\n",
    "                max_size = size\n",
    "                max_dim = dim \n",
    "        return max_dim \n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_dim_slice(tensor, dim, dim_slice):\n",
    "        return tensor[(dim if dim >= 0 else dim + tensor.dim()) * (slice(None), ) + (dim_slice, )]\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_tensor(tensor, dim, percents):\n",
    "        dim_size = tensor.size(dim)\n",
    "        g_per, c_per, _ = [percents[dev] for dev in ['cuda', 'cpu', 'disk']]\n",
    "        \n",
    "        g_cut = floor(dim_size * g_per)\n",
    "        c_cut = floor(dim_size * (g_per + c_per))\n",
    "\n",
    "        g_data = MixTensor.tensor_dim_slice(tensor, dim, slice(0, g_cut))\n",
    "        c_data = MixTensor.tensor_dim_slice(tensor, dim, slice(g_cut, c_cut))\n",
    "        d_data = MixTensor.tensor_dim_slice(tensor, dim, slice(c_cut, dim_size))\n",
    "        return g_data, c_data, d_data \n",
    "\n",
    "    @classmethod\n",
    "    def from_tensor(\n",
    "        cls, \n",
    "        tensor: torch.Tensor, \n",
    "        percents: Mapping[str, float],\n",
    "        file_path: str \n",
    "    ):\n",
    "        split_dim = 0# cls.get_split_dim(tensor) \n",
    "        device = tensor.device \n",
    "        shape = tensor.shape\n",
    "        dtype = tensor.dtype\n",
    "        \n",
    "        g_data, c_data, d_data = cls.split_tensor(tensor, split_dim, percents) \n",
    "        \n",
    "        g_data = g_data.to('cuda' if torch.cuda.is_available() else 'cpu') if g_data.numel() else None\n",
    "        c_data = c_data.to('cpu') if c_data.numel() else None\n",
    "        if d_data.numel():\n",
    "            d_data = d_data.cpu().numpy()\n",
    "            np_shape = d_data.shape\n",
    "            np_dtype = d_data.dtype \n",
    "\n",
    "            fp = np.memmap(file_path, mode=\"w+\", shape=np_shape, dtype=np_dtype)\n",
    "            fp[:] = d_data[:]\n",
    "            d_data = (np_shape, np_dtype)\n",
    "        else:\n",
    "            d_data = None \n",
    "        mix_data = (g_data, c_data, d_data)\n",
    "\n",
    "        return cls(\n",
    "            mix_data=mix_data,\n",
    "            split_dim=split_dim,\n",
    "            device=device,\n",
    "            shape=shape,\n",
    "            percents=percents,\n",
    "            file_path=file_path,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "    def to_tensor(self):\n",
    "        g_data, c_data, d_data = self.mix_data \n",
    "        compute_device = self.device \n",
    "\n",
    "        tensor = []\n",
    "        if g_data is not None:\n",
    "            if g_data.device != torch.device(compute_device):\n",
    "                g_data = g_data.to(compute_device) \n",
    "            tensor.append(g_data)\n",
    "        if c_data is not None:\n",
    "            if c_data.device != torch.device(compute_device):\n",
    "                c_data = c_data.to(compute_device) \n",
    "            tensor.append(c_data)\n",
    "        if d_data is not None:\n",
    "            (shape, np_dtype) = d_data \n",
    "            d_data = np.memmap(self.file_path, shape=shape, dtype=np_dtype, mode='r')\n",
    "            d_data = torch.from_numpy(d_data).to(compute_device)\n",
    "            tensor.append(d_data)\n",
    "            \n",
    "        tensor = torch.cat(tensor, dim=self.split_dim) \n",
    "\n",
    "        return tensor        \n",
    "\n",
    "    def __add__(self, mix_tensor):\n",
    "        assert self.shape == mix_tensor.shape and type(self) == type(mix_tensor) # is same shape mix tensor\n",
    "        res = self.to_tensor() + mix_tensor.to_tensor() # TODO: self.tensor\n",
    "        return self.from_tensor(res, self.percents, self.file_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    x = torch.rand(8, 500, 64, dtype=torch.float32)\n",
    "    m = MixTensor.from_tensor(x, percents={'cuda':0, 'cpu':0.5, 'disk':0.5}, file_path='test/m.dat')\n",
    "    m2 = MixTensor.from_tensor(x, percents={'cuda':0, 'cpu':0.5, 'disk':0.5}, file_path='test/m2.dat')\n",
    "    m = m + m2\n",
    "    print((m.to_tensor() - 2 * x).abs().sum() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "from accelerate.utils import honor_type\n",
    "from typing import Mapping\n",
    "from utils import logging \n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "def get_type_size_info(obj): # recursive\n",
    "    if isinstance(obj, (tuple, list)):\n",
    "        return honor_type(obj, (get_type_size_info(o) for o in obj))\n",
    "    elif isinstance(obj, Mapping):\n",
    "        return type(obj)({k:get_type_size_info(v) for k, v in obj.items()})\n",
    "    \n",
    "    elif isinstance(obj, (torch.Tensor, MixTensor, BatchMixTensor)):\n",
    "        return f'{type(obj)}: {obj.size()}'\n",
    "\n",
    "    elif isinstance(obj, (int, bool, type(None))): \n",
    "        return f'{type(obj)}: {obj}'\n",
    "    else:\n",
    "        logger.warning(f'inputs: {obj} of type \\'{type(obj)}\\' is not implemented.')\n",
    "        return f'{type(obj)}: {obj}'\n",
    "\n",
    "def to_compute_device(obj): # recursive\n",
    "    if isinstance(obj, (tuple, list)):\n",
    "        return honor_type(obj, (to_compute_device(o) for o in obj))\n",
    "    elif isinstance(obj, Mapping):\n",
    "        return type(obj)({k:to_compute_device(v) for k, v in obj.items()})\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj\n",
    "    elif isinstance(obj, (MixTensor, BatchMixTensor)):\n",
    "        return obj.to_tensor()\n",
    "\n",
    "    elif isinstance(obj, (int, bool, type(None))): \n",
    "        return obj\n",
    "    else:\n",
    "        logger.warning(f'inputs: {obj} of type \\'{type(obj)}\\' is not implemented.')\n",
    "        return obj\n",
    "\n",
    "def to_mixed_device(obj, policy, prefix): \n",
    "    if isinstance(obj, tuple) and len(obj) == 2 and isinstance(obj[0], torch.Tensor) and isinstance(obj[1], torch.Tensor): # KV cache\n",
    "        m0 = MixTensor.from_tensor(\n",
    "            obj[0], \n",
    "            percents={\n",
    "                'cuda':policy.cache_gpu_percent, \n",
    "                'cpu':policy.cache_cpu_percent, \n",
    "                'disk':policy.cache_disk_percent, \n",
    "            }, \n",
    "            file_path=f'{prefix}.key.dat'\n",
    "        )\n",
    "        m1 = MixTensor.from_tensor(\n",
    "            obj[1], \n",
    "            percents={\n",
    "                'cuda':policy.cache_gpu_percent, \n",
    "                'cpu':policy.cache_cpu_percent, \n",
    "                'disk':policy.cache_disk_percent, \n",
    "            }, \n",
    "            file_path=f'{prefix}.value.dat'\n",
    "        )\n",
    "        return (m0, m1)\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return MixTensor.from_tensor(\n",
    "            obj, \n",
    "            percents={\n",
    "                'cuda':policy.act_gpu_percent, \n",
    "                'cpu':policy.act_cpu_percent, \n",
    "                'disk':policy.act_disk_percent, \n",
    "            }, \n",
    "            file_path=f'{prefix}.dat'\n",
    "        )\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(to_mixed_device(o, policy, f'{prefix}.{i}') for i, o in enumerate(obj))\n",
    "    else:\n",
    "        logger.warning(f'inputs: {obj} of type \\'{type(obj)}\\' is not implemented.')\n",
    "        return obj\n",
    "\n",
    "from typing import Iterable\n",
    "class BatchMixTensor:\n",
    "    def __init__(self, batches: Iterable[MixTensor]):\n",
    "        self.batches = batches \n",
    "\n",
    "        self.shape = self.size()\n",
    "        self.dtype = batches[0].dtype\n",
    "        self.device = batches[0].device\n",
    "\n",
    "    # def __getitem__(self, i):\n",
    "    #     return self.batches[i]\n",
    "    \n",
    "    # def __setitem__(self, i, mt: MixTensor):\n",
    "    #     self.batches[i] = mt\n",
    "\n",
    "    # def __len__(self):\n",
    "    #     return len(self.batches)\n",
    "    \n",
    "    def size(self):\n",
    "        shape = list(self.batches[0].size()) \n",
    "        shape[0] *= len(self.batches)\n",
    "        return torch.Size(shape)\n",
    "\n",
    "    def __add__(self, bmt):\n",
    "        for k in range(len(self.batches)): \n",
    "            # TODO flexgen: parallelly load k+1\n",
    "            self_k = self.batches[k].to_tensor()\n",
    "            bmt_k = bmt.batches[k].to_tensor()\n",
    "            res = self_k + bmt_k \n",
    "            self.batches[k] = MixTensor.from_tensor(res, self.batches[k].percents, self.batches[k].file_path)\n",
    "        return self \n",
    "\n",
    "    def contiguous(self):\n",
    "        return self.to_tensor()\n",
    "\n",
    "    def to_tensor(self):\n",
    "        tensor = []\n",
    "        for mt in self.batches:\n",
    "            tensor.append(mt.to_tensor())\n",
    "        return torch.cat(tensor, dim=0)\n",
    "\n",
    "def concat_outputs(outputs): # concatenate K outputs to one output\n",
    "    assert len(outputs), 'empty outputs.'\n",
    "    assert isinstance(outputs[0], (MixTensor, torch.Tensor, tuple)), f'not supported type: {type(outputs[0])}.'\n",
    "    \n",
    "    if isinstance(outputs[0], torch.Tensor):\n",
    "        return torch.cat(outputs, dim=0)\n",
    "    elif isinstance(outputs[0], MixTensor):\n",
    "        return BatchMixTensor(outputs)\n",
    "    elif isinstance(outputs[0], tuple):\n",
    "        def f(outputs):\n",
    "            ans = []\n",
    "            for elem in zip(*outputs):\n",
    "                if isinstance(elem[0], torch.Tensor):\n",
    "                    ans.append(torch.cat(elem, dim=0))\n",
    "                elif isinstance(elem[0], MixTensor):\n",
    "                    ans.append(BatchMixTensor(elem))\n",
    "                elif isinstance(elem[0], tuple):\n",
    "                    ans.append(f(elem))\n",
    "                # else:\n",
    "                #     logger.warning(f'outputs: {elem[0]} of type \\'{type(elem[0])}\\' is not implemented.')\n",
    "                #     ans.append(elem[0])\n",
    "            return tuple(ans)\n",
    "\n",
    "        return f(outputs)\n",
    "\n",
    "\n",
    "def load_kth_batch_inputs(inputs, k, ngb): # for both args, kwargs, with a nested structure of tuple/list/dict/Tensor\n",
    "    if isinstance(inputs, (tuple, list)): # e.g. args\n",
    "        return honor_type(inputs, (load_kth_batch_inputs(inp, k, ngb) for inp in inputs))\n",
    "    elif isinstance(inputs, Mapping): # e.g. kwargs\n",
    "        return type(inputs)({key:load_kth_batch_inputs(value, k, ngb) for key, value in inputs.items()})\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        mini_size = inputs.size(0) // ngb\n",
    "        return inputs[k * mini_size:(k + 1) * mini_size]\n",
    "    # elif isinstance(inputs, MixTensor):\n",
    "    #     inputs = inputs.to_tensor()\n",
    "    #     mini_size = inputs.size(0) // ngb\n",
    "    #     return inputs[k * mini_size:(k + 1) * mini_size]\n",
    "    elif isinstance(inputs, BatchMixTensor):\n",
    "        mini_batch = inputs.batches[k]\n",
    "        return mini_batch.to_tensor()\n",
    "    elif isinstance(inputs, (int, bool, type(None))): \n",
    "        return inputs\n",
    "    else:\n",
    "        logger.warning(f'inputs: {inputs} of type \\'{type(inputs)}\\' is not implemented.')\n",
    "        return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_flexgen_forward(mpl, j, compute_device, args_offload_dir):\n",
    "    # rewrite the j-th layer's forward\n",
    "    layer_name = mpl.layer_names[j]\n",
    "    next_layer_name = mpl.layer_names[(j + 1) % len(mpl.layer_names)]\n",
    "\n",
    "    policy = mpl.policy\n",
    "    ngb = policy.num_gpu_batches\n",
    "\n",
    "    layer = get_module_from_name(mpl.model, layer_name)  \n",
    "    if hasattr(layer, \"_flexgen_old_forward\"): return  \n",
    "    \n",
    "    layer._flexgen_old_forward = old_forward = layer.forward \n",
    "\n",
    "    @functools.wraps(old_forward)\n",
    "    def new_forward(*args, **kwargs):\n",
    "        # pre fwd: load curr & next weights, TODO: cuda stream\n",
    "        mpl.load_layer_weights(layer_name, compute_device) \n",
    "        mpl.load_layer_weights(next_layer_name, compute_device) \n",
    "        \n",
    "        # loop forward pass of K minibatches, TODO: cuda stream\n",
    "        with torch.no_grad():\n",
    "\n",
    "            logger.debug(f'args: {get_type_size_info(args)}')\n",
    "            logger.debug(f'kwargs: {get_type_size_info(kwargs)}')\n",
    "\n",
    "            args = to_compute_device(args)\n",
    "            kwargs = to_compute_device(kwargs)\n",
    "            \n",
    "            outputs = []\n",
    "            for k in range(ngb):\n",
    "\n",
    "                # 'pre' fwd: load curr & next inputs (activations, KV cache) to compute device\n",
    "                args_k = load_kth_batch_inputs(args, k, ngb)\n",
    "                kwargs_k = load_kth_batch_inputs(kwargs, k, ngb)\n",
    "\n",
    "                # the k-th fwd pass\n",
    "                output = old_forward(*args_k, **kwargs_k)\n",
    "\n",
    "                # post fwd: 1) output: to mix, 2) args_k, kwargs_k: free (TODO?)\n",
    "                output = to_mixed_device(output, policy, prefix=f'{args_offload_dir}/{layer_name}.batch.{k}.output')\n",
    "                # output = to_compute_device(output)\n",
    "\n",
    "                logger.debug(f'layer: {layer_name}, '\n",
    "                             f'batch: {k}, '\n",
    "                             f'args: {get_type_size_info(args_k)}, '\n",
    "                             f'kwargs: {get_type_size_info(kwargs_k)}, '\n",
    "                             f'output: {get_type_size_info(output)}')\n",
    "                outputs.append(output) \n",
    "\n",
    "            output = concat_outputs(outputs)\n",
    "            output = to_compute_device(output)\n",
    "            logger.debug(f'outputs after concat: {get_type_size_info(output)}')  \n",
    "\n",
    "        # post fwd: free curr weights\n",
    "        mpl.offload_layer_weights(layer_name)\n",
    "        return output\n",
    "\n",
    "    layer.forward = new_forward\n",
    "    logger.debug(f'{layer_name} to flexgen forward')\n",
    "\n",
    "@contextlib.contextmanager \n",
    "def flexgen(checkpoint, policy, args_offload_dir = 'args_offload_dir'):\n",
    "    os.makedirs(args_offload_dir, exist_ok=True) \n",
    "\n",
    "    # init model \n",
    "    from model import ModelPolicyLoader\n",
    "    mpl = ModelPolicyLoader(checkpoint, policy)\n",
    "    mpl.init_all_weights() # init \n",
    "\n",
    "    # test run, get layer order\n",
    "    call_layer_log = []\n",
    "    with test(mpl, call_layer_log):\n",
    "        from test import test_hf_gen\n",
    "        test_hf_gen(mpl.checkpoint, mpl.model, 1,1, prompts=['0'])\n",
    "\n",
    "    assert len(call_layer_log) == len(mpl.layer_names) and set(call_layer_log) == set(mpl.layer_names)\n",
    "    mpl.layer_names = call_layer_log\n",
    "\n",
    "    # rewrite layer forward\n",
    "    for j, _ in enumerate(mpl.layer_names):\n",
    "        compute_device = 'cpu'\n",
    "        to_flexgen_forward(mpl, j, compute_device, args_offload_dir)\n",
    "    yield mpl.model \n",
    "    for layer_name in mpl.layer_names:\n",
    "        reset_forward(mpl.model, layer_name)\n",
    "    shutil.rmtree(args_offload_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 15:27:13,634 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-11 15:27:13,771 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 15:27:13,854 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']\n",
      "2023-10-11 15:27:13,899 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-11 15:27:13,981 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']\n",
      "2023-10-11 15:27:13,983 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'\n",
      "2023-10-11 15:27:13,990 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400\n",
      "2023-10-11 15:27:13,991 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000\n",
      "2023-10-11 15:27:13,992 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464\n",
      "2023-10-11 15:27:13,994 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592\n",
      "2023-10-11 15:27:13,996 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720\n",
      "2023-10-11 15:27:13,997 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848\n",
      "2023-10-11 15:27:13,999 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976\n",
      "2023-10-11 15:27:14,000 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104\n",
      "2023-10-11 15:27:14,002 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232\n",
      "2023-10-11 15:27:14,003 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360\n",
      "2023-10-11 15:27:14,005 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488\n",
      "2023-10-11 15:27:14,007 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616\n",
      "2023-10-11 15:27:14,008 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744\n",
      "2023-10-11 15:27:14,010 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872\n",
      "2023-10-11 15:27:14,012 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-10-11 15:27:14,013 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-10-11 15:27:14,013 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!\n",
      "2023-10-11 15:27:14,016 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: \n",
      "GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)\n",
      "2023-10-11 15:27:14,020 [model.py:241 in init_all_weights] DEBUG - init all weights...\n",
      "model init: loading by policy...: 100%|██████████| 197/197 [00:00<00:00, 4427.03it/s]\n",
      "2023-10-11 15:27:14,068 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward\n",
      "2023-10-11 15:27:14,069 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward\n",
      "2023-10-11 15:27:14,070 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward\n",
      "2023-10-11 15:27:14,071 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward\n",
      "2023-10-11 15:27:14,072 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward\n",
      "2023-10-11 15:27:14,074 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward\n",
      "2023-10-11 15:27:14,074 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward\n",
      "2023-10-11 15:27:14,075 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward\n",
      "2023-10-11 15:27:14,076 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward\n",
      "2023-10-11 15:27:14,077 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward\n",
      "2023-10-11 15:27:14,077 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward\n",
      "2023-10-11 15:27:14,079 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward\n",
      "2023-10-11 15:27:14,080 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward\n",
      "2023-10-11 15:27:14,081 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward\n",
      "2023-10-11 15:27:14,082 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward\n",
      "2023-10-11 15:27:14,082 [520681597.py:42 in to_test_forward] DEBUG - lm_head to test forward\n",
      "2023-10-11 15:27:14,122 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2023-10-11 15:27:14,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:14,294 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:14,297 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:14,307 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:14,318 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:14,329 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:14,340 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:14,350 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:14,360 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:14,369 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:14,378 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:14,393 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:14,406 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,409 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:14,417 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:14,429 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,432 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:14,434 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:14,449 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,459 [test.py:40 in test_hf_gen] INFO - 0.\n",
      "2023-10-11 15:27:14,460 [test.py:41 in test_hf_gen] INFO - ----------\n",
      "2023-10-11 15:27:14,476 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.\n",
      "2023-10-11 15:27:14,477 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.\n",
      "2023-10-11 15:27:14,478 [520681597.py:22 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.\n",
      "2023-10-11 15:27:14,478 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.\n",
      "2023-10-11 15:27:14,479 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.\n",
      "2023-10-11 15:27:14,480 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.\n",
      "2023-10-11 15:27:14,481 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.\n",
      "2023-10-11 15:27:14,482 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.\n",
      "2023-10-11 15:27:14,483 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.\n",
      "2023-10-11 15:27:14,483 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.\n",
      "2023-10-11 15:27:14,485 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.\n",
      "2023-10-11 15:27:14,486 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.\n",
      "2023-10-11 15:27:14,487 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.\n",
      "2023-10-11 15:27:14,488 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.\n",
      "2023-10-11 15:27:14,489 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.\n",
      "2023-10-11 15:27:14,490 [520681597.py:22 in reset_forward] DEBUG - lm_head from test to old.\n",
      "2023-10-11 15:27:14,491 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward\n",
      "2023-10-11 15:27:14,493 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward\n",
      "2023-10-11 15:27:14,494 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward\n",
      "2023-10-11 15:27:14,495 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward\n",
      "2023-10-11 15:27:14,496 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward\n",
      "2023-10-11 15:27:14,496 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward\n",
      "2023-10-11 15:27:14,498 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward\n",
      "2023-10-11 15:27:14,499 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward\n",
      "2023-10-11 15:27:14,500 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward\n",
      "2023-10-11 15:27:14,501 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward\n",
      "2023-10-11 15:27:14,503 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward\n",
      "2023-10-11 15:27:14,504 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward\n",
      "2023-10-11 15:27:14,505 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward\n",
      "2023-10-11 15:27:14,506 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward\n",
      "2023-10-11 15:27:14,507 [750919123.py:59 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward\n",
      "2023-10-11 15:27:14,508 [750919123.py:59 in to_flexgen_forward] DEBUG - lm_head to flexgen forward\n",
      "2023-10-11 15:27:14,548 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2023-10-11 15:27:14,699 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:14,701 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:14,702 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9])\",)\n",
      "2023-10-11 15:27:14,703 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:14,705 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:14,707 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:14,708 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:14,709 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:14,712 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 9, 768])\n",
      "2023-10-11 15:27:14,713 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:14,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:14,720 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9])\", \"<class 'int'>: 0\")\n",
      "2023-10-11 15:27:14,721 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:14,723 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9])\", \"<class 'int'>: 0\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:14,724 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9])\", \"<class 'int'>: 0\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:14,726 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9])\", \"<class 'int'>: 0\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:14,727 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9])\", \"<class 'int'>: 0\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:14,729 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 9, 768])\n",
      "2023-10-11 15:27:14,730 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:14,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:14,741 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:14,743 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:14,758 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,765 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,770 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,779 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,782 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,783 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,785 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:14,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:14,794 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:14,795 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:14,804 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,811 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,818 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,824 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,827 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,831 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:14,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:14,840 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:14,841 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:14,849 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,856 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,862 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,869 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,873 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,873 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:14,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:14,885 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:14,886 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:14,892 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,901 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,907 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,914 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,918 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,919 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,922 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:14,926 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:14,931 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:14,932 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:14,942 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,948 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,953 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,959 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,962 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,963 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:14,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:14,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:14,975 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:14,976 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:14,984 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,991 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:14,997 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,007 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,011 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,012 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,015 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:15,020 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:15,026 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:15,027 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,040 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,047 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,055 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,061 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,065 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,065 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,068 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:15,072 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:15,078 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:15,079 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,088 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,094 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,100 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,119 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,123 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,124 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,126 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:15,131 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:15,136 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:15,137 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,146 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,160 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,171 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,190 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,194 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,194 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:15,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:15,206 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:15,207 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,218 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,225 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,239 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,281 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,285 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,286 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:15,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:15,297 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:15,298 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,339 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,368 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,376 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,392 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,395 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,396 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,398 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:15,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:15,404 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:15,404 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,423 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,428 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,435 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,455 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 9, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,458 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"))\n",
      "2023-10-11 15:27:15,459 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:15,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:15,464 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:15,465 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:15,466 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:15,468 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:15,469 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:15,471 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 768])\n",
      "2023-10-11 15:27:15,472 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 9, 768])\n",
      "2023-10-11 15:27:15,473 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:15,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:15,477 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9, 768])\",)\n",
      "2023-10-11 15:27:15,478 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:15,498 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 50272])\n",
      "2023-10-11 15:27:15,521 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 50272])\n",
      "2023-10-11 15:27:15,537 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 50272])\n",
      "2023-10-11 15:27:15,552 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 9, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 9, 50272])\n",
      "2023-10-11 15:27:15,565 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 9, 50272])\n",
      "2023-10-11 15:27:15,566 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,573 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:15,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:15,576 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1])\",)\n",
      "2023-10-11 15:27:15,577 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:15,579 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:15,580 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:15,582 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:15,583 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:15,585 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:15,585 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,587 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:15,588 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:15,593 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 10])\", \"<class 'int'>: 9\")\n",
      "2023-10-11 15:27:15,594 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:15,595 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 10])\", \"<class 'int'>: 9\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:15,597 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 10])\", \"<class 'int'>: 9\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:15,598 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 10])\", \"<class 'int'>: 9\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:15,600 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 10])\", \"<class 'int'>: 9\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:15,602 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:15,603 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,604 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:15,608 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:15,613 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:15,614 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,624 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,635 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,647 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,653 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,656 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:15,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:15,671 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:15,673 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,683 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,704 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,709 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,714 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,718 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,718 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:15,726 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:15,731 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:15,731 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,737 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,743 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,748 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,752 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,755 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,756 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,758 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:15,763 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:15,768 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:15,769 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,780 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,788 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,806 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,812 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,816 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,817 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:15,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:15,828 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:15,829 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,836 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,846 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,850 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,854 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,858 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,858 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,861 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:15,865 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:15,870 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:15,871 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,876 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,885 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,896 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,906 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,909 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,910 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,913 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:15,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:15,922 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:15,923 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,929 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,936 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,941 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,947 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,950 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:15,951 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:15,954 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:15,959 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:15,965 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:15,966 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:15,995 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,005 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,011 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,024 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,028 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,029 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,031 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:16,036 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:16,041 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,042 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,051 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,121 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,160 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,170 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,174 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,175 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:16,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:16,187 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,188 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,207 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,215 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,226 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,232 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,236 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,237 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:16,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:16,249 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,250 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,258 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,263 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,272 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,280 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,283 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,284 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,287 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:16,291 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:16,293 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,294 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,304 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,314 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,320 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,325 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,329 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"))\n",
      "2023-10-11 15:27:16,330 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:16,334 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:16,335 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,336 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:16,339 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,342 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,344 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,347 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,349 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:16,350 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,351 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:16,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:16,354 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,355 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:16,367 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:16,379 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:16,391 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:16,402 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:16,413 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])\n",
      "2023-10-11 15:27:16,414 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,422 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:16,423 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:16,425 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1])\",)\n",
      "2023-10-11 15:27:16,426 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:16,428 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,429 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,431 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,432 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,434 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:16,435 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:16,437 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:16,442 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 11])\", \"<class 'int'>: 10\")\n",
      "2023-10-11 15:27:16,443 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:16,444 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 11])\", \"<class 'int'>: 10\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,446 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 11])\", \"<class 'int'>: 10\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,448 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 11])\", \"<class 'int'>: 10\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,449 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 11])\", \"<class 'int'>: 10\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:16,451 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:16,452 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,453 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:16,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:16,462 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,464 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,471 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,481 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,487 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,492 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,495 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,496 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,498 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:16,502 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:16,507 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,508 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,516 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,527 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,532 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,538 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,541 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,542 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,545 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:16,549 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:16,554 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,555 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,562 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,568 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,583 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,588 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,592 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,593 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,595 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:16,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:16,605 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,606 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,612 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,617 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,623 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,630 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,633 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,634 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,638 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:16,643 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:16,648 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,649 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,669 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,676 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,682 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,701 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,705 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,706 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,709 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:16,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:16,718 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,719 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,725 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,733 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,740 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,745 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,749 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,749 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,752 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:16,757 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:16,761 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,762 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,768 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,772 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,778 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,783 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,786 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,787 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,789 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:16,794 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:16,799 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,800 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,815 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,820 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,842 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,848 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,852 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,852 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,855 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:16,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:16,864 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,865 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,874 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,882 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,917 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,925 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,928 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,929 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:16,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:16,941 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,942 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,947 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,955 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,968 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,972 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,975 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:16,976 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:16,979 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:16,983 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:16,988 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:16,989 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:16,995 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:17,006 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:17,069 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:17,075 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:17,078 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:17,079 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,081 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:17,086 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:17,087 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,088 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,096 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:17,101 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:17,106 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:17,110 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:17,113 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"))\n",
      "2023-10-11 15:27:17,114 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:17,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:17,120 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,120 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:17,122 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,124 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,125 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,126 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,128 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:17,129 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:17,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:17,133 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,133 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:17,144 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:17,157 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:17,172 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:17,183 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:17,187 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])\n",
      "2023-10-11 15:27:17,188 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:17,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:17,198 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1])\",)\n",
      "2023-10-11 15:27:17,200 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:17,201 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,203 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,204 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,206 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,207 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:17,208 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,210 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:17,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:17,216 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 12])\", \"<class 'int'>: 11\")\n",
      "2023-10-11 15:27:17,217 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:17,219 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 12])\", \"<class 'int'>: 11\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,221 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 12])\", \"<class 'int'>: 11\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,222 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 12])\", \"<class 'int'>: 11\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,224 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 12])\", \"<class 'int'>: 11\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,225 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:17,226 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:17,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:17,237 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,237 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,243 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,249 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,254 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,263 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,266 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,267 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:17,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:17,279 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,279 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,286 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,293 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,299 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,304 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,307 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,308 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:17,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:17,320 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,321 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,328 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,346 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,356 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,362 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,366 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,367 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:17,374 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:17,379 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,380 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,385 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,391 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,397 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,405 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,409 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,410 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:17,417 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:17,423 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,424 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,430 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,436 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,443 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,448 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,452 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,453 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:17,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:17,464 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,465 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,470 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,479 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,491 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,497 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,501 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,502 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:17,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:17,514 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,515 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,585 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,594 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,625 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,633 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,636 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,637 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,639 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:17,643 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:17,648 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,649 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,657 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,664 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,668 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,674 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,677 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,678 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:17,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:17,689 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,690 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,696 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,703 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,710 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,717 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,720 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,721 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,724 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:17,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:17,737 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,738 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,747 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,757 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,767 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,774 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,778 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:17,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:17,791 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,791 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,798 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,810 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,869 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,905 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,909 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,909 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,912 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:17,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:17,918 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,919 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:17,927 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,933 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,951 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,955 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,959 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"))\n",
      "2023-10-11 15:27:17,959 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:17,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:17,964 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,965 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:17,966 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,969 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,971 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,973 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:17,974 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:17,975 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:17,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:17,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:17,978 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:17,979 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:17,994 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:18,005 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:18,014 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:18,026 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:18,031 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])\n",
      "2023-10-11 15:27:18,031 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:18,039 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:18,040 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1])\",)\n",
      "2023-10-11 15:27:18,041 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:18,043 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,045 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,046 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,047 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,049 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:18,049 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:18,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:18,057 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 13])\", \"<class 'int'>: 12\")\n",
      "2023-10-11 15:27:18,057 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:18,059 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 13])\", \"<class 'int'>: 12\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,060 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 13])\", \"<class 'int'>: 12\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,062 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 13])\", \"<class 'int'>: 12\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,063 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 13])\", \"<class 'int'>: 12\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,065 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:18,066 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:18,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:18,076 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,077 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,088 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,095 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,102 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,107 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,110 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,111 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:18,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:18,121 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,122 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,147 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,153 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,157 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,166 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,169 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,170 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,172 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:18,177 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:18,182 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,182 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,190 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,200 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,206 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,214 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,219 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,220 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,223 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:18,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:18,232 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,233 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,243 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,304 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,329 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,334 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,338 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,339 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,341 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:18,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:18,350 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,350 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,356 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,360 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,365 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,369 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,373 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,373 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,376 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:18,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:18,385 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,386 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,391 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,418 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,424 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,428 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,432 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,432 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,435 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:18,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:18,443 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,444 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,463 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,474 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,479 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,486 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,489 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,490 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:18,496 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:18,501 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,502 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,511 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,519 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,542 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,548 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,551 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,552 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:18,558 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:18,563 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,563 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,571 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,577 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,583 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,629 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,633 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,634 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:18,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:18,646 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,647 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,662 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,675 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,680 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,695 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,700 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,701 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,704 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:18,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:18,715 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,716 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,722 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,729 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,735 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,740 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,744 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,744 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:18,751 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:18,753 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,754 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,761 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,767 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,775 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,782 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,785 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"))\n",
      "2023-10-11 15:27:18,786 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:18,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:18,791 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,791 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:18,794 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,796 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,797 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,798 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,800 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:18,801 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:18,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:18,805 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,805 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:18,819 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:18,828 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:18,836 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:18,851 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:18,862 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])\n",
      "2023-10-11 15:27:18,862 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,868 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:18,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:18,871 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1])\",)\n",
      "2023-10-11 15:27:18,872 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:18,873 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,875 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,876 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,878 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,879 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:18,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:18,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:18,888 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 14])\", \"<class 'int'>: 13\")\n",
      "2023-10-11 15:27:18,888 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:18,890 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 14])\", \"<class 'int'>: 13\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,892 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 14])\", \"<class 'int'>: 13\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,893 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 14])\", \"<class 'int'>: 13\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,895 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 14])\", \"<class 'int'>: 13\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:18,896 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:18,897 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:18,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:18,907 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,908 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,915 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:18,920 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:18,925 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:18,929 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:18,933 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:18,933 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:18,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:18,945 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,946 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,952 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:18,959 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:18,966 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:18,971 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:18,975 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:18,975 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:18,978 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:18,982 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:18,987 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:18,987 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:18,995 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,043 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,084 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,095 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,098 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,099 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,101 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:19,106 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:19,110 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,111 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,118 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,138 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,142 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,149 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,152 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,153 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,156 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:19,160 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:19,165 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,165 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,171 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,183 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,191 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,195 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,199 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,199 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:19,206 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:19,211 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,212 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,228 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,234 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,239 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,246 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,250 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,250 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:19,257 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:19,261 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,262 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,270 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,276 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,282 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,287 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,290 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,291 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:19,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:19,303 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,304 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,309 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,315 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,320 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,330 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,334 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:19,346 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:19,352 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,353 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,361 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,371 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,386 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,399 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,402 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,403 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:19,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:19,415 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,415 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,424 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,432 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,437 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,449 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,453 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,454 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,456 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:19,460 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:19,465 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,466 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,472 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,478 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,488 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,497 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,500 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,501 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:19,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:19,511 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,512 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,520 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,527 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,535 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,548 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,552 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"))\n",
      "2023-10-11 15:27:19,553 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:19,557 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:19,558 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,559 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:19,562 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,566 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,569 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,573 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,575 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:19,576 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,577 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:19,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:19,579 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,580 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:19,594 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:19,615 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:19,630 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:19,650 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:19,672 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])\n",
      "2023-10-11 15:27:19,674 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:19,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:19,682 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1])\",)\n",
      "2023-10-11 15:27:19,683 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:19,684 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,686 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,687 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,689 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,690 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:19,691 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:19,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:19,699 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 15])\", \"<class 'int'>: 14\")\n",
      "2023-10-11 15:27:19,700 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:19,701 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 15])\", \"<class 'int'>: 14\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,703 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 15])\", \"<class 'int'>: 14\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,705 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 15])\", \"<class 'int'>: 14\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,706 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 15])\", \"<class 'int'>: 14\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:19,708 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:19,709 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:19,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:19,719 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,719 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,727 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,734 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,744 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,753 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,756 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,757 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,760 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:19,764 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:19,769 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,770 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,776 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,782 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,787 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,791 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,794 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,795 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:19,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:19,806 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,807 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,813 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,826 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,834 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,840 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,844 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,844 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:19,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:19,855 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,856 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,868 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,886 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,891 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,897 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,900 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,901 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:19,905 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:19,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:19,919 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:19,921 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:19,940 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,951 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:19,961 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,020 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,025 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,026 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:20,034 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:20,039 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,040 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,050 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,055 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,060 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,065 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,068 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,069 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,072 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:20,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:20,081 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,082 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,104 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,111 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,118 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,123 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,128 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,129 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:20,139 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:20,146 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,147 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,155 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,167 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,171 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,178 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,182 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,182 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:20,189 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:20,193 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,194 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,201 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,209 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,216 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,265 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,268 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,269 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,271 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:20,276 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:20,281 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,282 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,307 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,312 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,322 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,327 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,330 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,331 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:20,337 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:20,343 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,343 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,349 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,355 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,359 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,369 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,373 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,373 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,376 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:20,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:20,382 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,382 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,389 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,399 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,404 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,409 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,412 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"))\n",
      "2023-10-11 15:27:20,412 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:20,416 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:20,417 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,418 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:20,421 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,422 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,424 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,426 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,427 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:20,428 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,429 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:20,431 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:20,432 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,433 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:20,444 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:20,453 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:20,467 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:20,479 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:20,484 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])\n",
      "2023-10-11 15:27:20,484 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,489 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:20,491 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:20,492 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1])\",)\n",
      "2023-10-11 15:27:20,493 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:20,495 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,496 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,498 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,499 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,501 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:20,502 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,503 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:20,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:20,509 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 16])\", \"<class 'int'>: 15\")\n",
      "2023-10-11 15:27:20,510 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:20,511 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 16])\", \"<class 'int'>: 15\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,513 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 16])\", \"<class 'int'>: 15\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,514 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 16])\", \"<class 'int'>: 15\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,516 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 16])\", \"<class 'int'>: 15\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:20,520 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:20,521 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,523 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:20,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:20,534 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,534 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,544 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,564 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,571 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,576 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,579 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,580 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,582 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:20,586 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:20,591 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,592 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,631 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,661 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,671 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,677 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,680 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,681 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,683 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:20,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:20,692 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,692 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,703 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,711 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,733 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,742 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,745 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:20,753 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:20,757 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,758 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,767 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,777 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,783 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,799 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,802 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,803 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:20,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:20,815 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,816 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,864 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,871 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,877 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,887 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,890 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,891 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,893 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:20,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:20,902 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,903 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,908 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,917 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,922 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,929 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,932 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,933 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:20,935 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:20,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:20,944 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:20,945 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:20,969 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,979 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,989 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:20,998 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,002 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,003 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,005 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:21,010 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:21,014 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,015 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,025 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,030 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,034 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,062 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,065 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,066 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,068 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:21,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:21,078 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,078 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,084 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,089 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,098 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,103 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,106 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,107 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:21,114 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:21,119 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,120 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,127 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,134 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,139 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,145 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,149 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,150 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,152 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:21,156 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:21,161 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,162 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,167 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,174 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,193 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,197 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,201 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,201 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,204 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:21,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:21,209 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,210 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,239 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,244 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,252 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,257 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,261 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"))\n",
      "2023-10-11 15:27:21,262 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,264 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:21,265 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:21,266 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,267 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:21,270 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,271 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,273 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,274 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,275 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:21,276 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:21,279 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:21,280 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,281 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:21,294 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:21,304 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:21,313 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:21,322 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:21,333 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])\n",
      "2023-10-11 15:27:21,334 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:21,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:21,342 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1])\",)\n",
      "2023-10-11 15:27:21,343 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:21,344 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,345 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,346 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,348 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,349 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:21,350 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:21,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:21,358 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 17])\", \"<class 'int'>: 16\")\n",
      "2023-10-11 15:27:21,358 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:21,360 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 17])\", \"<class 'int'>: 16\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,361 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 17])\", \"<class 'int'>: 16\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,363 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 17])\", \"<class 'int'>: 16\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,365 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 17])\", \"<class 'int'>: 16\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:21,366 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:21,367 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,368 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:21,372 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:21,377 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,377 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,391 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,412 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,422 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,433 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,436 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,437 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:21,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:21,448 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,448 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,453 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,461 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,469 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,478 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,482 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,483 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,485 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:21,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:21,496 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,497 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,504 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,510 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,514 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,522 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,526 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,526 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:21,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:21,538 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,539 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,588 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,654 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,675 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,680 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,684 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,684 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,686 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:21,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:21,696 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,697 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,702 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,710 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,717 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,722 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,725 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,726 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:21,733 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:21,737 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,738 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,745 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,755 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,761 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,767 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,770 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,771 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:21,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:21,782 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,783 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,809 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,815 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,821 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,826 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,830 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,830 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,833 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:21,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:21,842 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,842 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,851 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,856 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,861 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,867 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,870 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,871 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:21,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:21,882 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,883 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,890 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,898 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,906 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,921 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,925 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,925 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:21,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:21,936 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,937 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,948 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,954 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,970 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,974 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,977 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:21,978 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:21,980 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:21,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:21,990 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:21,991 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:21,999 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:22,008 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:22,015 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:22,024 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:22,027 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:22,028 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:22,034 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:22,036 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,036 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,043 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:22,051 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:22,060 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:22,066 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:22,069 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"))\n",
      "2023-10-11 15:27:22,069 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,072 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:22,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:22,074 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,075 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:22,078 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,079 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,082 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,083 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,085 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:22,086 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,087 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:22,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:22,089 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,090 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:22,102 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:22,111 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:22,122 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:22,133 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:22,144 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])\n",
      "2023-10-11 15:27:22,145 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,150 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:22,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:22,152 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1])\",)\n",
      "2023-10-11 15:27:22,153 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:22,154 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,156 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,157 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,159 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,160 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:22,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 15:27:22,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:22,168 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 18])\", \"<class 'int'>: 17\")\n",
      "2023-10-11 15:27:22,169 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:22,171 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 18])\", \"<class 'int'>: 17\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,173 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 18])\", \"<class 'int'>: 17\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,174 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 18])\", \"<class 'int'>: 17\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,176 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 18])\", \"<class 'int'>: 17\"), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,177 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:22,178 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,179 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 15:27:22,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:22,188 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,188 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,199 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,204 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,217 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,232 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,235 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 15:27:22,243 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:22,247 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,248 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,255 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,264 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,274 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,289 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,293 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,293 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 15:27:22,300 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:22,305 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,306 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,311 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,318 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,327 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,332 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,335 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,336 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,338 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 15:27:22,343 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:22,347 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,348 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,356 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,362 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,371 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,379 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,382 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,382 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 15:27:22,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:22,394 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,394 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,402 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,408 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,414 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,418 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,421 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,422 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,424 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 15:27:22,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:22,432 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,433 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,447 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,452 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,461 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,467 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,471 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 15:27:22,478 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:22,482 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,483 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,495 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,501 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,509 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,518 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,521 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,522 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 15:27:22,528 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:22,533 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,533 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,541 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,547 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,559 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,566 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,570 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,570 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,573 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 15:27:22,577 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:22,581 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,582 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,587 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,618 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,634 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,642 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,646 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,647 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,650 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 15:27:22,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:22,659 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,660 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,669 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,675 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,682 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,702 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,706 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,707 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 15:27:22,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:22,719 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,720 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,732 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,739 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,788 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,800 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,804 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,808 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,811 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 15:27:22,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:22,818 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,819 [750919123.py:24 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 15:27:22,834 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,843 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,858 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,864 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': (\"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\", \"<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])\"), 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}, output: (\"<class '__main__.MixTensor'>: torch.Size([2, 1, 768])\", (\"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\", \"<class '__main__.MixTensor'>: torch.Size([2, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,867 [750919123.py:52 in new_forward] DEBUG - outputs after concat: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\", (\"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\", \"<class 'torch.Tensor'>: torch.Size([8, 12, 18, 64])\"))\n",
      "2023-10-11 15:27:22,868 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 15:27:22,872 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:22,873 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,873 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:22,875 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,879 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,882 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,884 [750919123.py:43 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 768])\n",
      "2023-10-11 15:27:22,886 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 768])\n",
      "2023-10-11 15:27:22,887 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 15:27:22,889 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 15:27:22,891 [750919123.py:23 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 1, 768])\",)\n",
      "2023-10-11 15:27:22,892 [750919123.py:24 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 15:27:22,905 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 0, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:22,916 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 1, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:22,926 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 2, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:22,939 [750919123.py:43 in new_forward] DEBUG - layer: lm_head, batch: 3, args: (\"<class 'torch.Tensor'>: torch.Size([2, 1, 768])\",), kwargs: {}, output: <class '__main__.MixTensor'>: torch.Size([2, 1, 50272])\n",
      "2023-10-11 15:27:22,964 [750919123.py:52 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])\n",
      "2023-10-11 15:27:22,968 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 15:27:22,984 [test.py:40 in test_hf_gen] INFO - for i in range(10):           \n",
      "2023-10-11 15:27:22,985 [test.py:41 in test_hf_gen] INFO - ----------\n",
      "2023-10-11 15:27:22,985 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?\n",
      "I'm a woman. I'm not conscious\n",
      "2023-10-11 15:27:22,986 [test.py:41 in test_hf_gen] INFO - ----------\n",
      "2023-10-11 15:27:22,987 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?\n",
      "I'm in Germany.\n",
      "2023-10-11 15:27:22,988 [test.py:41 in test_hf_gen] INFO - ----------\n",
      "2023-10-11 15:27:22,989 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?\n",
      "Huawei Mate 60 Pro is a premium smartphone\n",
      "2023-10-11 15:27:22,993 [test.py:41 in test_hf_gen] INFO - ----------\n",
      "2023-10-11 15:27:22,995 [test.py:40 in test_hf_gen] INFO - for i in range(10):           \n",
      "2023-10-11 15:27:22,996 [test.py:41 in test_hf_gen] INFO - ----------\n",
      "2023-10-11 15:27:22,996 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?\n",
      "I'm a woman. I'm not conscious\n",
      "2023-10-11 15:27:22,997 [test.py:41 in test_hf_gen] INFO - ----------\n",
      "2023-10-11 15:27:22,998 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?\n",
      "I'm in Germany.\n",
      "2023-10-11 15:27:22,999 [test.py:41 in test_hf_gen] INFO - ----------\n",
      "2023-10-11 15:27:23,000 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?\n",
      "Huawei Mate 60 Pro is a premium smartphone\n",
      "2023-10-11 15:27:23,001 [test.py:41 in test_hf_gen] INFO - ----------\n",
      "2023-10-11 15:27:23,011 [520681597.py:17 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.\n",
      "2023-10-11 15:27:23,012 [520681597.py:17 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.\n",
      "2023-10-11 15:27:23,013 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.\n",
      "2023-10-11 15:27:23,013 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.\n",
      "2023-10-11 15:27:23,014 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.\n",
      "2023-10-11 15:27:23,015 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.\n",
      "2023-10-11 15:27:23,015 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.\n",
      "2023-10-11 15:27:23,017 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.\n",
      "2023-10-11 15:27:23,018 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.\n",
      "2023-10-11 15:27:23,019 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.\n",
      "2023-10-11 15:27:23,020 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.\n",
      "2023-10-11 15:27:23,020 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.\n",
      "2023-10-11 15:27:23,021 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.\n",
      "2023-10-11 15:27:23,022 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.\n",
      "2023-10-11 15:27:23,023 [520681597.py:17 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.\n",
      "2023-10-11 15:27:23,024 [520681597.py:17 in reset_forward] DEBUG - lm_head from flexgen to old.\n",
      "100%|██████████| 197/197 [00:00<00:00, 4160.53it/s]\n"
     ]
    }
   ],
   "source": [
    "with flexgen(checkpoint, policy) as model:\n",
    "    num_prompts = policy.gpu_batch_size * policy.num_gpu_batches\n",
    "    test_hf_gen(checkpoint, model, num_prompts, gen_len=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
