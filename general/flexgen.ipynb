{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Policy, logging\n",
    "# from forward import flexgen\n",
    "from test import test_hf_gen\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "checkpoint = \"facebook/opt-125m\" # 125m 6.7b 13b 30b\n",
    "# checkpoint = \"Salesforce/codegen-350M-mono\"\n",
    "# checkpoint = 'bigscience/bloom-560m'\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=2, \n",
    "    num_gpu_batches=4, \n",
    "    weights_gpu_percent=0.0, \n",
    "    weights_cpu_percent=0.3, \n",
    "    cache_gpu_percent=0.0, \n",
    "    cache_cpu_percent=0.2, \n",
    "    act_gpu_percent=0.0, \n",
    "    act_cpu_percent=0.5, \n",
    "    overlap=True, \n",
    "    pin_weight=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward.py: rewrite layer forward function\n",
    "\n",
    "import torch\n",
    "import functools \n",
    "import contextlib\n",
    "\n",
    "# from minibatch import get_size_info, load_kth_batch_inputs, concat_outputs\n",
    "from utils import get_module_from_name\n",
    "\n",
    "\n",
    "def reset_forward(model, layer_name):        \n",
    "    layer = get_module_from_name(model, layer_name) \n",
    "\n",
    "    if hasattr(layer, \"_flexgen_old_forward\"):\n",
    "        layer.forward = layer._flexgen_old_forward\n",
    "        delattr(layer, \"_flexgen_old_forward\")\n",
    "        logger.debug(f'{layer_name} from flexgen to old.')\n",
    "\n",
    "    if hasattr(layer, \"_test_old_forward\"):\n",
    "        layer.forward = layer._test_old_forward\n",
    "        delattr(layer, \"_test_old_forward\")\n",
    "        logger.debug(f'{layer_name} from test to old.')\n",
    "\n",
    "def to_test_forward(mpl, layer_name, call_layer_log):\n",
    "    layer = get_module_from_name(mpl.model, layer_name) \n",
    "    compute_device = 'cpu' \n",
    "    layer._test_old_forward = old_forward = layer.forward \n",
    "\n",
    "    @functools.wraps(old_forward)\n",
    "    def new_forward(*args, **kwargs):\n",
    "        mpl.load_layer_weights(layer_name, compute_device) \n",
    "\n",
    "        call_layer_log.append(layer_name)  # \n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = old_forward(*args, **kwargs)\n",
    "\n",
    "        mpl.offload_layer_weights(layer_name)\n",
    "        return output\n",
    "\n",
    "    layer.forward = new_forward\n",
    "    logger.debug(f'{layer_name} to test forward') \n",
    "\n",
    "@contextlib.contextmanager\n",
    "def test(mpl, call_layer_log):\n",
    "    model = mpl.model\n",
    "    layer_names = mpl.layer_names\n",
    "\n",
    "    # test run to get layer calling order\n",
    "    for layer_name in layer_names:\n",
    "        to_test_forward(mpl, layer_name, call_layer_log)\n",
    "    yield \n",
    "    for layer_name in layer_names:\n",
    "        reset_forward(model, layer_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13940/752697477.py:115: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  d_data = torch.from_numpy(d_data).to(compute_device)\n"
     ]
    }
   ],
   "source": [
    "from typing import Mapping, Tuple\n",
    "import numpy as np \n",
    "import os \n",
    "import torch\n",
    "from math import floor\n",
    "\n",
    "class MixTensor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        mix_data: Tuple, \n",
    "        split_dim: int, \n",
    "        device: torch.device, \n",
    "        shape: torch.Size,\n",
    "        percents: Mapping[str, float],\n",
    "        file_path: str,\n",
    "        dtype\n",
    "    ):\n",
    "        self.mix_data = mix_data\n",
    "        self.split_dim = split_dim \n",
    "        self.device = device \n",
    "        self.shape = shape \n",
    "        self.percents = percents\n",
    "        self.file_path = file_path\n",
    "        self.dtype = dtype\n",
    "    \n",
    "    def size(self):\n",
    "        return self.shape \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_split_dim(tensor):\n",
    "        dim_sizes = tensor.size()\n",
    "        max_dim, max_size = -1, -1\n",
    "        for dim, size in enumerate(dim_sizes):\n",
    "            if size > max_size:\n",
    "                max_size = size\n",
    "                max_dim = dim \n",
    "        return max_dim \n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_dim_slice(tensor, dim, dim_slice):\n",
    "        return tensor[(dim if dim >= 0 else dim + tensor.dim()) * (slice(None), ) + (dim_slice, )]\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_tensor(tensor, dim, percents):\n",
    "        dim_size = tensor.size(dim)\n",
    "        g_per, c_per, _ = [percents[dev] for dev in ['cuda', 'cpu', 'disk']]\n",
    "        \n",
    "        g_cut = floor(dim_size * g_per)\n",
    "        c_cut = floor(dim_size * (g_per + c_per))\n",
    "\n",
    "        g_data = MixTensor.tensor_dim_slice(tensor, dim, slice(0, g_cut))\n",
    "        c_data = MixTensor.tensor_dim_slice(tensor, dim, slice(g_cut, c_cut))\n",
    "        d_data = MixTensor.tensor_dim_slice(tensor, dim, slice(c_cut, dim_size))\n",
    "        return g_data, c_data, d_data \n",
    "\n",
    "    @classmethod\n",
    "    def from_tensor(\n",
    "        cls, \n",
    "        tensor: torch.Tensor, \n",
    "        percents: Mapping[str, float],\n",
    "        file_path: str \n",
    "    ):\n",
    "        split_dim = cls.get_split_dim(tensor) \n",
    "        device = tensor.device \n",
    "        shape = tensor.shape\n",
    "        dtype = tensor.dtype\n",
    "        \n",
    "        g_data, c_data, d_data = cls.split_tensor(tensor, split_dim, percents) \n",
    "        \n",
    "        g_data = g_data.to('cuda' if torch.cuda.is_available() else 'cpu') if g_data.numel() else None\n",
    "        c_data = c_data.to('cpu') if c_data.numel() else None\n",
    "        if d_data.numel():\n",
    "            d_data = d_data.cpu().numpy()\n",
    "            shape = d_data.shape\n",
    "            np_dtype = d_data.dtype \n",
    "\n",
    "            fp = np.memmap(file_path, mode=\"w+\", shape=shape, dtype=np_dtype)\n",
    "            fp[:] = d_data[:]\n",
    "            d_data = (shape, np_dtype)\n",
    "        else:\n",
    "            d_data = None \n",
    "        mix_data = (g_data, c_data, d_data)\n",
    "\n",
    "        return cls(\n",
    "            mix_data=mix_data,\n",
    "            split_dim=split_dim,\n",
    "            device=device,\n",
    "            shape=shape,\n",
    "            percents=percents,\n",
    "            file_path=file_path,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "    @classmethod \n",
    "    def from_mixtensor(cls, mix_tensor):\n",
    "        self = mix_tensor \n",
    "        return self \n",
    "\n",
    "    def to_tensor(self):\n",
    "        g_data, c_data, d_data = self.mix_data \n",
    "        compute_device = self.device \n",
    "\n",
    "        tensor = []\n",
    "        if g_data is not None:\n",
    "            if g_data.device != torch.device(compute_device):\n",
    "                g_data = g_data.to(compute_device) \n",
    "            tensor.append(g_data)\n",
    "        if c_data is not None:\n",
    "            if c_data.device != torch.device(compute_device):\n",
    "                c_data = c_data.to(compute_device) \n",
    "            tensor.append(c_data)\n",
    "        if d_data is not None:\n",
    "            (shape, np_dtype) = d_data \n",
    "            d_data = np.memmap(self.file_path, shape=shape, dtype=np_dtype, mode='r')\n",
    "            d_data = torch.from_numpy(d_data).to(compute_device)\n",
    "            tensor.append(d_data)\n",
    "            \n",
    "        tensor = torch.cat(tensor, dim=self.split_dim) \n",
    "\n",
    "        return tensor        \n",
    "\n",
    "    def __add__(self, mix_tensor):\n",
    "        assert self.shape == mix_tensor.shape and type(self) == type(mix_tensor) # is same shape mix tensor\n",
    "        res = self.to_tensor() + mix_tensor.to_tensor() \n",
    "        return self.from_tensor(res, self.percents, self.file_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    x = torch.tensor([1,2,3])\n",
    "    m = MixTensor.from_tensor(x, percents={'cuda':0, 'cpu':0.5, 'disk':0.5}, file_path='test/m.dat')\n",
    "    m2 = MixTensor.from_tensor(x, percents={'cuda':0, 'cpu':0.5, 'disk':0.5}, file_path='test/m2.dat')\n",
    "    m = m + m2\n",
    "    print(m.to_tensor())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "from accelerate.utils import honor_type\n",
    "from typing import Mapping\n",
    "from utils import logging \n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "def get_type_size_info(obj): # recursive\n",
    "    if isinstance(obj, (tuple, list)):\n",
    "        return honor_type(obj, (get_type_size_info(o) for o in obj))\n",
    "    elif isinstance(obj, Mapping):\n",
    "        return type(obj)({k:get_type_size_info(v) for k, v in obj.items()})\n",
    "    \n",
    "    elif isinstance(obj, (torch.Tensor, MixTensor, BatchMixTensor)):\n",
    "        return f'{type(obj)}: {obj.size()}'\n",
    "\n",
    "    elif isinstance(obj, (int, bool, type(None))): \n",
    "        return f'{type(obj)}: {obj}'\n",
    "    else:\n",
    "        logger.warning(f'inputs: {obj} of type \\'{type(obj)}\\' is not implemented.')\n",
    "        return f'{type(obj)}: {obj}'\n",
    "\n",
    "def to_mixed_device(obj, policy, prefix): \n",
    "    if isinstance(obj, tuple) and len(obj) == 2 and isinstance(obj[0], torch.Tensor): # KV cache\n",
    "        m0 = MixTensor.from_tensor(\n",
    "            obj[0], \n",
    "            percents={\n",
    "                'cuda':policy.cache_gpu_percent, \n",
    "                'cpu':policy.cache_cpu_percent, \n",
    "                'disk':policy.cache_disk_percent, \n",
    "            }, \n",
    "            file_path=f'{prefix}_key'\n",
    "        )\n",
    "        m1 = MixTensor.from_tensor(\n",
    "            obj[1], \n",
    "            percents={\n",
    "                'cuda':policy.cache_gpu_percent, \n",
    "                'cpu':policy.cache_cpu_percent, \n",
    "                'disk':policy.cache_disk_percent, \n",
    "            }, \n",
    "            file_path=f'{prefix}_value'\n",
    "        )\n",
    "        return (m0, m1)\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return MixTensor.from_tensor(\n",
    "            obj, percents={\n",
    "                'cuda':policy.act_gpu_percent, \n",
    "                'cpu':policy.act_cpu_percent, \n",
    "                'disk':policy.act_disk_percent, \n",
    "            }, \n",
    "            file_path=f'{prefix}'\n",
    "        )\n",
    "    elif isinstance(obj, tuple):\n",
    "        return honor_type(obj, (to_mixed_device(o, policy, f'{prefix}[{i}]') for i, o in enumerate(obj)))\n",
    "    else:\n",
    "        logger.warning(f'inputs: {obj} of type \\'{type(obj)}\\' is not implemented.')\n",
    "        return obj\n",
    "\n",
    "from typing import Iterable\n",
    "class BatchMixTensor:\n",
    "    def __init__(self, batches: Iterable[MixTensor]):\n",
    "        self.dtype = batches[0].dtype\n",
    "        self.device = batches[0].device\n",
    "        self.batches = batches \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.batches[i]\n",
    "    \n",
    "    def __setitem__(self, i, mt: MixTensor):\n",
    "        self.batches[i] = mt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "    \n",
    "    def size(self):\n",
    "        shape = list(self[0].size()) \n",
    "        shape[0] *= len(self)\n",
    "        return torch.Size(shape)\n",
    "\n",
    "    def __add__(self, bmt):\n",
    "        for k in range(len(self)): # K batches \n",
    "            # TODO flexgen: parallelly load k+1\n",
    "            self_k = self[k].to_tensor()\n",
    "            bmt_k = bmt[k].to_tensor()\n",
    "            res = self_k + bmt_k \n",
    "            self[k] = MixTensor.from_tensor(res, self[k].percents, self[k].file_path)\n",
    "        return self \n",
    "\n",
    "def concat_outputs(outputs): # concatenate K outputs to one output\n",
    "    assert len(outputs), 'empty outputs.'\n",
    "    assert isinstance(outputs[0], (MixTensor, torch.Tensor, tuple)), f'not supported type: {type(outputs[0])}.'\n",
    "    \n",
    "    if isinstance(outputs[0], torch.Tensor):\n",
    "        return torch.cat(outputs, dim=0)\n",
    "    elif isinstance(outputs[0], MixTensor):\n",
    "        return BatchMixTensor(outputs)\n",
    "    elif isinstance(outputs[0], tuple):\n",
    "        def f(outputs):\n",
    "            ans = []\n",
    "            for elem in zip(*outputs):\n",
    "                if isinstance(elem[0], torch.Tensor):\n",
    "                    ans.append(torch.cat(elem, dim=0))\n",
    "                elif isinstance(elem[0], MixTensor):\n",
    "                    ans.append(BatchMixTensor(elem))\n",
    "                elif isinstance(elem[0], tuple):\n",
    "                    ans.append(f(elem))\n",
    "                else:\n",
    "                    logger.warning(f'outputs: {elem[0]} of type \\'{type(elem[0])}\\' is not implemented.')\n",
    "                    ans.append(elem[0])\n",
    "            return tuple(ans)\n",
    "\n",
    "        return f(outputs)\n",
    "\n",
    "\n",
    "def load_kth_batch_inputs(inputs, k, ngb): # for both args, kwargs, with a nested structure of tuple/list/dict/Tensor\n",
    "    if isinstance(inputs, (tuple, list)): # e.g. args\n",
    "        return honor_type(inputs, (load_kth_batch_inputs(inp, k, ngb) for inp in inputs))\n",
    "    elif isinstance(inputs, Mapping): # e.g. kwargs\n",
    "        return type(inputs)({key:load_kth_batch_inputs(value, k, ngb) for key, value in inputs.items()})\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        mini_size = inputs.size(0) // ngb\n",
    "        return inputs[k * mini_size:(k + 1) * mini_size]\n",
    "    elif isinstance(inputs, BatchMixTensor):\n",
    "        mini_batch = inputs.batches[k]\n",
    "        return mini_batch.to_tensor()\n",
    "    elif isinstance(inputs, (int, bool, type(None))): \n",
    "        return inputs\n",
    "    else:\n",
    "        logger.warning(f'inputs: {inputs} of type \\'{type(inputs)}\\' is not implemented.')\n",
    "        return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def to_flexgen_forward(mpl, j, compute_device):\n",
    "    # rewrite the j-th layer's forward\n",
    "    layer_name = mpl.layer_names[j]\n",
    "    next_layer_name = mpl.layer_names[(j + 1) % len(mpl.layer_names)]\n",
    "\n",
    "    policy = mpl.policy\n",
    "    ngb = policy.num_gpu_batches\n",
    "\n",
    "    layer = get_module_from_name(mpl.model, layer_name)  \n",
    "    if hasattr(layer, \"_flexgen_old_forward\"): return  \n",
    "    \n",
    "    layer._flexgen_old_forward = old_forward = layer.forward \n",
    "\n",
    "    @functools.wraps(old_forward)\n",
    "    def new_forward(*args, **kwargs):\n",
    "        # pre fwd: load curr & next weights, TODO: cuda stream\n",
    "        mpl.load_layer_weights(layer_name, compute_device) \n",
    "        mpl.load_layer_weights(next_layer_name, compute_device) \n",
    "        \n",
    "        # loop forward pass of K minibatches, TODO: cuda stream\n",
    "        with torch.no_grad():\n",
    "            logger.debug(f'args: {get_type_size_info(args)}')\n",
    "            logger.debug(f'kwargs: {get_type_size_info(kwargs)}')\n",
    "            \n",
    "            outputs = []\n",
    "            for k in range(ngb):\n",
    "                logger.debug(f'layer: {layer_name}, batch: {k}')\n",
    "\n",
    "                # 'pre' fwd: load curr & next inputs (activations, KV cache)\n",
    "                args_k = load_kth_batch_inputs(args, k, ngb)\n",
    "                kwargs_k = load_kth_batch_inputs(kwargs, k, ngb)\n",
    "\n",
    "                # TODO: load args, kwargs to compute device\n",
    "\n",
    "                # the k-th fwd pass\n",
    "                output = old_forward(*args_k, **kwargs_k)\n",
    "\n",
    "                # TODO: 1) output: to mix, 2) args_k, kwargs_k: free\n",
    "                output = to_mixed_device(output, policy, prefix=f'tmp/{layer_name}_output')\n",
    "                outputs.append(output) \n",
    "\n",
    "            output = concat_outputs(outputs)\n",
    "            logger.debug(f'outputs after concat: {get_type_size_info(output)}')                \n",
    "\n",
    "        # post fwd: free curr weights\n",
    "        mpl.offload_layer_weights(layer_name)\n",
    "        return output\n",
    "\n",
    "    layer.forward = new_forward\n",
    "    logger.debug(f'{layer_name} to flexgen forward')\n",
    "\n",
    "@contextlib.contextmanager \n",
    "def flexgen(checkpoint, policy):\n",
    "    # init model \n",
    "    from model import ModelPolicyLoader\n",
    "    mpl = ModelPolicyLoader(checkpoint, policy)\n",
    "    mpl.init_all_weights() # init \n",
    "\n",
    "    # test run, get layer order\n",
    "    call_layer_log = []\n",
    "    with test(mpl, call_layer_log):\n",
    "        from test import test_hf_gen\n",
    "        test_hf_gen(mpl.checkpoint, mpl.model, 1,1, prompts=['0'])\n",
    "\n",
    "    assert len(call_layer_log) == len(mpl.layer_names) and set(call_layer_log) == set(mpl.layer_names)\n",
    "    mpl.layer_names = call_layer_log\n",
    "\n",
    "    # rewrite layer forward\n",
    "    for j, _ in enumerate(mpl.layer_names):\n",
    "        compute_device = 'cpu'\n",
    "        to_flexgen_forward(mpl, j, compute_device)\n",
    "    yield mpl.model \n",
    "    for layer_name in mpl.layer_names:\n",
    "        reset_forward(mpl.model, layer_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 12:27:00,277 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-11 12:27:00,408 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-11 12:27:00,492 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 12:27:00,532 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-10-11 12:27:00,627 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']\n",
      "2023-10-11 12:27:00,629 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'\n",
      "2023-10-11 12:27:00,639 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400\n",
      "2023-10-11 12:27:00,640 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000\n",
      "2023-10-11 12:27:00,642 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464\n",
      "2023-10-11 12:27:00,645 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592\n",
      "2023-10-11 12:27:00,648 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720\n",
      "2023-10-11 12:27:00,651 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848\n",
      "2023-10-11 12:27:00,654 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976\n",
      "2023-10-11 12:27:00,657 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104\n",
      "2023-10-11 12:27:00,660 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232\n",
      "2023-10-11 12:27:00,664 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360\n",
      "2023-10-11 12:27:00,666 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488\n",
      "2023-10-11 12:27:00,669 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616\n",
      "2023-10-11 12:27:00,672 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744\n",
      "2023-10-11 12:27:00,674 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872\n",
      "2023-10-11 12:27:00,677 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-10-11 12:27:00,678 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-10-11 12:27:00,680 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!\n",
      "2023-10-11 12:27:00,686 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: \n",
      "GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)\n",
      "2023-10-11 12:27:00,689 [model.py:241 in init_all_weights] DEBUG - init all weights...\n",
      "model init: loading by policy...: 100%|██████████| 197/197 [00:00<00:00, 4911.80it/s]\n",
      "2023-10-11 12:27:00,733 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward\n",
      "2023-10-11 12:27:00,734 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward\n",
      "2023-10-11 12:27:00,735 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward\n",
      "2023-10-11 12:27:00,736 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward\n",
      "2023-10-11 12:27:00,737 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward\n",
      "2023-10-11 12:27:00,738 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward\n",
      "2023-10-11 12:27:00,740 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward\n",
      "2023-10-11 12:27:00,741 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward\n",
      "2023-10-11 12:27:00,742 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward\n",
      "2023-10-11 12:27:00,743 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward\n",
      "2023-10-11 12:27:00,744 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward\n",
      "2023-10-11 12:27:00,745 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward\n",
      "2023-10-11 12:27:00,746 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward\n",
      "2023-10-11 12:27:00,747 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward\n",
      "2023-10-11 12:27:00,748 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward\n",
      "2023-10-11 12:27:00,749 [520681597.py:42 in to_test_forward] DEBUG - lm_head to test forward\n",
      "2023-10-11 12:27:00,793 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1535: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "2023-10-11 12:27:00,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 12:27:00,969 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 12:27:00,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 12:27:00,973 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 12:27:00,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 12:27:00,986 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0\n",
      "\n",
      "\n",
      "2023-10-11 12:27:00,989 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 12:27:00,998 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,000 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu\n",
      "2023-10-11 12:27:01,009 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu\n",
      "2023-10-11 12:27:01,024 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu\n",
      "2023-10-11 12:27:01,037 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu\n",
      "2023-10-11 12:27:01,051 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,054 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu\n",
      "2023-10-11 12:27:01,064 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,066 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu\n",
      "2023-10-11 12:27:01,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu\n",
      "2023-10-11 12:27:01,083 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,086 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu\n",
      "2023-10-11 12:27:01,093 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu\n",
      "2023-10-11 12:27:01,103 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,106 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu\n",
      "2023-10-11 12:27:01,112 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu\n",
      "2023-10-11 12:27:01,116 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu\n",
      "2023-10-11 12:27:01,127 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,134 [test.py:40 in test_hf_gen] INFO - 0.\n",
      "2023-10-11 12:27:01,135 [test.py:41 in test_hf_gen] INFO - ----------\n",
      "2023-10-11 12:27:01,147 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.\n",
      "2023-10-11 12:27:01,148 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.\n",
      "2023-10-11 12:27:01,148 [520681597.py:22 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.\n",
      "2023-10-11 12:27:01,149 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.\n",
      "2023-10-11 12:27:01,150 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.\n",
      "2023-10-11 12:27:01,151 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.\n",
      "2023-10-11 12:27:01,152 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.\n",
      "2023-10-11 12:27:01,153 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.\n",
      "2023-10-11 12:27:01,154 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.\n",
      "2023-10-11 12:27:01,155 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.\n",
      "2023-10-11 12:27:01,156 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.\n",
      "2023-10-11 12:27:01,157 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.\n",
      "2023-10-11 12:27:01,158 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.\n",
      "2023-10-11 12:27:01,159 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.\n",
      "2023-10-11 12:27:01,159 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.\n",
      "2023-10-11 12:27:01,160 [520681597.py:22 in reset_forward] DEBUG - lm_head from test to old.\n",
      "2023-10-11 12:27:01,161 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward\n",
      "2023-10-11 12:27:01,163 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward\n",
      "2023-10-11 12:27:01,163 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward\n",
      "2023-10-11 12:27:01,164 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward\n",
      "2023-10-11 12:27:01,165 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward\n",
      "2023-10-11 12:27:01,166 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward\n",
      "2023-10-11 12:27:01,167 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward\n",
      "2023-10-11 12:27:01,168 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward\n",
      "2023-10-11 12:27:01,170 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward\n",
      "2023-10-11 12:27:01,170 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward\n",
      "2023-10-11 12:27:01,171 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward\n",
      "2023-10-11 12:27:01,172 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward\n",
      "2023-10-11 12:27:01,172 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward\n",
      "2023-10-11 12:27:01,173 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward\n",
      "2023-10-11 12:27:01,174 [3420557143.py:50 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward\n",
      "2023-10-11 12:27:01,175 [3420557143.py:50 in to_flexgen_forward] DEBUG - lm_head to flexgen forward\n",
      "2023-10-11 12:27:01,219 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2023-10-11 12:27:01,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu\n",
      "2023-10-11 12:27:01,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 12:27:01,361 [3420557143.py:22 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9])\",)\n",
      "2023-10-11 12:27:01,362 [3420557143.py:23 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 12:27:01,363 [3420557143.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0\n",
      "2023-10-11 12:27:01,365 [3420557143.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1\n",
      "2023-10-11 12:27:01,367 [3420557143.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2\n",
      "2023-10-11 12:27:01,368 [3420557143.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3\n",
      "2023-10-11 12:27:01,371 [3420557143.py:43 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])\n",
      "2023-10-11 12:27:01,371 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,373 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu\n",
      "2023-10-11 12:27:01,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 12:27:01,380 [3420557143.py:22 in new_forward] DEBUG - args: (\"<class 'torch.Tensor'>: torch.Size([8, 9])\", \"<class 'int'>: 0\")\n",
      "2023-10-11 12:27:01,381 [3420557143.py:23 in new_forward] DEBUG - kwargs: {}\n",
      "2023-10-11 12:27:01,382 [3420557143.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0\n",
      "2023-10-11 12:27:01,384 [3420557143.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1\n",
      "2023-10-11 12:27:01,385 [3420557143.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2\n",
      "2023-10-11 12:27:01,387 [3420557143.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3\n",
      "2023-10-11 12:27:01,389 [3420557143.py:43 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])\n",
      "2023-10-11 12:27:01,390 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions\n",
      "\n",
      "\n",
      "2023-10-11 12:27:01,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu\n",
      "2023-10-11 12:27:01,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu\n",
      "2023-10-11 12:27:01,403 [3420557143.py:22 in new_forward] DEBUG - args: (\"<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])\",)\n",
      "2023-10-11 12:27:01,405 [3420557143.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': \"<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])\", 'layer_head_mask': \"<class 'NoneType'>: None\", 'past_key_value': \"<class 'NoneType'>: None\", 'output_attentions': \"<class 'bool'>: False\", 'use_cache': \"<class 'bool'>: True\"}\n",
      "2023-10-11 12:27:01,406 [3420557143.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m flexgen(checkpoint, policy) \u001b[39mas\u001b[39;00m model:\n\u001b[1;32m      2\u001b[0m     num_prompts \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mgpu_batch_size \u001b[39m*\u001b[39m policy\u001b[39m.\u001b[39mnum_gpu_batches\n\u001b[0;32m----> 3\u001b[0m     test_hf_gen(checkpoint, model, num_prompts)\n",
      "File \u001b[0;32m~/FlexGen/general/test.py:28\u001b[0m, in \u001b[0;36mtest_hf_gen\u001b[0;34m(checkpoint, model, num_prompts, gen_len, prompts)\u001b[0m\n\u001b[1;32m     25\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(prompts, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[39m# generate\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m generate_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     29\u001b[0m     inputs\u001b[39m.\u001b[39;49minput_ids, \n\u001b[1;32m     30\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49mgen_len, \u001b[39m# max_lengths\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m     \u001b[39m# num_beams=2, #\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m     \u001b[39m# num_beam_groups=2, #\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m     \u001b[39m# diversity_penalty=0.1, #\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39m# do_sample=True, #\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[39m# outputs\u001b[39;00m\n\u001b[1;32m     38\u001b[0m output_texts \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(generate_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1602\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1586\u001b[0m         input_ids,\n\u001b[1;32m   1587\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1599\u001b[0m     )\n\u001b[1;32m   1600\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1601\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1602\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1603\u001b[0m         input_ids,\n\u001b[1;32m   1604\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1605\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1606\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1607\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1608\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1609\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1610\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1611\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1612\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1613\u001b[0m     )\n\u001b[1;32m   1615\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1616\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:2450\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2447\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2449\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2450\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2451\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2452\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2453\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2454\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2455\u001b[0m )\n\u001b[1;32m   2457\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2458\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:944\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    941\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    943\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 944\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m    945\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    946\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    947\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    948\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    949\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    950\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    951\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    952\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    953\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    954\u001b[0m )\n\u001b[1;32m    956\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    958\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:710\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    702\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    703\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    704\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    708\u001b[0m     )\n\u001b[1;32m    709\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    711\u001b[0m         hidden_states,\n\u001b[1;32m    712\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[1;32m    713\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    714\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    715\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    716\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    717\u001b[0m     )\n\u001b[1;32m    719\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    721\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[11], line 39\u001b[0m, in \u001b[0;36mto_flexgen_forward.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs_k, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_k)\n\u001b[1;32m     38\u001b[0m     \u001b[39m# TODO: 1) output: to mix, 2) args_k, kwargs_k: free\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     output \u001b[39m=\u001b[39m to_mixed_device(output, policy, prefix\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtmp/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlayer_name\u001b[39m}\u001b[39;49;00m\u001b[39m_output\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     40\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output) \n\u001b[1;32m     42\u001b[0m output \u001b[39m=\u001b[39m concat_outputs(outputs)\n",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m, in \u001b[0;36mto_mixed_device\u001b[0;34m(obj, policy, prefix)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(obj) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(obj[\u001b[39m0\u001b[39m], torch\u001b[39m.\u001b[39mTensor): \u001b[39m# KV cache\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     m0 \u001b[39m=\u001b[39m MixTensor\u001b[39m.\u001b[39mfrom_tensor(\n\u001b[1;32m     28\u001b[0m         obj[\u001b[39m0\u001b[39m], \n\u001b[1;32m     29\u001b[0m         percents\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m         file_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39m_key\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[0;32m---> 36\u001b[0m     m1 \u001b[39m=\u001b[39m MixTensor\u001b[39m.\u001b[39;49mfrom_tensor(\n\u001b[1;32m     37\u001b[0m         obj[\u001b[39m1\u001b[39;49m], \n\u001b[1;32m     38\u001b[0m         percents\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     39\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m:policy\u001b[39m.\u001b[39;49mcache_gpu_percent, \n\u001b[1;32m     40\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m:policy\u001b[39m.\u001b[39;49mcache_cpu_percent, \n\u001b[1;32m     41\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mdisk\u001b[39;49m\u001b[39m'\u001b[39;49m:policy\u001b[39m.\u001b[39;49mcache_disk_percent, \n\u001b[1;32m     42\u001b[0m         }, \n\u001b[1;32m     43\u001b[0m         file_path\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mprefix\u001b[39m}\u001b[39;49;00m\u001b[39m_value\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m (m0, m1)\n\u001b[1;32m     46\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mTensor):\n",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m, in \u001b[0;36mMixTensor.from_tensor\u001b[0;34m(cls, tensor, percents, file_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[39mcls\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     file_path: \u001b[39mstr\u001b[39m \n\u001b[1;32m     62\u001b[0m ):\n\u001b[0;32m---> 63\u001b[0m     split_dim \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_split_dim(tensor) \n\u001b[1;32m     64\u001b[0m     device \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mdevice \n\u001b[1;32m     65\u001b[0m     shape \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mMixTensor.get_split_dim\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_split_dim\u001b[39m(tensor):\n\u001b[0;32m---> 31\u001b[0m     dim_sizes \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m     32\u001b[0m     max_dim, max_size \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     33\u001b[0m     \u001b[39mfor\u001b[39;00m dim, size \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dim_sizes):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "with flexgen(checkpoint, policy) as model:\n",
    "    num_prompts = policy.gpu_batch_size * policy.num_gpu_batches\n",
    "    test_hf_gen(checkpoint, model, num_prompts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
