{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    style='{',\n",
    "    format='{asctime} [{filename}:{lineno} in {funcName}] {levelname} - {message}',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\".log\", 'w'),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    level=logging.INFO\n",
    ")\n",
    "logging.info('Importing...')\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import find_tied_parameters, named_module_tensors, set_module_tensor_to_device\n",
    "\n",
    "from policy import Policy\n",
    "logging.info('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"facebook/opt-13b\" # 1.3b 6.7b 13b 30b 66b \n",
    "offload_folder = 'offload/' + checkpoint.replace('/', '.')\n",
    "\n",
    "# empty model\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    __slots__ = () \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=16, \n",
    "    num_gpu_batches=8, \n",
    "    weights_gpu_percent=0.0, \n",
    "    weights_cpu_percent=0.3, \n",
    "    cache_gpu_percent=0.0, \n",
    "    cache_cpu_percent=0.2, \n",
    "    act_gpu_percent=0.0, \n",
    "    act_cpu_percent=0.5, \n",
    "    overlap=True, \n",
    "    pin_weight=True,\n",
    ")\n",
    "\n",
    "def get_layers_dict(lm_model: Module, prefix: str='') -> dict:\n",
    "    # return a dict of {layer_name : layer_module ('meta')} with only leaf nodes & transformer layers\n",
    "    layers_dict = {}\n",
    "    for name, module in lm_model.named_children():\n",
    "        # leaf nodes\n",
    "        if len(list(module.named_children())) == 0:\n",
    "            layers_dict[prefix+name] = module\n",
    "        # ModuleList: transformer  \n",
    "        elif isinstance(module, ModuleList):\n",
    "            for block_name, block_module in module.named_children():\n",
    "                layers_dict[prefix+name+'.'+block_name] = block_module\n",
    "        else:\n",
    "            layers_dict.update(get_layers_dict(module, prefix+name+'.'))\n",
    "    return layers_dict\n",
    "\n",
    "def named_module_tensors(module: Module, include_buffers: bool = True, recurse: bool = True):\n",
    "    for named_parameter in module.named_parameters(recurse=recurse):\n",
    "        yield named_parameter\n",
    "\n",
    "    if include_buffers:\n",
    "        for named_buffer in module.named_buffers(recurse=recurse):\n",
    "            yield named_buffer\n",
    "\n",
    "def get_device(cur_percent, percents, choices):\n",
    "    # choose a device (gpu / cpu / disk) for a weight tensor by its percent of size\n",
    "    percents = np.cumsum(percents)\n",
    "    assert np.abs(percents[-1] - 1.0) < 1e-5, f'{percents}'\n",
    "\n",
    "    for i in range(len(percents)):\n",
    "        if cur_percent < percents[i]:\n",
    "            return choices[i]\n",
    "    return choices[-1]\n",
    "\n",
    "def get_policy_weight_map(model: PreTrainedModel, policy: Policy):\n",
    "    \"\"\"{module_name: device}\"\"\"\n",
    "    assert model.device == torch.device('meta'), 'model is not on device meta.'\n",
    "    \n",
    "    # to ensure the tied params are allocated to the same device in the weight_map\n",
    "    model.tie_weights()\n",
    "    tied_params = find_tied_parameters(model)\n",
    "\n",
    "    # layers to be scheduled\n",
    "    layers_dict = get_layers_dict(model)\n",
    "\n",
    "    # device assignment for each tensor in the model\n",
    "    weight_assign_dict = {}\n",
    "    devices = ['cuda', 'cpu', 'disk']\n",
    "    percents_target = np.array([\n",
    "        policy.weights_gpu_percent, \n",
    "        policy.weights_cpu_percent, \n",
    "        policy.weights_disk_percent\n",
    "    ])\n",
    "    \n",
    "    # model size (parameters + buffers), here we do not repeatly sum the tied paramters \n",
    "    size_total = sum(np.prod(tensor.shape) for _, tensor in named_module_tensors(model))\n",
    "    size_done, size_todo = 0, size_total\n",
    "    percents_done, percents_todo = 0 * percents_target, percents_target  \n",
    "\n",
    "    for layer_name, layer_module in layers_dict.items():\n",
    "        # current layer\n",
    "        tensor_sizes = [np.prod(tensor.shape) for _, tensor in named_module_tensors(layer_module)]\n",
    "        tensor_sizes_cumsum = np.cumsum(tensor_sizes)\n",
    "\n",
    "        device_allo_size_dict = {device: 0 for device in devices} # to balance the percents\n",
    "        for i, (tensor_name, tensor) in enumerate(named_module_tensors(layer_module)):\n",
    "            abs_tensor_name = layer_name + '.' + tensor_name\n",
    "\n",
    "            def find_processed_tied(abs_tensor_name, tied_params, weight_assign_dict):\n",
    "                # find the processed parameter (in weight_assign_dict) of the tied parameters.\n",
    "                for tp in tied_params:\n",
    "                    if abs_tensor_name in tp:\n",
    "                        for p in tp:\n",
    "                            if p in weight_assign_dict:\n",
    "                                return p, tuple(tp)\n",
    "                return None\n",
    "            \n",
    "            processed_tied = find_processed_tied(abs_tensor_name, tied_params, weight_assign_dict) \n",
    "            if processed_tied: # this tensor is tied and processed.\n",
    "                p, tp = processed_tied\n",
    "                weight_assign_dict[abs_tensor_name] = {\n",
    "                    # 'shape':  tensor.shape,\n",
    "                    'assigned_device': weight_assign_dict[p]['assigned_device'],\n",
    "                    'tied': tp\n",
    "                }\n",
    "            else:\n",
    "                mid_percent = (tensor_sizes_cumsum[i] - tensor_sizes[i] / 2) / tensor_sizes_cumsum[-1] # tensor mid size percent \n",
    "                device = get_device(mid_percent, percents_todo, devices)\n",
    "                weight_assign_dict[abs_tensor_name] = {\n",
    "                    'shape':  tensor.shape,\n",
    "                    'assigned_device': device\n",
    "                }\n",
    "                \n",
    "                device_allo_size_dict[device] += tensor_sizes[i]\n",
    "\n",
    "        # update percents_todo\n",
    "        size_layer = sum(device_allo_size_dict.values())\n",
    "        if size_layer > 0:\n",
    "            device_allo_percents = np.array([device_allo_size_dict[device] * 1. for device in devices]) / size_layer\n",
    "            percents_done = (percents_done * size_done + device_allo_percents * size_layer) / (size_done + size_layer)      \n",
    "        size_done += size_layer\n",
    "        size_todo -= size_layer\n",
    "        if size_todo > 0:\n",
    "            percents_todo = (size_total * percents_target - size_done * percents_done) / size_todo \n",
    "        \n",
    "        logging.info(f'{layer_name}, {percents_done}, size_todo: {size_todo}')\n",
    "\n",
    "\n",
    "    device_map = {k:v['assigned_device'] for k, v in weight_assign_dict.items()}\n",
    "    logging.info('device_map is prepared!')\n",
    "\n",
    "    mem_g = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if 'cuda' in v['assigned_device'] and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem_c = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if v['assigned_device'] == 'cpu' and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem_d = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if v['assigned_device'] == 'disk' and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem = mem_d + mem_c + mem_g\n",
    "    logging.info(f'CausalLM {checkpoint} is to be loaded on: ' \n",
    "                 f'\\nGPU Mem {mem_g:.2f} GiB ({mem_g / mem:.2%}), ' \n",
    "                 f'CPU Mem {mem_c:.2f} GiB ({mem_c / mem:.2%}), '\n",
    "                 f'Disk Mem {mem_d:.2f} Gib ({mem_d / mem:.2%})')\n",
    "    \n",
    "    # prepare output\n",
    "    output = {\n",
    "        'model': model,\n",
    "        'tied_params': tied_params,\n",
    "        'layers_dict': layers_dict,\n",
    "        'weight_assign_dict': weight_assign_dict,\n",
    "        'device_map': device_map\n",
    "    }\n",
    "    output = AttrDict(output)\n",
    "    return output\n",
    "\n",
    "output = get_policy_weight_map(model, policy)\n",
    "policy_device_map = output.device_map\n",
    "flexgen_layers = output.layers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_disk(checkpoint, offload_folder):\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "    model.tie_weights()\n",
    "    tensor_names = [n for n, _ in named_module_tensors(model, include_buffers=True, recurse=True)]\n",
    "    dat_file_names = [file[:-4] for file in os.listdir(offload_folder) if file.endswith('.dat')]\n",
    "    # logging.info(set(tensor_names) - set(dat_file_names))\n",
    "    return set(tensor_names) == set(dat_file_names)\n",
    "\n",
    "if not check_disk(checkpoint, offload_folder):\n",
    "    # download and process to .dat files\n",
    "    disk_weight_map = {name:'disk' for name in policy_device_map}\n",
    "    try:\n",
    "        AutoModelForCausalLM.from_pretrained(\n",
    "            checkpoint, \n",
    "            device_map=disk_weight_map, \n",
    "            offload_folder=offload_folder, \n",
    "            offload_state_dict=True\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if check_disk(checkpoint, offload_folder):\n",
    "    logging.info(f'The whole model has been downloaded an processed to offload_folder: \\'{offload_folder}\\'')\n",
    "else:\n",
    "    err_msg = 'Mismatch between offload folder and model'\n",
    "    logging.error(err_msg)\n",
    "    raise RuntimeError(err_msg)\n",
    "\n",
    "# get empty model\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "model.tie_weights()\n",
    "model.eval()\n",
    "logging.info(f'Got empty CausalLM: \\'{checkpoint}\\' on meta device.')\n",
    "\n",
    "tied_params = find_tied_parameters(model)\n",
    "print(tied_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_from_name(lm_model, name):\n",
    "    splits = name.split('.')\n",
    "    module = lm_model\n",
    "    for split in splits:\n",
    "        if split == '': \n",
    "            continue \n",
    "\n",
    "        new_module = getattr(module, split)\n",
    "        if new_module is None:\n",
    "            raise ValueError(f\"{module} has no attribute {split}.\")\n",
    "        module = new_module\n",
    "    return module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm \n",
    "import gc \n",
    "\n",
    "dat_files = [f for f in os.listdir(offload_folder) if f.endswith('.dat')]\n",
    "with open(os.path.join(offload_folder, 'index.json'), 'r') as f:\n",
    "    index = json.load(f) # {name: {dtype, shape}}\n",
    "\n",
    "def flexgen_load_module_tensor(model, tensor_name, device):\n",
    "    old_tensor_name = tensor_name\n",
    "    \n",
    "    # if tensor_name is tied and without a .dat file\n",
    "    for group in tied_params:\n",
    "        if tensor_name in group:\n",
    "            for name in group:\n",
    "                if name + '.dat' in dat_files:\n",
    "                    tensor_name = name \n",
    "\n",
    "    metadata = index[tensor_name]\n",
    "\n",
    "    # copied from accelerate.utils.offload\n",
    "    shape = tuple(metadata[\"shape\"])\n",
    "    if shape == ():\n",
    "        # NumPy memory-mapped arrays can't have 0 dims so it was saved as 1d tensor\n",
    "        shape = (1,)\n",
    "\n",
    "    dtype = metadata[\"dtype\"]\n",
    "    if dtype == \"bfloat16\":\n",
    "        # NumPy does not support bfloat16 so this was saved as a int16\n",
    "        dtype = \"int16\"\n",
    "    \n",
    "    # load .dat file\n",
    "    save_path = os.path.join(offload_folder, tensor_name + '.dat')\n",
    "\n",
    "    # to device \n",
    "    np_memmap = np.memmap(save_path, dtype=dtype, shape=shape, mode='r') \n",
    "    tmp = torch.from_numpy(np_memmap).to(device) \n",
    "    set_module_tensor_to_device(model, old_tensor_name, device, tmp)\n",
    "\n",
    "def flexgen_offload_module_tensor(model, tensor_name):\n",
    "    tensor = get_obj_from_name(model, tensor_name)\n",
    "    device = policy_device_map[tensor_name]\n",
    "    device = device if device != 'disk' else 'meta' \n",
    "    if tensor.device != device:\n",
    "        set_module_tensor_to_device(model, tensor_name, device, tensor) # gtoc, ctog\n",
    "\n",
    "def policy_init(model, policy_device_map):\n",
    "    for tensor_name, device in tqdm(policy_device_map.items(), desc='model init: loading by policy...'):\n",
    "        if device != 'disk':\n",
    "            flexgen_load_module_tensor(model, tensor_name, device) \n",
    "\n",
    "    logging.info('model has been loaded by policy.')        \n",
    "\n",
    "policy_init(model, policy_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.hooks import (\n",
    "    ModelHook, \n",
    "    SequentialHook, \n",
    "    add_hook_to_module, \n",
    "    remove_hook_from_module\n",
    ")\n",
    "# TODO: add_zigzag_hook, remove_zigzag_hook\n",
    "\n",
    "from accelerate.utils import (\n",
    "    find_device,\n",
    "    named_module_tensors,\n",
    "    send_to_device,\n",
    "    set_module_tensor_to_device,\n",
    ")\n",
    "\n",
    "# global buffers: {layer_name: value_holder}\n",
    "weight_home = {}\n",
    "weight_load_buf = {}\n",
    "\n",
    "act_home = {}\n",
    "act_load_buf = {}\n",
    "act_store_buf = {}\n",
    "\n",
    "kv_home = {} \n",
    "kv_load_buf = {}\n",
    "kv_store_buf = {}\n",
    "\n",
    "# TODO: cuda streams / cpu threads (?)\n",
    "\n",
    "\n",
    "from typing import Optional, Union, Mapping\n",
    "class LayerWeightHook(ModelHook):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model, \n",
    "        layer_name,\n",
    "        compute_device,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name\n",
    "        self.compute_device = compute_device\n",
    "        \n",
    "    def check_dat(self, dat_file):\n",
    "        return os.path.isfile(dat_file)\n",
    "\n",
    "    def init_hook(self, module):\n",
    "        self.weight_names = [self.layer_name + '.' + name for name, _ in module.named_parameters()]\n",
    "\n",
    "        dat_files = [os.path.join(offload_folder, w + '.dat') for w in self.weight_names]\n",
    "        assert all([self.check_dat(f) for f in dat_files]), 'dat file error'\n",
    "        \n",
    "        return module \n",
    "    \n",
    "    def pre_forward(self, module: Module, *args, **kwargs):\n",
    "        # load weights\n",
    "        for w in self.weight_names:\n",
    "            flexgen_load_module_tensor(model, w, self.compute_device)\n",
    "        print(f'pre {self.layer_name} forward')\n",
    "        return args, kwargs\n",
    "    def post_forward(self, module, output):\n",
    "        # offload weights\n",
    "        for w in self.weight_names:\n",
    "            flexgen_offload_module_tensor(model, w, policy_device_map[w])\n",
    "        print(f'post {self.layer_name} forward')\n",
    "        return output\n",
    "    \n",
    "    def detach_hook(self, module):\n",
    "        return module \n",
    "\n",
    "class LayerActHook(ModelHook): pass \n",
    "class LayerKVCacheHook(ModelHook): pass \n",
    "\n",
    "def to_zigzag_forward(layer):\n",
    "    pass \n",
    "\n",
    "\n",
    "# clear hooks \n",
    "remove_hook_from_module(model, recurse=True)\n",
    "\n",
    "compute_device = 'cpu' \n",
    "\n",
    "for layer_name in flexgen_layers:\n",
    "    layer_module = get_obj_from_name(model, layer_name)\n",
    "\n",
    "    layer_weight_hook = LayerWeightHook(model=model, layer_name=layer_name, compute_device=compute_device)\n",
    "    add_hook_to_module(layer_module, layer_weight_hook, append=True)\n",
    "    break\n",
    "\n",
    "# generate test\n",
    "\n",
    "prompts = [\n",
    "    'Who are you? Are you conscious?',\n",
    "    'Where is Deutschland?',\n",
    "    'How is Huawei Mate 60 Pro?'\n",
    "] * 4\n",
    "\n",
    "prompt_len = 20\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompts, padding=\"max_length\", max_length=prompt_len, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    max_length=30 + prompt_len,\n",
    "    # num_beams=2,\n",
    "    # num_beam_groups=2,\n",
    "    # diversity_penalty=0.1,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "output_texts = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "for output_text in output_texts:\n",
    "    print(output_text)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_module._hf_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load / offload\n",
    "#     module object, .dat file path\n",
    "#     layer: pre / post forward hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_activation_assignment(num_layers, offload_config: Policy):\n",
    "    logging.debug(f\"<compute_activation_assignment> enter\")\n",
    "    gpu_batch_limit = int(offload_config.num_gpu_batches * offload_config.act_gpu_percent)\n",
    "    cpu_batch_limit = int(offload_config.num_gpu_batches * (offload_config.act_gpu_percent + offload_config.act_cpu_percent))\n",
    "    logging.debug(f\"<compute_activation_assignment> gpu_batch_limit: {gpu_batch_limit}, cpu_batch_limit: {cpu_batch_limit}\")\n",
    "    \n",
    "    act_assign_dict = {}\n",
    "    for l in range(num_layers):\n",
    "        for i in range(offload_config.num_gpu_batches):\n",
    "            act_key = f\"layer.{l}_index.{i}\"\n",
    "            if i < gpu_batch_limit:\n",
    "                device = 'cuda'\n",
    "            elif i < cpu_batch_limit:\n",
    "                device = 'cpu'\n",
    "            else:\n",
    "                device = 'disk'\n",
    "            act_assign_dict[act_key]= {'assigned_device': device}\n",
    "    return act_assign_dict\n",
    "\n",
    "\n",
    "def compute_kv_cache_assignment(num_layers, offload_config: OffloadConfig):\n",
    "    logging.debug(f\"<compute_kv_cache_assignment> enter\")\n",
    "    gpu_batch_limit = int(offload_config.num_gpu_batches * offload_config.cache_gpu_percent)\n",
    "    cpu_batch_limit = int(offload_config.num_gpu_batches * (offload_config.cache_gpu_percent + offload_config.cache_cpu_percent))\n",
    "    logging.debug(f\"<compute_kv_cache_assignment> gpu_batch_limit: {gpu_batch_limit}, cpu_batch_limit: {cpu_batch_limit}\")\n",
    "    \n",
    "    act_assign_dict = {}\n",
    "    for l in range(num_layers):\n",
    "        for i in range(offload_config.num_gpu_batches):\n",
    "            key_cache_key = f\"key_layer.{l}_index.{i}\"\n",
    "            value_cache_key = f\"key_layer.{l}_index.{i}\"\n",
    "            if i < gpu_batch_limit:\n",
    "                device = 'cuda'\n",
    "            elif i < cpu_batch_limit:\n",
    "                device = 'cpu'\n",
    "            else:\n",
    "                device = 'disk'\n",
    "            act_assign_dict[key_cache_key] = {'assigned_device': device}\n",
    "            act_assign_dict[value_cache_key] = {'assigned_device': device}\n",
    "    return act_assign_dict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
