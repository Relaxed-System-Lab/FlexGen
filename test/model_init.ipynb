{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 09:01:10,984 [1432878970.py:11 in <module>] INFO - Importing...\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-09-21 09:01:16,015 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpuj3gynbx\n",
      "2023-09-21 09:01:16,017 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpuj3gynbx/_remote_module_non_scriptable.py\n",
      "2023-09-21 09:01:17.929241: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 09:01:22,796 [1432878970.py:21 in <module>] INFO - Done!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    style='{',\n",
    "    format='{asctime} [{filename}:{lineno} in {funcName}] {levelname} - {message}',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\".log\", 'w'),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    level=logging.INFO\n",
    ")\n",
    "logging.info('Importing...')\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import find_tied_parameters\n",
    "logging.info('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 09:01:23,838 [1104660553.py:3 in <module>] INFO - Initializing CausalLM: 'facebook/opt-13b'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['lm_head.weight', 'model.decoder.embed_tokens.weight']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"facebook/opt-13b\" # 1.3b 6.7b 13b 30b 66b \n",
    "\n",
    "logging.info(f'Initializing CausalLM: \\'{checkpoint}\\'')\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "    \n",
    "# model.base_model_prefix # -> 'model'\n",
    "\n",
    "model.tie_weights()\n",
    "tied_params = find_tied_parameters(model)\n",
    "tied_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 09:15:30,856 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 12596080640\n",
      "2023-09-21 09:15:30,857 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.embed_positions, [0. 0. 1.], size_todo: 12585584640\n",
      "2023-09-21 09:15:30,859 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 12585574400\n",
      "2023-09-21 09:15:30,861 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.0, [0.         0.18003639 0.81996361], size_todo: 12270935040\n",
      "2023-09-21 09:15:30,863 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.1, [0.         0.23378988 0.76621012], size_todo: 11956295680\n",
      "2023-09-21 09:15:30,864 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.2, [0.         0.25962997 0.74037003], size_todo: 11641656320\n",
      "2023-09-21 09:15:30,866 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.3, [0.         0.27481753 0.72518247], size_todo: 11327016960\n",
      "2023-09-21 09:15:30,868 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.4, [0.         0.28481405 0.71518595], size_todo: 11012377600\n",
      "2023-09-21 09:15:30,870 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.5, [0.        0.2918925 0.7081075], size_todo: 10697738240\n",
      "2023-09-21 09:15:30,871 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.6, [0.         0.29716785 0.70283215], size_todo: 10383098880\n",
      "2023-09-21 09:15:30,874 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.7, [0.         0.30125122 0.69874878], size_todo: 10068459520\n",
      "2023-09-21 09:15:30,876 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.8, [0.         0.30450561 0.69549439], size_todo: 9753820160\n",
      "2023-09-21 09:15:30,877 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.9, [0.         0.30716019 0.69283981], size_todo: 9439180800\n",
      "2023-09-21 09:15:30,879 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.10, [0.         0.30936679 0.69063321], size_todo: 9124541440\n",
      "2023-09-21 09:15:30,880 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.11, [0.         0.31122999 0.68877001], size_todo: 8809902080\n",
      "2023-09-21 09:15:30,882 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.12, [0.         0.31282417 0.68717583], size_todo: 8495262720\n",
      "2023-09-21 09:15:30,883 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.13, [0.         0.31420366 0.68579634], size_todo: 8180623360\n",
      "2023-09-21 09:15:30,885 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.14, [0.        0.3154091 0.6845909], size_todo: 7865984000\n",
      "2023-09-21 09:15:30,887 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.15, [0.         0.31152735 0.68847265], size_todo: 7551344640\n",
      "2023-09-21 09:15:30,888 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.16, [0.         0.31274766 0.68725234], size_todo: 7236705280\n",
      "2023-09-21 09:15:30,890 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.17, [0.         0.30941891 0.69058109], size_todo: 6922065920\n",
      "2023-09-21 09:15:30,891 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.18, [0.         0.31062249 0.68937751], size_todo: 6607426560\n",
      "2023-09-21 09:15:30,893 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.19, [0.         0.30771494 0.69228506], size_todo: 6292787200\n",
      "2023-09-21 09:15:30,894 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.20, [0.         0.30888634 0.69111366], size_todo: 5978147840\n",
      "2023-09-21 09:15:30,896 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.21, [0.         0.30630924 0.69369076], size_todo: 5663508480\n",
      "2023-09-21 09:15:30,897 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.22, [0.         0.30744135 0.69255865], size_todo: 5348869120\n",
      "2023-09-21 09:15:30,899 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.23, [0.         0.30512979 0.69487021], size_todo: 5034229760\n",
      "2023-09-21 09:15:30,901 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.24, [0.         0.30621994 0.69378006], size_todo: 4719590400\n",
      "2023-09-21 09:15:30,902 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.25, [0.         0.30412605 0.69587395], size_todo: 4404951040\n",
      "2023-09-21 09:15:30,904 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.26, [0.         0.30517395 0.69482605], size_todo: 4090311680\n",
      "2023-09-21 09:15:30,905 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.27, [0.         0.30326146 0.69673854], size_todo: 3775672320\n",
      "2023-09-21 09:15:30,907 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.28, [0.         0.30426812 0.69573188], size_todo: 3461032960\n",
      "2023-09-21 09:15:30,908 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.29, [0.         0.30250897 0.69749103], size_todo: 3146393600\n",
      "2023-09-21 09:15:30,910 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.30, [0.         0.30347605 0.69652395], size_todo: 2831754240\n",
      "2023-09-21 09:15:30,911 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.31, [0.         0.30184811 0.69815189], size_todo: 2517114880\n",
      "2023-09-21 09:15:30,913 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.32, [0.         0.30277757 0.69722243], size_todo: 2202475520\n",
      "2023-09-21 09:15:30,914 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.33, [0.         0.30126309 0.69873691], size_todo: 1887836160\n",
      "2023-09-21 09:15:30,916 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.34, [0.         0.30215702 0.69784298], size_todo: 1573196800\n",
      "2023-09-21 09:15:30,917 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.35, [0.         0.30074158 0.69925842], size_todo: 1258557440\n",
      "2023-09-21 09:15:30,919 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.36, [0.         0.30160205 0.69839795], size_todo: 943918080\n",
      "2023-09-21 09:15:30,920 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.37, [0.         0.30027375 0.69972625], size_todo: 629278720\n",
      "2023-09-21 09:15:30,922 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.38, [0.         0.30110278 0.69889722], size_todo: 314639360\n",
      "2023-09-21 09:15:30,923 [1337013409.py:154 in get_policy_weight_map] INFO - model.decoder.layers.39, [0.         0.29985174 0.70014826], size_todo: 0\n",
      "2023-09-21 09:15:30,924 [1337013409.py:154 in get_policy_weight_map] INFO - lm_head, [0.         0.29985174 0.70014826], size_todo: 0\n",
      "2023-09-21 09:15:30,925 [1337013409.py:158 in get_policy_weight_map] INFO - device_map is prepared!\n",
      "2023-09-21 09:15:30,932 [1337013409.py:164 in get_policy_weight_map] INFO - CausalLM facebook/opt-13b is to be loaded on: \n",
      "GPU Mem 0.00 GiB (0.00%), CPU Mem 7.18 GiB (29.99%), Disk Mem 16.76 Gib (70.01%)\n"
     ]
    }
   ],
   "source": [
    "class AttrDict(dict):\n",
    "    __slots__ = () \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Policy:\n",
    "    gpu_batch_size: int\n",
    "    num_gpu_batches: int\n",
    "\n",
    "    # percent of weights/cache/activations on GPU/CPU/Disk %\n",
    "    weights_gpu_percent: float\n",
    "    weights_cpu_percent: float\n",
    "    cache_gpu_percent: float\n",
    "    cache_cpu_percent: float\n",
    "    act_gpu_percent: float\n",
    "    act_cpu_percent: float\n",
    "\n",
    "    # Whether to overlap the I/O and compute\n",
    "    overlap: bool\n",
    "\n",
    "    # Whether to use pinned memory for weights on CPU\n",
    "    pin_weight: bool\n",
    "\n",
    "    @property\n",
    "    def weights_disk_percent(self):\n",
    "        return 1.0 - self.weights_gpu_percent - self.weights_cpu_percent\n",
    "\n",
    "    @property\n",
    "    def cache_disk_percent(self):\n",
    "        return 1.0 - self.cache_gpu_percent - self.cache_cpu_percent\n",
    "\n",
    "    @property\n",
    "    def act_disk_percent(self):\n",
    "        return 1.0 - self.act_gpu_percent - self.act_cpu_percent\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=8, \n",
    "    num_gpu_batches=8, \n",
    "    weights_gpu_percent=0.0, \n",
    "    weights_cpu_percent=0.3, \n",
    "    cache_gpu_percent=0.0, \n",
    "    cache_cpu_percent=0.2, \n",
    "    act_gpu_percent=0.0, \n",
    "    act_cpu_percent=0.5, \n",
    "    overlap=True, \n",
    "    pin_weight=True,\n",
    ")\n",
    "\n",
    "def get_layers_dict(lm_model: Module, prefix: str='') -> dict:\n",
    "    # return a dict of {layer_name : layer_module ('meta')} with only leaf nodes & transformer layers\n",
    "    layers_dict = {}\n",
    "    for name, module in lm_model.named_children():\n",
    "        # leaf nodes\n",
    "        if len(list(module.named_children())) == 0:\n",
    "            layers_dict[prefix+name] = module\n",
    "        # ModuleList: transformer  \n",
    "        elif isinstance(module, ModuleList):\n",
    "            for block_name, block_module in module.named_children():\n",
    "                layers_dict[prefix+name+'.'+block_name] = block_module\n",
    "        else:\n",
    "            layers_dict.update(get_layers_dict(module, prefix+name+'.'))\n",
    "    return layers_dict\n",
    "\n",
    "def named_module_tensors(module: Module, include_buffers: bool = True, recurse: bool = True):\n",
    "    for named_parameter in module.named_parameters(recurse=recurse):\n",
    "        yield named_parameter\n",
    "\n",
    "    if include_buffers:\n",
    "        for named_buffer in module.named_buffers(recurse=recurse):\n",
    "            yield named_buffer\n",
    "\n",
    "def get_device(cur_percent, percents, choices):\n",
    "    # choose a device (gpu / cpu / disk) for a weight tensor by its percent of size\n",
    "    percents = np.cumsum(percents)\n",
    "    assert np.abs(percents[-1] - 1.0) < 1e-5, f'{percents}'\n",
    "\n",
    "    for i in range(len(percents)):\n",
    "        if cur_percent < percents[i]:\n",
    "            return choices[i]\n",
    "    return choices[-1]\n",
    "\n",
    "def get_policy_weight_map(model: PreTrainedModel, policy: Policy):\n",
    "    \"\"\"{module_name: device}\"\"\"\n",
    "    assert model.device == torch.device('meta'), 'model is not on device meta.'\n",
    "    \n",
    "    # to ensure the tied params are allocated to the same device in the weight_map\n",
    "    model.tie_weights()\n",
    "    tied_params = find_tied_parameters(model)\n",
    "\n",
    "    # layers to be scheduled\n",
    "    layers_dict = get_layers_dict(model)\n",
    "\n",
    "    # device assignment for each tensor in the model\n",
    "    weight_assign_dict = {}\n",
    "    devices = ['cuda', 'cpu', 'disk']\n",
    "    percents_target = np.array([\n",
    "        policy.weights_gpu_percent, \n",
    "        policy.weights_cpu_percent, \n",
    "        policy.weights_disk_percent\n",
    "    ])\n",
    "    \n",
    "    # model size (parameters + buffers), here we do not repeatly sum the tied paramters \n",
    "    size_total = sum(np.prod(tensor.shape) for _, tensor in named_module_tensors(model))\n",
    "    size_done, size_todo = 0, size_total\n",
    "    percents_done, percents_todo = 0 * percents_target, percents_target  \n",
    "\n",
    "    for layer_name, layer_module in layers_dict.items():\n",
    "        # current layer\n",
    "        tensor_sizes = [np.prod(tensor.shape) for _, tensor in named_module_tensors(layer_module)]\n",
    "        tensor_sizes_cumsum = np.cumsum(tensor_sizes)\n",
    "\n",
    "        device_allo_size_dict = {device: 0 for device in devices} # to balance the percents\n",
    "        for i, (tensor_name, tensor) in enumerate(named_module_tensors(layer_module)):\n",
    "            abs_tensor_name = layer_name + '.' + tensor_name\n",
    "\n",
    "            def find_processed_tied(abs_tensor_name, tied_params, weight_assign_dict):\n",
    "                # find the processed parameter (in weight_assign_dict) of the tied parameters.\n",
    "                for tp in tied_params:\n",
    "                    if abs_tensor_name in tp:\n",
    "                        for p in tp:\n",
    "                            if p in weight_assign_dict:\n",
    "                                return p, tuple(tp)\n",
    "                return None\n",
    "            \n",
    "            processed_tied = find_processed_tied(abs_tensor_name, tied_params, weight_assign_dict) \n",
    "            if processed_tied: # this tensor is tied and processed.\n",
    "                p, tp = processed_tied\n",
    "                weight_assign_dict[abs_tensor_name] = {\n",
    "                    # 'shape':  tensor.shape,\n",
    "                    'assigned_device': weight_assign_dict[p]['assigned_device'],\n",
    "                    'tied': tp\n",
    "                }\n",
    "            else:\n",
    "                mid_percent = (tensor_sizes_cumsum[i] - tensor_sizes[i] / 2) / tensor_sizes_cumsum[-1] # tensor mid size percent \n",
    "                device = get_device(mid_percent, percents_todo, devices)\n",
    "                weight_assign_dict[abs_tensor_name] = {\n",
    "                    'shape':  tensor.shape,\n",
    "                    'assigned_device': device\n",
    "                }\n",
    "                \n",
    "                device_allo_size_dict[device] += tensor_sizes[i]\n",
    "\n",
    "        # update percents_todo\n",
    "        size_layer = sum(device_allo_size_dict.values())\n",
    "        if size_layer > 0:\n",
    "            device_allo_percents = np.array([device_allo_size_dict[device] * 1. for device in devices]) / size_layer\n",
    "            percents_done = (percents_done * size_done + device_allo_percents * size_layer) / (size_done + size_layer)      \n",
    "        size_done += size_layer\n",
    "        size_todo -= size_layer\n",
    "        if size_todo > 0:\n",
    "            percents_todo = (size_total * percents_target - size_done * percents_done) / size_todo \n",
    "        \n",
    "        logging.info(f'{layer_name}, {percents_done}, size_todo: {size_todo}')\n",
    "\n",
    "\n",
    "    device_map = {k:v['assigned_device'] for k, v in weight_assign_dict.items()}\n",
    "    logging.info('device_map is prepared!')\n",
    "\n",
    "    mem_g = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if 'cuda' in v['assigned_device'] and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem_c = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if v['assigned_device'] == 'cpu' and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem_d = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if v['assigned_device'] == 'disk' and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem = mem_d + mem_c + mem_g\n",
    "    logging.info(f'CausalLM {checkpoint} is to be loaded on: ' \n",
    "                 f'\\nGPU Mem {mem_g:.2f} GiB ({mem_g / mem:.2%}), ' \n",
    "                 f'CPU Mem {mem_c:.2f} GiB ({mem_c / mem:.2%}), '\n",
    "                 f'Disk Mem {mem_d:.2f} Gib ({mem_d / mem:.2%})')\n",
    "    \n",
    "    # prepare output\n",
    "    output = {\n",
    "        'model': model,\n",
    "        'tied_params': tied_params,\n",
    "        'layers_dict': layers_dict,\n",
    "        'weight_assign_dict': weight_assign_dict,\n",
    "        'device_map': device_map\n",
    "    }\n",
    "    output = AttrDict(output)\n",
    "    return output\n",
    "\n",
    "output = get_policy_weight_map(model, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['disk', 'disk']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_map = output.device_map\n",
    "[[device_map[p] for p in tp] for tp in output.tied_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec5b37b7a214b3ca63e0aee179b8f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 08:47:51,271 [18664957.py:20 in <module>] INFO - Model initialized!\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# weights_location = snapshot_download(checkpoint, allow_patterns=[\"*.bin\", 'pytorch_model.bin.index.json'])\n",
    "# model.model = load_checkpoint_and_dispatch(\n",
    "#     output.model.model, # should be base model? e.g. output.model.model\n",
    "#     weights_location,\n",
    "#     device_map=device_map, \n",
    "#     offload_folder='offload/' + checkpoint.replace('/', '.'), \n",
    "#     offload_state_dict=True\n",
    "# )\n",
    "\n",
    "offload_folder = 'offload/' + checkpoint.replace('/', '.')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint, \n",
    "    device_map=device_map, \n",
    "    offload_folder=offload_folder, \n",
    "    offload_state_dict=True\n",
    ")\n",
    "\n",
    "logging.info(f'Model initialized!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerLoader: # FlexGen\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.decoder.embed_tokens', 'model.decoder.embed_positions', 'model.decoder.final_layer_norm', 'model.decoder.layers.0', 'model.decoder.layers.1', 'model.decoder.layers.2', 'model.decoder.layers.3', 'model.decoder.layers.4', 'model.decoder.layers.5', 'model.decoder.layers.6', 'model.decoder.layers.7', 'model.decoder.layers.8', 'model.decoder.layers.9', 'model.decoder.layers.10', 'model.decoder.layers.11', 'model.decoder.layers.12', 'model.decoder.layers.13', 'model.decoder.layers.14', 'model.decoder.layers.15', 'model.decoder.layers.16', 'model.decoder.layers.17', 'model.decoder.layers.18', 'model.decoder.layers.19', 'model.decoder.layers.20', 'model.decoder.layers.21', 'model.decoder.layers.22', 'model.decoder.layers.23', 'lm_head'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.layers_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load / offload\n",
    "#     module object, .dat file path\n",
    "#     layer: pre / post forward hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OffloadConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m             act_assign_dict[act_key]\u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39massigned_device\u001b[39m\u001b[39m'\u001b[39m: device}\n\u001b[1;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m act_assign_dict\n\u001b[0;32m---> 21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_kv_cache_assignment\u001b[39m(num_layers, offload_config: OffloadConfig):\n\u001b[1;32m     22\u001b[0m     logging\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<compute_kv_cache_assignment> enter\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     gpu_batch_limit \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(offload_config\u001b[39m.\u001b[39mnum_gpu_batches \u001b[39m*\u001b[39m offload_config\u001b[39m.\u001b[39mcache_gpu_percent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OffloadConfig' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_activation_assignment(num_layers, offload_config: Policy):\n",
    "    logging.debug(f\"<compute_activation_assignment> enter\")\n",
    "    gpu_batch_limit = int(offload_config.num_gpu_batches * offload_config.act_gpu_percent)\n",
    "    cpu_batch_limit = int(offload_config.num_gpu_batches * (offload_config.act_gpu_percent + offload_config.act_cpu_percent))\n",
    "    logging.debug(f\"<compute_activation_assignment> gpu_batch_limit: {gpu_batch_limit}, cpu_batch_limit: {cpu_batch_limit}\")\n",
    "    \n",
    "    act_assign_dict = {}\n",
    "    for l in range(num_layers):\n",
    "        for i in range(offload_config.num_gpu_batches):\n",
    "            act_key = f\"layer.{l}_index.{i}\"\n",
    "            if i < gpu_batch_limit:\n",
    "                device = 'cuda'\n",
    "            elif i < cpu_batch_limit:\n",
    "                device = 'cpu'\n",
    "            else:\n",
    "                device = 'disk'\n",
    "            act_assign_dict[act_key]= {'assigned_device': device}\n",
    "    return act_assign_dict\n",
    "\n",
    "\n",
    "def compute_kv_cache_assignment(num_layers, offload_config: OffloadConfig):\n",
    "    logging.debug(f\"<compute_kv_cache_assignment> enter\")\n",
    "    gpu_batch_limit = int(offload_config.num_gpu_batches * offload_config.cache_gpu_percent)\n",
    "    cpu_batch_limit = int(offload_config.num_gpu_batches * (offload_config.cache_gpu_percent + offload_config.cache_cpu_percent))\n",
    "    logging.debug(f\"<compute_kv_cache_assignment> gpu_batch_limit: {gpu_batch_limit}, cpu_batch_limit: {cpu_batch_limit}\")\n",
    "    \n",
    "    act_assign_dict = {}\n",
    "    for l in range(num_layers):\n",
    "        for i in range(offload_config.num_gpu_batches):\n",
    "            key_cache_key = f\"key_layer.{l}_index.{i}\"\n",
    "            value_cache_key = f\"key_layer.{l}_index.{i}\"\n",
    "            if i < gpu_batch_limit:\n",
    "                device = 'cuda'\n",
    "            elif i < cpu_batch_limit:\n",
    "                device = 'cpu'\n",
    "            else:\n",
    "                device = 'disk'\n",
    "            act_assign_dict[key_cache_key] = {'assigned_device': device}\n",
    "            act_assign_dict[value_cache_key] = {'assigned_device': device}\n",
    "    return act_assign_dict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
