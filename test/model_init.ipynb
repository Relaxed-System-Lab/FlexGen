{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 09:37:58,806 [361951628.py:11 in <module>] INFO - Importing...\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-09-24 09:38:01,416 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp9jt58ufo\n",
      "2023-09-24 09:38:01,418 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp9jt58ufo/_remote_module_non_scriptable.py\n",
      "2023-09-24 09:38:02.088319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-24 09:38:05,019 [361951628.py:22 in <module>] INFO - Done!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    style='{',\n",
    "    format='{asctime} [{filename}:{lineno} in {funcName}] {levelname} - {message}',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\".log\", 'w'),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    level=logging.INFO\n",
    ")\n",
    "logging.info('Importing...')\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import find_tied_parameters, named_module_tensors, set_module_tensor_to_device\n",
    "\n",
    "from policy import Policy\n",
    "logging.info('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c7bca257164bcfb4c9387271f7342b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"facebook/opt-125m\" \n",
    "# checkpoint = \"facebook/opt-13b\" # 1.3b 6.7b 13b 30b 66b \n",
    "offload_folder = 'offload/' + checkpoint.replace('/', '.')\n",
    "\n",
    "# empty model\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 09:38:05,860 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400\n",
      "2023-09-24 09:38:05,861 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000\n",
      "2023-09-24 09:38:05,863 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464\n",
      "2023-09-24 09:38:05,864 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592\n",
      "2023-09-24 09:38:05,866 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720\n",
      "2023-09-24 09:38:05,868 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848\n",
      "2023-09-24 09:38:05,869 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976\n",
      "2023-09-24 09:38:05,871 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104\n",
      "2023-09-24 09:38:05,872 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232\n",
      "2023-09-24 09:38:05,874 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360\n",
      "2023-09-24 09:38:05,877 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488\n",
      "2023-09-24 09:38:05,880 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616\n",
      "2023-09-24 09:38:05,881 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744\n",
      "2023-09-24 09:38:05,883 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872\n",
      "2023-09-24 09:38:05,885 [1132556772.py:124 in get_policy_weight_map] INFO - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-09-24 09:38:05,886 [1132556772.py:124 in get_policy_weight_map] INFO - lm_head, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-09-24 09:38:05,887 [1132556772.py:128 in get_policy_weight_map] INFO - device_map is prepared!\n",
      "2023-09-24 09:38:05,889 [1132556772.py:134 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: \n",
      "GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)\n"
     ]
    }
   ],
   "source": [
    "class AttrDict(dict):\n",
    "    __slots__ = () \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=16, \n",
    "    num_gpu_batches=8, \n",
    "    weights_gpu_percent=0.0, \n",
    "    weights_cpu_percent=0.3, \n",
    "    cache_gpu_percent=0.0, \n",
    "    cache_cpu_percent=0.2, \n",
    "    act_gpu_percent=0.0, \n",
    "    act_cpu_percent=0.5, \n",
    "    overlap=True, \n",
    "    pin_weight=True,\n",
    ")\n",
    "\n",
    "def get_layers_dict(lm_model: Module, prefix: str='') -> dict:\n",
    "    # return a dict of {layer_name : layer_module ('meta')} with only leaf nodes & transformer layers\n",
    "    layers_dict = {}\n",
    "    for name, module in lm_model.named_children():\n",
    "        # leaf nodes\n",
    "        if len(list(module.named_children())) == 0:\n",
    "            layers_dict[prefix+name] = module\n",
    "        # ModuleList: transformer  \n",
    "        elif isinstance(module, ModuleList):\n",
    "            for block_name, block_module in module.named_children():\n",
    "                layers_dict[prefix+name+'.'+block_name] = block_module\n",
    "        else:\n",
    "            layers_dict.update(get_layers_dict(module, prefix+name+'.'))\n",
    "    return layers_dict\n",
    "\n",
    "def named_module_tensors(module: Module, include_buffers: bool = True, recurse: bool = True):\n",
    "    for named_parameter in module.named_parameters(recurse=recurse):\n",
    "        yield named_parameter\n",
    "\n",
    "    if include_buffers:\n",
    "        for named_buffer in module.named_buffers(recurse=recurse):\n",
    "            yield named_buffer\n",
    "\n",
    "def get_device(cur_percent, percents, choices):\n",
    "    # choose a device (gpu / cpu / disk) for a weight tensor by its percent of size\n",
    "    percents = np.cumsum(percents)\n",
    "    assert np.abs(percents[-1] - 1.0) < 1e-5, f'{percents}'\n",
    "\n",
    "    for i in range(len(percents)):\n",
    "        if cur_percent < percents[i]:\n",
    "            return choices[i]\n",
    "    return choices[-1]\n",
    "\n",
    "def get_policy_weight_map(model: PreTrainedModel, policy: Policy):\n",
    "    \"\"\"{module_name: device}\"\"\"\n",
    "    assert model.device == torch.device('meta'), 'model is not on device meta.'\n",
    "    \n",
    "    # to ensure the tied params are allocated to the same device in the weight_map\n",
    "    model.tie_weights()\n",
    "    tied_params = find_tied_parameters(model)\n",
    "\n",
    "    # layers to be scheduled\n",
    "    layers_dict = get_layers_dict(model)\n",
    "\n",
    "    # device assignment for each tensor in the model\n",
    "    weight_assign_dict = {}\n",
    "    devices = ['cuda', 'cpu', 'disk']\n",
    "    percents_target = np.array([\n",
    "        policy.weights_gpu_percent, \n",
    "        policy.weights_cpu_percent, \n",
    "        policy.weights_disk_percent\n",
    "    ])\n",
    "    \n",
    "    # model size (parameters + buffers), here we do not repeatly sum the tied paramters \n",
    "    size_total = sum(np.prod(tensor.shape) for _, tensor in named_module_tensors(model))\n",
    "    size_done, size_todo = 0, size_total\n",
    "    percents_done, percents_todo = 0 * percents_target, percents_target  \n",
    "\n",
    "    for layer_name, layer_module in layers_dict.items():\n",
    "        # current layer\n",
    "        tensor_sizes = [np.prod(tensor.shape) for _, tensor in named_module_tensors(layer_module)]\n",
    "        tensor_sizes_cumsum = np.cumsum(tensor_sizes)\n",
    "\n",
    "        device_allo_size_dict = {device: 0 for device in devices} # to balance the percents\n",
    "        for i, (tensor_name, tensor) in enumerate(named_module_tensors(layer_module)):\n",
    "            abs_tensor_name = layer_name + '.' + tensor_name\n",
    "\n",
    "            def find_processed_tied(abs_tensor_name, tied_params, weight_assign_dict):\n",
    "                # find the processed parameter (in weight_assign_dict) of the tied parameters.\n",
    "                for tp in tied_params:\n",
    "                    if abs_tensor_name in tp:\n",
    "                        for p in tp:\n",
    "                            if p in weight_assign_dict:\n",
    "                                return p, tuple(tp)\n",
    "                return None\n",
    "            \n",
    "            processed_tied = find_processed_tied(abs_tensor_name, tied_params, weight_assign_dict) \n",
    "            if processed_tied: # this tensor is tied and processed.\n",
    "                p, tp = processed_tied\n",
    "                weight_assign_dict[abs_tensor_name] = {\n",
    "                    # 'shape':  tensor.shape,\n",
    "                    'assigned_device': weight_assign_dict[p]['assigned_device'],\n",
    "                    'tied': tp\n",
    "                }\n",
    "            else:\n",
    "                mid_percent = (tensor_sizes_cumsum[i] - tensor_sizes[i] / 2) / tensor_sizes_cumsum[-1] # tensor mid size percent \n",
    "                device = get_device(mid_percent, percents_todo, devices)\n",
    "                weight_assign_dict[abs_tensor_name] = {\n",
    "                    'shape':  tensor.shape,\n",
    "                    'assigned_device': device\n",
    "                }\n",
    "                \n",
    "                device_allo_size_dict[device] += tensor_sizes[i]\n",
    "\n",
    "        # update percents_todo\n",
    "        size_layer = sum(device_allo_size_dict.values())\n",
    "        if size_layer > 0:\n",
    "            device_allo_percents = np.array([device_allo_size_dict[device] * 1. for device in devices]) / size_layer\n",
    "            percents_done = (percents_done * size_done + device_allo_percents * size_layer) / (size_done + size_layer)      \n",
    "        size_done += size_layer\n",
    "        size_todo -= size_layer\n",
    "        if size_todo > 0:\n",
    "            percents_todo = (size_total * percents_target - size_done * percents_done) / size_todo \n",
    "        \n",
    "        logging.info(f'{layer_name}, {percents_done}, size_todo: {size_todo}')\n",
    "\n",
    "\n",
    "    device_map = {k:v['assigned_device'] for k, v in weight_assign_dict.items()}\n",
    "    logging.info('device_map is prepared!')\n",
    "\n",
    "    mem_g = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if 'cuda' in v['assigned_device'] and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem_c = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if v['assigned_device'] == 'cpu' and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem_d = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if v['assigned_device'] == 'disk' and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem = mem_d + mem_c + mem_g\n",
    "    logging.info(f'CausalLM {checkpoint} is to be loaded on: ' \n",
    "                 f'\\nGPU Mem {mem_g:.2f} GiB ({mem_g / mem:.2%}), ' \n",
    "                 f'CPU Mem {mem_c:.2f} GiB ({mem_c / mem:.2%}), '\n",
    "                 f'Disk Mem {mem_d:.2f} Gib ({mem_d / mem:.2%})')\n",
    "    \n",
    "    # prepare output\n",
    "    output = {\n",
    "        'model': model,\n",
    "        'tied_params': tied_params,\n",
    "        'layers_dict': layers_dict,\n",
    "        'weight_assign_dict': weight_assign_dict,\n",
    "        'device_map': device_map\n",
    "    }\n",
    "    output = AttrDict(output)\n",
    "    return output\n",
    "\n",
    "output = get_policy_weight_map(model, policy)\n",
    "policy_device_map = output.device_map\n",
    "flexgen_layers = output.layers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 09:41:53,456 [1111749824.py:28 in <module>] INFO - The whole model has been downloaded an processed to offload_folder: 'offload/facebook.opt-125m'\n",
      "2023-09-24 09:41:53,579 [1111749824.py:40 in <module>] INFO - Got empty CausalLM: 'facebook/opt-125m' on meta device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lm_head.weight', 'model.decoder.embed_tokens.weight']]\n"
     ]
    }
   ],
   "source": [
    "def check_disk(checkpoint, offload_folder):\n",
    "    if not os.path.isdir(offload_folder):\n",
    "        return False \n",
    "    \n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "    model.tie_weights()\n",
    "    tensor_names = [n for n, _ in named_module_tensors(model, include_buffers=True, recurse=True)]\n",
    "    dat_file_names = [file[:-4] for file in os.listdir(offload_folder) if file.endswith('.dat')]\n",
    "    # logging.info(set(tensor_names) - set(dat_file_names), set(dat_file_names) - set(tensor_names))\n",
    "    return len(set(tensor_names) - set(dat_file_names)) == 0\n",
    "\n",
    "if not check_disk(checkpoint, offload_folder):\n",
    "    # download and process to .dat files\n",
    "    disk_weight_map = {name:'disk' for name in policy_device_map}\n",
    "    try:\n",
    "        AutoModelForCausalLM.from_pretrained(\n",
    "            checkpoint, \n",
    "            device_map=disk_weight_map, \n",
    "            offload_folder=offload_folder, \n",
    "            offload_state_dict=True\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if check_disk(checkpoint, offload_folder):\n",
    "    logging.info(f'The whole model has been downloaded an processed to offload_folder: \\'{offload_folder}\\'')\n",
    "else:\n",
    "    err_msg = 'Mismatch between offload folder and model'\n",
    "    logging.error(err_msg)\n",
    "    raise RuntimeError(err_msg)\n",
    "\n",
    "# get empty model\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "model.tie_weights()\n",
    "model.eval()\n",
    "logging.info(f'Got empty CausalLM: \\'{checkpoint}\\' on meta device.')\n",
    "\n",
    "tied_params = find_tied_parameters(model)\n",
    "print(tied_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_from_name(lm_model, name):\n",
    "    splits = name.split('.')\n",
    "    module = lm_model\n",
    "    for split in splits:\n",
    "        if split == '': \n",
    "            continue \n",
    "\n",
    "        new_module = getattr(module, split)\n",
    "        if new_module is None:\n",
    "            raise ValueError(f\"{module} has no attribute {split}.\")\n",
    "        module = new_module\n",
    "    return module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model init: loading by policy...:   0%|          | 0/197 [00:00<?, ?it/s]/tmp/ipykernel_14292/3593259108.py:45: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tmp = torch.from_numpy(np_memmap).to(device)\n",
      "model init: loading by policy...: 100%|██████████| 197/197 [00:00<00:00, 4656.92it/s]\n",
      "2023-09-24 09:41:58,704 [3593259108.py:60 in policy_init] INFO - model has been loaded by policy.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm \n",
    "import gc \n",
    "\n",
    "dat_files = [f for f in os.listdir(offload_folder) if f.endswith('.dat')]\n",
    "with open(os.path.join(offload_folder, 'index.json'), 'r') as f:\n",
    "    index = json.load(f) # {name: {dtype, shape}}\n",
    "\n",
    "def get_tied_target(tensor_name):\n",
    "    # if tensor_name is tied and without a .dat file, if it is not tied, return itself\n",
    "    for group in tied_params:\n",
    "        if tensor_name in group:\n",
    "            for name in group:\n",
    "                if name + '.dat' in dat_files:\n",
    "                    return name \n",
    "    return tensor_name\n",
    "\n",
    "def flexgen_load_module_tensor(model, tensor_name, device):\n",
    "    tensor = get_obj_from_name(model, tensor_name)\n",
    "    if tensor.device == device:\n",
    "        return \n",
    "    \n",
    "    # else\n",
    "    old_tensor_name = tensor_name\n",
    "    \n",
    "    tensor_name = get_tied_target(tensor_name) \n",
    "    metadata = index[tensor_name]\n",
    "\n",
    "    # copied from accelerate.utils.offload\n",
    "    shape = tuple(metadata[\"shape\"])\n",
    "    if shape == ():\n",
    "        # NumPy memory-mapped arrays can't have 0 dims so it was saved as 1d tensor\n",
    "        shape = (1,)\n",
    "\n",
    "    dtype = metadata[\"dtype\"]\n",
    "    if dtype == \"bfloat16\":\n",
    "        # NumPy does not support bfloat16 so this was saved as a int16\n",
    "        dtype = \"int16\"\n",
    "    \n",
    "    # load .dat file\n",
    "    save_path = os.path.join(offload_folder, tensor_name + '.dat')\n",
    "\n",
    "    # to device \n",
    "    np_memmap = np.memmap(save_path, dtype=dtype, shape=shape, mode='r') \n",
    "    tmp = torch.from_numpy(np_memmap).to(device) \n",
    "    set_module_tensor_to_device(model, old_tensor_name, device, tmp)\n",
    "\n",
    "def flexgen_offload_module_tensor(model, tensor_name):\n",
    "    tensor = get_obj_from_name(model, tensor_name)\n",
    "    device = policy_device_map[tensor_name]\n",
    "    device = device if device != 'disk' else 'meta' \n",
    "    if tensor.device != device:\n",
    "        set_module_tensor_to_device(model, tensor_name, device, tensor) # gtoc, ctog\n",
    "\n",
    "def policy_init(model, policy_device_map):\n",
    "    for tensor_name, device in tqdm(policy_device_map.items(), desc='model init: loading by policy...'):\n",
    "        if device != 'disk':\n",
    "            flexgen_load_module_tensor(model, tensor_name, device) \n",
    "\n",
    "    logging.info('model has been loaded by policy.')        \n",
    "\n",
    "policy_init(model, policy_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "pre model.decoder.embed_tokens forward\n",
      "post model.decoder.embed_tokens forward\n",
      "pre model.decoder.embed_positions forward\n",
      "post model.decoder.embed_positions forward\n",
      "pre model.decoder.layers.0 forward\n",
      "post model.decoder.layers.0 forward\n",
      "pre model.decoder.layers.1 forward\n",
      "post model.decoder.layers.1 forward\n",
      "pre model.decoder.layers.2 forward\n",
      "post model.decoder.layers.2 forward\n",
      "pre model.decoder.layers.3 forward\n",
      "post model.decoder.layers.3 forward\n",
      "pre model.decoder.layers.4 forward\n",
      "post model.decoder.layers.4 forward\n",
      "pre model.decoder.layers.5 forward\n",
      "post model.decoder.layers.5 forward\n",
      "pre model.decoder.layers.6 forward\n",
      "post model.decoder.layers.6 forward\n",
      "pre model.decoder.layers.7 forward\n",
      "post model.decoder.layers.7 forward\n",
      "pre model.decoder.layers.8 forward\n",
      "post model.decoder.layers.8 forward\n",
      "pre model.decoder.layers.9 forward\n",
      "post model.decoder.layers.9 forward\n",
      "pre model.decoder.layers.10 forward\n",
      "post model.decoder.layers.10 forward\n",
      "pre model.decoder.layers.11 forward\n",
      "post model.decoder.layers.11 forward\n",
      "pre model.decoder.final_layer_norm forward\n",
      "post model.decoder.final_layer_norm forward\n",
      "pre lm_head forward\n",
      "post lm_head forward\n",
      "Who are you? Are you conscious?... I wonder which part of my brain is thinking that when you say I am conscious?\n",
      "That's the thing -- when I hear the word being\n",
      "----------\n",
      "Where is Deutschland?)\n",
      "This is a German sub reddit.\n",
      "It has no English language subs.\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?�\n",
      "\n",
      "For Huawei Mate 60 Pro, the camera is quite improved thanks to the new 4K camera sensors, which are a couple of shots above\n",
      "----------\n",
      "Who are you? Are you conscious?_\n",
      "\n",
      "[quote][p][bold]Who are you? Are you conscious?_[/bold]\n",
      "[quote][p][bold]\n",
      "----------\n",
      "Where is Deutschland?\n",
      "The capital of Germany, the capital of the Austrian republic.\n",
      "Yes, thanks for clarifying.   I lived there since 2012 and the\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?1 review\n",
      "Huawei's latest Huawei laptop released on Monday is designed to be the next step in the Huawei Mate series and looks to be more than\n",
      "----------\n",
      "Who are you? Are you conscious? I had to start watching some shows on Netflix and it's a really good choice.\n",
      "You are indeed! Are you sure you think the world would\n",
      "----------\n",
      "Where is Deutschland?/u/thedukeman/ photographer ? I want to visit it. How come?\n",
      "I want to visit it, I want to see\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?�\n",
      "The latest Huawei Mate 60 Pro is out, released earlier this month and available in a number of models and specifications. There are a variety of\n",
      "----------\n",
      "Who are you? Are you conscious?�You're not conscious\n",
      "How do you know?\n",
      "You don't see. Can't feel.\n",
      "----------\n",
      "Where is Deutschland?\n",
      "Where? Where are the fags? I live in Berlin, not in Germany, and I've been there 4 straighttimes since I went to\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?IPL\n",
      "The latest rumors, which were announced on the occasion of Huawei's 60th anniversary in April 2018, have now been updated, as the\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from accelerate.hooks import (\n",
    "    ModelHook, \n",
    "    SequentialHook, \n",
    "    add_hook_to_module, \n",
    "    remove_hook_from_module\n",
    ")\n",
    "# TODO: add_zigzag_hook, remove_zigzag_hook\n",
    "\n",
    "from accelerate.utils import (\n",
    "    find_device,\n",
    "    named_module_tensors,\n",
    "    send_to_device,\n",
    "    set_module_tensor_to_device,\n",
    ")\n",
    "\n",
    "# global buffers: {layer_name: value_holder}\n",
    "weight_home = {}\n",
    "weight_load_buf = {}\n",
    "\n",
    "act_home = {}\n",
    "act_load_buf = {}\n",
    "act_store_buf = {}\n",
    "\n",
    "kv_home = {} \n",
    "kv_load_buf = {}\n",
    "kv_store_buf = {}\n",
    "\n",
    "# TODO: cuda streams / cpu threads (?)\n",
    "\n",
    "\n",
    "from typing import Optional, Union, Mapping\n",
    "class LayerWeightHook(ModelHook):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model, \n",
    "        layer_name,\n",
    "        compute_device,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name\n",
    "        self.compute_device = compute_device\n",
    "        \n",
    "    def check_dat(self, dat_file):\n",
    "        return os.path.isfile(dat_file)\n",
    "\n",
    "    def init_hook(self, module):\n",
    "        self.weight_names = [self.layer_name + '.' + name for name, _ in named_module_tensors(module, True, True)]\n",
    "        \n",
    "        self.actual_weight_names = [get_tied_target(w) for w in self.weight_names]\n",
    "        dat_files = [os.path.join(offload_folder, w + '.dat') for w in self.actual_weight_names]\n",
    "        assert all([self.check_dat(f) for f in dat_files]), f'dat file error, {dat_files}'\n",
    "        \n",
    "        return module \n",
    "    \n",
    "    def pre_forward(self, module: Module, *args, **kwargs):\n",
    "        print(f'pre {self.layer_name} forward')\n",
    "        # load weights\n",
    "        for w in self.weight_names:\n",
    "            flexgen_load_module_tensor(model, w, self.compute_device)\n",
    "        return args, kwargs\n",
    "    def post_forward(self, module, output):\n",
    "        print(f'post {self.layer_name} forward')\n",
    "        # offload weights\n",
    "        for w in self.weight_names:\n",
    "            flexgen_offload_module_tensor(model, w)\n",
    "        return output\n",
    "    \n",
    "    def detach_hook(self, module):\n",
    "        return module \n",
    "\n",
    "class LayerActHook(ModelHook): pass \n",
    "class LayerKVCacheHook(ModelHook): pass \n",
    "\n",
    "def to_zigzag_forward(layer):\n",
    "    pass \n",
    "\n",
    "\n",
    "# clear hooks \n",
    "remove_hook_from_module(model, recurse=True)\n",
    "\n",
    "compute_device = 'cpu' \n",
    "\n",
    "for layer_name in flexgen_layers:\n",
    "    layer_module = get_obj_from_name(model, layer_name)\n",
    "\n",
    "    layer_weight_hook = LayerWeightHook(model=model, layer_name=layer_name, compute_device=compute_device)\n",
    "    add_hook_to_module(layer_module, layer_weight_hook, append=True)\n",
    "    # break\n",
    "\n",
    "# generate test\n",
    "\n",
    "prompts = [\n",
    "    'Who are you? Are you conscious?',\n",
    "    'Where is Deutschland?',\n",
    "    'How is Huawei Mate 60 Pro?'\n",
    "] * 4\n",
    "\n",
    "prompt_len = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompts, padding=\"max_length\", max_length=prompt_len, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    max_length=30 + prompt_len,\n",
    "    # num_beams=2,\n",
    "    # num_beam_groups=2,\n",
    "    # diversity_penalty=0.1,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "output_texts = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "for output_text in output_texts:\n",
    "    print(output_text)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_module._hf_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load / offload\n",
    "#     module object, .dat file path\n",
    "#     layer: pre / post forward hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_activation_assignment(num_layers, offload_config: Policy):\n",
    "    logging.debug(f\"<compute_activation_assignment> enter\")\n",
    "    gpu_batch_limit = int(offload_config.num_gpu_batches * offload_config.act_gpu_percent)\n",
    "    cpu_batch_limit = int(offload_config.num_gpu_batches * (offload_config.act_gpu_percent + offload_config.act_cpu_percent))\n",
    "    logging.debug(f\"<compute_activation_assignment> gpu_batch_limit: {gpu_batch_limit}, cpu_batch_limit: {cpu_batch_limit}\")\n",
    "    \n",
    "    act_assign_dict = {}\n",
    "    for l in range(num_layers):\n",
    "        for i in range(offload_config.num_gpu_batches):\n",
    "            act_key = f\"layer.{l}_index.{i}\"\n",
    "            if i < gpu_batch_limit:\n",
    "                device = 'cuda'\n",
    "            elif i < cpu_batch_limit:\n",
    "                device = 'cpu'\n",
    "            else:\n",
    "                device = 'disk'\n",
    "            act_assign_dict[act_key]= {'assigned_device': device}\n",
    "    return act_assign_dict\n",
    "\n",
    "\n",
    "def compute_kv_cache_assignment(num_layers, offload_config: OffloadConfig):\n",
    "    logging.debug(f\"<compute_kv_cache_assignment> enter\")\n",
    "    gpu_batch_limit = int(offload_config.num_gpu_batches * offload_config.cache_gpu_percent)\n",
    "    cpu_batch_limit = int(offload_config.num_gpu_batches * (offload_config.cache_gpu_percent + offload_config.cache_cpu_percent))\n",
    "    logging.debug(f\"<compute_kv_cache_assignment> gpu_batch_limit: {gpu_batch_limit}, cpu_batch_limit: {cpu_batch_limit}\")\n",
    "    \n",
    "    act_assign_dict = {}\n",
    "    for l in range(num_layers):\n",
    "        for i in range(offload_config.num_gpu_batches):\n",
    "            key_cache_key = f\"key_layer.{l}_index.{i}\"\n",
    "            value_cache_key = f\"key_layer.{l}_index.{i}\"\n",
    "            if i < gpu_batch_limit:\n",
    "                device = 'cuda'\n",
    "            elif i < cpu_batch_limit:\n",
    "                device = 'cpu'\n",
    "            else:\n",
    "                device = 'disk'\n",
    "            act_assign_dict[key_cache_key] = {'assigned_device': device}\n",
    "            act_assign_dict[value_cache_key] = {'assigned_device': device}\n",
    "    return act_assign_dict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
