{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 12:01:26,627 [361951628.py:11 in <module>] INFO - Importing...\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-09-24 12:01:29,194 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpjobtn5mn\n",
      "2023-09-24 12:01:29,199 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpjobtn5mn/_remote_module_non_scriptable.py\n",
      "2023-09-24 12:01:29.879606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-24 12:01:32,654 [361951628.py:22 in <module>] INFO - Done!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    style='{',\n",
    "    format='{asctime} [{filename}:{lineno} in {funcName}] {levelname} - {message}',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\".log\", 'w'),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    level=logging.INFO\n",
    ")\n",
    "logging.info('Importing...')\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import find_tied_parameters, named_module_tensors, set_module_tensor_to_device\n",
    "\n",
    "from policy import Policy\n",
    "logging.info('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"facebook/opt-125m\" \n",
    "# checkpoint = \"facebook/opt-13b\" # 1.3b 6.7b 13b 30b 66b \n",
    "offload_folder = 'offload/' + checkpoint.replace('/', '.')\n",
    "\n",
    "# empty model\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 12:01:32,936 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400\n",
      "2023-09-24 12:01:32,938 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000\n",
      "2023-09-24 12:01:32,940 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464\n",
      "2023-09-24 12:01:32,942 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592\n",
      "2023-09-24 12:01:32,944 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720\n",
      "2023-09-24 12:01:32,946 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848\n",
      "2023-09-24 12:01:32,948 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976\n",
      "2023-09-24 12:01:32,950 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104\n",
      "2023-09-24 12:01:32,952 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232\n",
      "2023-09-24 12:01:32,954 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360\n",
      "2023-09-24 12:01:32,956 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488\n",
      "2023-09-24 12:01:32,958 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616\n",
      "2023-09-24 12:01:32,960 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744\n",
      "2023-09-24 12:01:32,962 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872\n",
      "2023-09-24 12:01:32,964 [4043888472.py:116 in get_policy_weight_map] INFO - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-09-24 12:01:32,966 [4043888472.py:116 in get_policy_weight_map] INFO - lm_head, [0.         0.30186053 0.69813947], size_todo: 0\n",
      "2023-09-24 12:01:32,968 [4043888472.py:120 in get_policy_weight_map] INFO - device_map is prepared!\n",
      "2023-09-24 12:01:32,971 [4043888472.py:126 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: \n",
      "GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)\n"
     ]
    }
   ],
   "source": [
    "class AttrDict(dict):\n",
    "    __slots__ = () \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=16, \n",
    "    num_gpu_batches=8, \n",
    "    weights_gpu_percent=0.0, \n",
    "    weights_cpu_percent=0.3, \n",
    "    cache_gpu_percent=0.0, \n",
    "    cache_cpu_percent=0.2, \n",
    "    act_gpu_percent=0.0, \n",
    "    act_cpu_percent=0.5, \n",
    "    overlap=True, \n",
    "    pin_weight=True,\n",
    ")\n",
    "\n",
    "def get_layers_dict(lm_model: Module, prefix: str='') -> dict:\n",
    "    # return a dict of {layer_name : layer_module ('meta')} with only leaf nodes & transformer layers\n",
    "    layers_dict = {}\n",
    "    for name, module in lm_model.named_children():\n",
    "        # leaf nodes\n",
    "        if len(list(module.named_children())) == 0:\n",
    "            layers_dict[prefix+name] = module\n",
    "        # ModuleList: transformer  \n",
    "        elif isinstance(module, ModuleList):\n",
    "            for block_name, block_module in module.named_children():\n",
    "                layers_dict[prefix+name+'.'+block_name] = block_module\n",
    "        else:\n",
    "            layers_dict.update(get_layers_dict(module, prefix+name+'.'))\n",
    "    return layers_dict\n",
    "\n",
    "def get_device(cur_percent, percents, choices):\n",
    "    # choose a device (gpu / cpu / disk) for a weight tensor by its percent of size\n",
    "    percents = np.cumsum(percents)\n",
    "    assert np.abs(percents[-1] - 1.0) < 1e-5, f'{percents}'\n",
    "\n",
    "    for i in range(len(percents)):\n",
    "        if cur_percent < percents[i]:\n",
    "            return choices[i]\n",
    "    return choices[-1]\n",
    "\n",
    "def get_policy_weight_map(model: PreTrainedModel, policy: Policy):\n",
    "    \"\"\"{module_name: device}\"\"\"\n",
    "    assert model.device == torch.device('meta'), 'model is not on device meta.'\n",
    "    \n",
    "    # to ensure the tied params are allocated to the same device in the weight_map\n",
    "    model.tie_weights()\n",
    "    tied_params = find_tied_parameters(model)\n",
    "\n",
    "    # layers to be scheduled\n",
    "    layers_dict = get_layers_dict(model)\n",
    "\n",
    "    # device assignment for each tensor in the model\n",
    "    weight_assign_dict = {}\n",
    "    devices = ['cuda', 'cpu', 'disk']\n",
    "    percents_target = np.array([\n",
    "        policy.weights_gpu_percent, \n",
    "        policy.weights_cpu_percent, \n",
    "        policy.weights_disk_percent\n",
    "    ])\n",
    "    \n",
    "    # model size (parameters + buffers), here we do not repeatly sum the tied paramters \n",
    "    size_total = sum(np.prod(tensor.shape) for _, tensor in named_module_tensors(model, include_buffers=True, recurse=True))\n",
    "    size_done, size_todo = 0, size_total\n",
    "    percents_done, percents_todo = 0 * percents_target, percents_target  \n",
    "\n",
    "    for layer_name, layer_module in layers_dict.items():\n",
    "        # current layer\n",
    "        tensor_sizes = [np.prod(tensor.shape) for _, tensor in named_module_tensors(layer_module, include_buffers=True, recurse=True)]\n",
    "        tensor_sizes_cumsum = np.cumsum(tensor_sizes)\n",
    "\n",
    "        device_allo_size_dict = {device: 0 for device in devices} # to balance the percents\n",
    "        for i, (tensor_name, tensor) in enumerate(named_module_tensors(layer_module, include_buffers=True, recurse=True)):\n",
    "            abs_tensor_name = layer_name + '.' + tensor_name\n",
    "\n",
    "            def find_processed_tied(abs_tensor_name, tied_params, weight_assign_dict):\n",
    "                # find the processed parameter (in weight_assign_dict) of the tied parameters.\n",
    "                for tp in tied_params:\n",
    "                    if abs_tensor_name in tp:\n",
    "                        for p in tp:\n",
    "                            if p in weight_assign_dict:\n",
    "                                return p, tuple(tp)\n",
    "                return None\n",
    "            \n",
    "            processed_tied = find_processed_tied(abs_tensor_name, tied_params, weight_assign_dict) \n",
    "            if processed_tied: # this tensor is tied and processed.\n",
    "                p, tp = processed_tied\n",
    "                weight_assign_dict[abs_tensor_name] = {\n",
    "                    # 'shape':  tensor.shape,\n",
    "                    'assigned_device': weight_assign_dict[p]['assigned_device'],\n",
    "                    'tied': tp\n",
    "                }\n",
    "            else:\n",
    "                mid_percent = (tensor_sizes_cumsum[i] - tensor_sizes[i] / 2) / tensor_sizes_cumsum[-1] # tensor mid size percent \n",
    "                device = get_device(mid_percent, percents_todo, devices)\n",
    "                weight_assign_dict[abs_tensor_name] = {\n",
    "                    'shape':  tensor.shape,\n",
    "                    'assigned_device': device\n",
    "                }\n",
    "                \n",
    "                device_allo_size_dict[device] += tensor_sizes[i]\n",
    "\n",
    "        # update percents_todo\n",
    "        size_layer = sum(device_allo_size_dict.values())\n",
    "        if size_layer > 0:\n",
    "            device_allo_percents = np.array([device_allo_size_dict[device] * 1. for device in devices]) / size_layer\n",
    "            percents_done = (percents_done * size_done + device_allo_percents * size_layer) / (size_done + size_layer)      \n",
    "        size_done += size_layer\n",
    "        size_todo -= size_layer\n",
    "        if size_todo > 0:\n",
    "            percents_todo = (size_total * percents_target - size_done * percents_done) / size_todo \n",
    "        \n",
    "        logging.info(f'{layer_name}, {percents_done}, size_todo: {size_todo}')\n",
    "\n",
    "\n",
    "    device_map = {k:v['assigned_device'] for k, v in weight_assign_dict.items()}\n",
    "    logging.info('device_map is prepared!')\n",
    "\n",
    "    mem_g = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if 'cuda' in v['assigned_device'] and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem_c = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if v['assigned_device'] == 'cpu' and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem_d = sum([np.prod(v['shape']) for _, v in weight_assign_dict.items() if v['assigned_device'] == 'disk' and 'shape' in v]) * 2 / (2 ** 30)\n",
    "    mem = mem_d + mem_c + mem_g\n",
    "    logging.info(f'CausalLM {checkpoint} is to be loaded on: ' \n",
    "                 f'\\nGPU Mem {mem_g:.2f} GiB ({mem_g / mem:.2%}), ' \n",
    "                 f'CPU Mem {mem_c:.2f} GiB ({mem_c / mem:.2%}), '\n",
    "                 f'Disk Mem {mem_d:.2f} Gib ({mem_d / mem:.2%})')\n",
    "    \n",
    "    # prepare output\n",
    "    output = {\n",
    "        'model': model,\n",
    "        'tied_params': tied_params,\n",
    "        'layers_dict': layers_dict,\n",
    "        'weight_assign_dict': weight_assign_dict,\n",
    "        'device_map': device_map\n",
    "    }\n",
    "    output = AttrDict(output)\n",
    "    return output\n",
    "\n",
    "output = get_policy_weight_map(model, policy)\n",
    "policy_device_map = output.device_map\n",
    "flexgen_layers = output.layers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 12:01:33,278 [1111749824.py:28 in <module>] INFO - The whole model has been downloaded an processed to offload_folder: 'offload/facebook.opt-125m'\n",
      "2023-09-24 12:01:33,407 [1111749824.py:40 in <module>] INFO - Got empty CausalLM: 'facebook/opt-125m' on meta device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lm_head.weight', 'model.decoder.embed_tokens.weight']]\n"
     ]
    }
   ],
   "source": [
    "def check_disk(checkpoint, offload_folder):\n",
    "    if not os.path.isdir(offload_folder):\n",
    "        return False \n",
    "    \n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "    model.tie_weights()\n",
    "    tensor_names = [n for n, _ in named_module_tensors(model, include_buffers=True, recurse=True)]\n",
    "    dat_file_names = [file[:-4] for file in os.listdir(offload_folder) if file.endswith('.dat')]\n",
    "    # logging.info(set(tensor_names) - set(dat_file_names), set(dat_file_names) - set(tensor_names))\n",
    "    return len(set(tensor_names) - set(dat_file_names)) == 0\n",
    "\n",
    "if not check_disk(checkpoint, offload_folder):\n",
    "    # download and process to .dat files\n",
    "    disk_weight_map = {name:'disk' for name in policy_device_map}\n",
    "    try:\n",
    "        AutoModelForCausalLM.from_pretrained(\n",
    "            checkpoint, \n",
    "            device_map=disk_weight_map, \n",
    "            offload_folder=offload_folder, \n",
    "            offload_state_dict=True\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if check_disk(checkpoint, offload_folder):\n",
    "    logging.info(f'The whole model has been downloaded an processed to offload_folder: \\'{offload_folder}\\'')\n",
    "else:\n",
    "    err_msg = 'Mismatch between offload folder and model'\n",
    "    logging.error(err_msg)\n",
    "    raise RuntimeError(err_msg)\n",
    "\n",
    "# get empty model\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "model.tie_weights()\n",
    "model.eval()\n",
    "logging.info(f'Got empty CausalLM: \\'{checkpoint}\\' on meta device.')\n",
    "\n",
    "tied_params = find_tied_parameters(model)\n",
    "print(tied_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_from_name(lm_model, name):\n",
    "    splits = name.split('.')\n",
    "    module = lm_model\n",
    "    for split in splits:\n",
    "        if split == '': \n",
    "            continue \n",
    "\n",
    "        new_module = getattr(module, split)\n",
    "        if new_module is None:\n",
    "            raise ValueError(f\"{module} has no attribute {split}.\")\n",
    "        module = new_module\n",
    "    return module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model init: loading by policy...:   0%|          | 0/197 [00:00<?, ?it/s]/tmp/ipykernel_24381/3593259108.py:45: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tmp = torch.from_numpy(np_memmap).to(device)\n",
      "model init: loading by policy...: 100%|██████████| 197/197 [00:00<00:00, 3791.57it/s]\n",
      "2023-09-24 12:01:33,501 [3593259108.py:60 in policy_init] INFO - model has been loaded by policy.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm \n",
    "import gc \n",
    "\n",
    "dat_files = [f for f in os.listdir(offload_folder) if f.endswith('.dat')]\n",
    "with open(os.path.join(offload_folder, 'index.json'), 'r') as f:\n",
    "    index = json.load(f) # {name: {dtype, shape}}\n",
    "\n",
    "def get_tied_target(tensor_name):\n",
    "    # if tensor_name is tied and without a .dat file, if it is not tied, return itself\n",
    "    for group in tied_params:\n",
    "        if tensor_name in group:\n",
    "            for name in group:\n",
    "                if name + '.dat' in dat_files:\n",
    "                    return name \n",
    "    return tensor_name\n",
    "\n",
    "def flexgen_load_module_tensor(model, tensor_name, device):\n",
    "    tensor = get_obj_from_name(model, tensor_name)\n",
    "    if tensor.device == device:\n",
    "        return \n",
    "    \n",
    "    # else\n",
    "    old_tensor_name = tensor_name\n",
    "    \n",
    "    tensor_name = get_tied_target(tensor_name) \n",
    "    metadata = index[tensor_name]\n",
    "\n",
    "    # copied from accelerate.utils.offload\n",
    "    shape = tuple(metadata[\"shape\"])\n",
    "    if shape == ():\n",
    "        # NumPy memory-mapped arrays can't have 0 dims so it was saved as 1d tensor\n",
    "        shape = (1,)\n",
    "\n",
    "    dtype = metadata[\"dtype\"]\n",
    "    if dtype == \"bfloat16\":\n",
    "        # NumPy does not support bfloat16 so this was saved as a int16\n",
    "        dtype = \"int16\"\n",
    "    \n",
    "    # load .dat file\n",
    "    save_path = os.path.join(offload_folder, tensor_name + '.dat')\n",
    "\n",
    "    # to device \n",
    "    np_memmap = np.memmap(save_path, dtype=dtype, shape=shape, mode='r') \n",
    "    tmp = torch.from_numpy(np_memmap).to(device) \n",
    "    set_module_tensor_to_device(model, old_tensor_name, device, tmp)\n",
    "\n",
    "def flexgen_offload_module_tensor(model, tensor_name):\n",
    "    tensor = get_obj_from_name(model, tensor_name)\n",
    "    device = policy_device_map[tensor_name]\n",
    "    device = device if device != 'disk' else 'meta' \n",
    "    if tensor.device != device:\n",
    "        set_module_tensor_to_device(model, tensor_name, device, tensor) # gtoc, ctog\n",
    "\n",
    "def policy_init(model, policy_device_map):\n",
    "    for tensor_name, device in tqdm(policy_device_map.items(), desc='model init: loading by policy...'):\n",
    "        if device != 'disk':\n",
    "            flexgen_load_module_tensor(model, tensor_name, device) \n",
    "\n",
    "    logging.info('model has been loaded by policy.')        \n",
    "\n",
    "policy_init(model, policy_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(obj): # recursive\n",
    "\n",
    "    if isinstance(obj, tuple):\n",
    "        return tuple(get_info(o) for o in obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k:get_info(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj.size()\n",
    "    else:\n",
    "        return type(obj)\n",
    "              \n",
    "num_gpu_batches = 6\n",
    "\n",
    "def get_kth_batch_inputs(inputs, k): # for both args, kwargs\n",
    "    if isinstance(inputs, tuple):\n",
    "        return tuple(get_kth_batch_inputs(inp, k) for inp in inputs)\n",
    "    elif isinstance(inputs, dict):\n",
    "        return {k:get_kth_batch_inputs(v, k) for k, v in inputs.items()}\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        block_size = inputs.size(0)\n",
    "        assert block_size % num_gpu_batches == 0\n",
    "\n",
    "        gpu_batch_size = block_size // num_gpu_batches\n",
    "        return inputs[k * gpu_batch_size:(k + 1) * gpu_batch_size]\n",
    "    else:\n",
    "        return inputs\n",
    "\n",
    "def concat_outputs(outputs, ): # concat K outputs to one output\n",
    "    ans = []\n",
    "    for elem in zip(*outputs):\n",
    "        if isinstance(elem[0], torch.Tensor):\n",
    "            ans.append(torch.cat(elem, dim=0))\n",
    "        elif isinstance(elem[0], tuple):\n",
    "            ans.append(tuple(concat_outputs(elem)))\n",
    "    return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aei', 'bfj', ['cgk', ['dhl']]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = ['a', 'b', ['c', ['d']]]\n",
    "l2 = ['e', 'f', ['g', ['h']]]\n",
    "l3 = ['i', 'j', ['k', ['l']]]\n",
    "ls = [l1, l2, l3]\n",
    "def f(ls):\n",
    "    ans = []\n",
    "    for elem in zip(*ls):\n",
    "        if isinstance(elem[0], str):\n",
    "            ans.append(''.join(elem))\n",
    "        elif isinstance(elem[0], list):\n",
    "            ans.append(f(elem))\n",
    "        # print(elem)\n",
    "    return ans\n",
    "f(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.hooks import (\n",
    "    ModelHook, \n",
    "    SequentialHook, \n",
    "    add_hook_to_module, \n",
    "    remove_hook_from_module\n",
    ")\n",
    "# TODO: add_minibatch_hook, remove_minibatch_hook\n",
    "\n",
    "from accelerate.utils import (\n",
    "    find_device,\n",
    "    named_module_tensors,\n",
    "    send_to_device,\n",
    "    set_module_tensor_to_device,\n",
    ")\n",
    "\n",
    "# global buffers: {layer_name: value_holder}\n",
    "# weight_home = {}\n",
    "# weight_load_buf = {}\n",
    "\n",
    "act_home = {}\n",
    "act_load_buf = {}\n",
    "act_store_buf = {}\n",
    "\n",
    "kv_home = {} \n",
    "kv_load_buf = {}\n",
    "kv_store_buf = {}\n",
    "\n",
    "# TODO: cuda streams / cpu threads (?)\n",
    "\n",
    "\n",
    "from typing import Optional, Union, Mapping\n",
    "class LayerWeightHook(ModelHook):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model, \n",
    "        layer_name,\n",
    "        next_layer_name,\n",
    "        compute_device,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name\n",
    "        self.next_layer_name = next_layer_name\n",
    "        self.compute_device = compute_device\n",
    "\n",
    "        # get weight names\n",
    "        layer_module = get_obj_from_name(model, layer_name)\n",
    "        self.weight_names = [layer_name + '.' + name for name, _ in named_module_tensors(layer_module, True, True)]\n",
    "        dat_files = [os.path.join(offload_folder, get_tied_target(w) + '.dat') for w in self.weight_names]\n",
    "        assert all([self.check_dat(f) for f in dat_files]), f'dat file error, {dat_files}'\n",
    "        \n",
    "        if next_layer_name:\n",
    "            next_layer_module = get_obj_from_name(model, next_layer_name)\n",
    "            self.next_weight_names = [next_layer_name + '.' + name for name, _ in named_module_tensors(next_layer_module, True, True)]\n",
    "            dat_files = [os.path.join(offload_folder, get_tied_target(w) + '.dat') for w in self.next_weight_names]\n",
    "            assert all([self.check_dat(f) for f in dat_files]), f'dat file error, {dat_files}'\n",
    "\n",
    "        \n",
    "    def check_dat(self, dat_file):\n",
    "        return os.path.isfile(dat_file)\n",
    "\n",
    "    def init_hook(self, module):\n",
    "        return module \n",
    "\n",
    "    def load_layer(self, weight_names):\n",
    "        for w in weight_names:\n",
    "            flexgen_load_module_tensor(self.model, w, self.compute_device)\n",
    "\n",
    "    def offload_layer(self, weight_names):\n",
    "        for w in weight_names:\n",
    "            flexgen_offload_module_tensor(self.model, w)\n",
    "    \n",
    "    def pre_forward(self, module: Module, *args, **kwargs):\n",
    "        logging.info(f'pre {self.layer_name} forward,'\n",
    "                     f'\\n args:{get_info(args)},'\n",
    "                     f'\\n kwargs: {get_info(kwargs)}')\n",
    "\n",
    "\n",
    "        self.load_layer(self.weight_names) \n",
    "        if self.next_layer_name:\n",
    "            self.load_layer(self.next_weight_names) \n",
    "\n",
    "        return args, kwargs\n",
    "    \n",
    "    def post_forward(self, module, output):\n",
    "        \n",
    "        logging.info(f'post {self.layer_name} forward, \\noutput: {get_info(output)}\\n\\n')\n",
    "\n",
    "        self.offload_layer(self.weight_names)\n",
    "        return output\n",
    "    \n",
    "    def detach_hook(self, module):\n",
    "        return module \n",
    "\n",
    "import functools\n",
    "\n",
    "def add_minibatch_hook(module, num_gpu_batches):\n",
    "    if hasattr(module, \"_hf_hook\") and hasattr(module, \"_old_forward\"):\n",
    "        # If we already put some hook on this module, we replace it with the new one.\n",
    "        old_forward = module._old_forward\n",
    "    else:\n",
    "        old_forward = module.forward\n",
    "        module._old_forward = old_forward\n",
    "\n",
    "    @functools.wraps(old_forward)\n",
    "    def new_forward(*args, **kwargs):\n",
    "        args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
    "        if module._hf_hook.no_grad:\n",
    "            with torch.no_grad():\n",
    "                outputs = []\n",
    "                for k in range(num_gpu_batches):\n",
    "                    args_k = get_kth_batch_inputs(args, k)\n",
    "                    kwargs_k = get_kth_batch_inputs(kwargs, k)\n",
    "                    output = old_forward(*args_k, **kwargs_k)\n",
    "                    outputs.append(output) \n",
    "                \n",
    "                output = concat_outputs(outputs)\n",
    "                \n",
    "        else:\n",
    "            output = old_forward(*args, **kwargs)\n",
    "        return module._hf_hook.post_forward(module, output)\n",
    "\n",
    "    module.forward = new_forward\n",
    "    return module  \n",
    "\n",
    "def remove_minibatch_hook(module, recurse):\n",
    "    if hasattr(module, \"_hf_hook\"):\n",
    "        module._hf_hook.detach_hook(module)\n",
    "        delattr(module, \"_hf_hook\")\n",
    "\n",
    "    if hasattr(module, \"_old_forward\"):\n",
    "        module.forward = module._old_forward\n",
    "        delattr(module, \"_old_forward\")\n",
    "\n",
    "    if recurse:\n",
    "        for child in module.children():\n",
    "            remove_hook_from_module(child, recurse)\n",
    "\n",
    "    return module\n",
    "\n",
    "class LayerActHook(ModelHook): pass \n",
    "class LayerKVCacheHook(ModelHook): pass \n",
    "\n",
    "def clear_hooks(model):\n",
    "    remove_hook_from_module(model, recurse=True)\n",
    "\n",
    "\n",
    "clear_hooks(model)\n",
    "\n",
    "compute_device = 'cpu' \n",
    "\n",
    "layer_names = list(flexgen_layers)\n",
    "for i, layer_name in enumerate(layer_names): # layer names\n",
    "    layer_module = get_obj_from_name(model, layer_name)\n",
    "    next_layer_name = layer_names[i + 1] if i < len(layer_names) - 1 else None \n",
    "\n",
    "    layer_weight_hook = LayerWeightHook(\n",
    "        model=model, layer_name=layer_name, next_layer_name=next_layer_name, compute_device=compute_device)\n",
    "    add_hook_to_module(layer_module, layer_weight_hook, append=True)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1535: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "2023-09-24 12:01:33,880 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 10]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:33,888 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 10, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:33,891 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 10]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:33,903 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 10, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:33,907 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:34,052 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:34,056 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:34,187 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:34,191 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:34,314 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:34,318 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:34,438 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:34,442 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:34,574 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:34,579 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:34,719 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:34,725 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:34,865 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:34,869 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:35,007 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:35,011 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:35,145 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:35,150 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:35,218 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:35,222 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:35,403 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:35,408 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 10, 10]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': <class 'NoneType'>, 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:35,538 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 10, 768]), (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:35,542 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:35,556 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 10, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:35,559 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 10, 768]),),\n",
      " kwargs: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 12:01:35,675 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 10, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:35,757 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:35,761 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:35,763 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 11]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:35,765 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:35,766 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:35,884 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:35,890 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:35,998 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,002 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:36,084 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,089 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:36,172 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,176 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:36,277 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,285 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:36,366 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,371 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:36,464 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,468 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:36,539 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,543 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:36,643 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,647 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:36,728 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,733 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:36,810 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,816 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 11]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 10, 64]), torch.Size([18, 12, 10, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:36,896 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,899 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:36,915 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:36,917 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:36,942 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,001 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:37,006 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,007 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 12]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:37,013 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,015 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:37,087 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,098 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:37,172 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,175 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:37,276 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,283 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:37,396 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,403 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:37,522 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,530 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:37,635 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,644 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:37,711 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,716 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:37,787 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,791 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:37,856 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,871 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:37,937 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:37,943 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,006 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,011 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 12]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 11, 64]), torch.Size([18, 12, 11, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,081 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,085 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:38,093 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,096 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:38,118 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,179 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:38,182 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,183 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 13]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:38,185 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,187 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,264 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,274 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,338 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,342 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,389 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,394 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,438 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,442 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,472 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,475 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,540 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,544 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,609 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,613 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,747 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,754 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,836 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,841 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,908 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,914 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:38,985 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:38,989 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 13]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 12, 64]), torch.Size([18, 12, 12, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,053 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,060 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:39,067 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,069 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:39,094 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,153 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:39,156 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,158 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 14]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:39,160 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,162 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,228 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,232 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,299 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,305 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,367 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,373 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,484 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,489 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,540 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,546 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,591 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,595 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,676 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,680 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,744 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,748 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,838 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,844 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:39,917 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:39,923 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,001 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,005 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 14]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 13, 64]), torch.Size([18, 12, 13, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,070 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,073 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:40,085 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,086 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:40,113 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,163 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:40,166 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,167 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 15]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:40,169 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,171 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,238 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,242 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,314 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,318 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,390 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,397 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,468 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,472 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,540 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,545 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,634 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,641 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,724 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,727 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,797 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,801 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,874 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,878 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,949 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,952 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:40,977 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:40,982 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 15]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 14, 64]), torch.Size([18, 12, 14, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,050 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,058 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:41,067 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,070 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:41,094 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,156 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:41,159 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,161 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 16]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:41,164 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,166 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,235 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,239 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,336 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,342 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,411 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,416 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,462 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,466 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,563 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,568 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,657 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,663 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,754 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,758 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,823 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,828 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,905 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,908 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:41,988 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:41,997 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,074 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,079 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 16]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 15, 64]), torch.Size([18, 12, 15, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,141 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,145 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:42,153 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,155 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:42,182 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,236 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:42,239 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,241 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 17]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:42,244 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,246 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,317 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,321 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,383 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,388 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,453 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,457 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,520 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,523 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,587 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,591 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,649 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,653 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,713 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,718 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,786 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,789 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,851 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,858 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,934 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,938 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:42,956 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:42,959 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 17]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 16, 64]), torch.Size([18, 12, 16, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,023 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,027 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:43,034 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,036 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:43,055 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,107 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:43,110 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,111 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 18]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:43,114 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,115 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,176 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,181 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,209 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,216 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,287 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,292 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,354 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,360 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,422 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,428 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,491 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,494 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,554 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,558 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,618 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,622 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,639 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,643 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,709 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,713 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,784 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,787 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 18]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 17, 64]), torch.Size([18, 12, 17, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:43,857 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,861 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:43,869 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,870 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:43,890 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,943 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:43,946 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,947 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 19]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:43,949 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:43,951 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,015 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,020 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,082 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,087 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,149 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,153 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,213 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,216 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,262 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,266 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,319 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,322 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,337 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,340 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,399 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,403 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,465 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,469 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,535 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,538 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,607 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,613 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 19]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 18, 64]), torch.Size([18, 12, 18, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,672 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,676 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:44,684 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,686 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:44,707 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,758 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:44,760 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,762 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 20]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:44,764 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,765 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,786 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,790 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,850 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,854 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,915 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,920 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:44,984 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:44,988 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,056 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,061 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,139 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,142 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,212 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,216 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,297 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,302 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,381 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,385 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,456 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,459 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,531 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,535 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 20]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 19, 64]), torch.Size([18, 12, 19, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,597 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,602 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:45,611 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,613 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:45,633 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,684 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:45,688 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,689 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 21]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:45,692 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,693 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,766 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,770 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,828 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,832 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,891 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,895 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:45,954 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:45,957 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,019 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,024 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,079 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,083 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,151 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,155 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,170 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,174 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,201 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,205 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,265 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,268 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,330 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,334 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 21]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 20, 64]), torch.Size([18, 12, 20, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,394 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,399 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:46,406 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,407 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:46,426 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,478 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:46,481 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,482 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 22]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:46,484 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,486 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,544 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,548 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,612 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,616 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,682 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,688 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,726 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,731 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,815 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,820 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,891 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,896 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:46,966 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:46,972 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,042 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,047 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,115 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,119 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,190 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,194 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,232 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,236 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 22]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 21, 64]), torch.Size([18, 12, 21, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,299 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,303 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:47,313 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,315 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:47,339 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,403 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:47,407 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,409 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 23]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:47,412 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,414 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,478 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,483 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,545 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,548 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,615 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,622 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,691 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,694 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,758 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,763 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,827 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,831 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,902 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,913 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:47,973 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:47,978 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,046 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,051 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,103 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,107 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,172 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,176 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 23]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 22, 64]), torch.Size([18, 12, 22, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,210 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,214 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:48,222 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,225 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:48,267 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,345 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:48,351 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,355 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 24]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:48,357 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,358 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,475 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,481 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,594 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,600 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,661 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,665 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,686 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,690 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,750 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,754 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,816 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,819 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,879 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,883 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:48,944 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:48,947 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,011 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,015 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,093 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,096 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,178 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,183 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 24]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 23, 64]), torch.Size([18, 12, 23, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,252 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,258 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:49,265 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,268 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:49,301 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,327 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:49,330 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,332 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 25]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:49,334 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,336 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,398 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,402 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,435 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,441 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,500 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,507 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,570 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,574 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,637 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,643 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,708 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,711 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,774 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,778 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,840 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,843 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,900 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,904 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:49,963 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:49,967 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,005 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,010 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 25]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 24, 64]), torch.Size([18, 12, 24, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,026 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,029 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:50,038 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,040 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:50,063 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,090 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:50,093 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,094 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 26]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:50,096 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,098 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,159 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,165 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,178 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,181 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,208 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,212 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,275 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,280 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,352 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,356 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,430 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,436 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,512 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,518 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,589 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,594 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,659 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,663 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,749 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,752 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,823 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,827 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 26]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 25, 64]), torch.Size([18, 12, 25, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:50,902 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,906 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:50,913 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,917 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:50,940 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,989 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:50,992 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,994 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 27]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:50,996 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:50,997 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,067 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,071 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,142 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,146 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,219 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,225 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,295 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,299 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,376 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,384 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,459 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,465 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,535 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,540 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,637 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,644 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,722 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,727 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,798 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,801 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,869 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,876 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 27]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 26, 64]), torch.Size([18, 12, 26, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:51,956 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,960 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:51,968 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:51,971 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:51,996 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,051 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:52,054 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,055 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 28]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:52,058 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,059 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,124 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,128 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,196 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,201 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,242 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,246 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,315 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,319 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,392 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,396 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,513 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,517 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,596 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,600 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,668 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,672 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,743 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,747 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,811 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,814 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,878 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,884 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 28]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 27, 64]), torch.Size([18, 12, 27, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:52,952 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,956 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:52,966 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:52,969 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:52,990 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,038 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:53,043 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,045 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 29]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:53,047 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,048 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,111 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,115 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,178 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,184 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,254 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,259 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,324 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,328 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,396 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,400 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,465 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,468 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,540 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,544 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,610 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,614 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,679 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,684 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,759 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,762 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,825 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,830 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 29]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 28, 64]), torch.Size([18, 12, 28, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,887 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,893 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:53,901 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,903 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:53,925 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,977 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:53,979 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,981 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 30]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:53,983 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:53,985 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:53,997 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,001 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,042 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,046 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,110 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,114 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,175 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,181 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,215 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,220 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,287 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,291 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,356 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,362 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,429 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,432 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,489 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,495 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,569 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,572 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,634 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,638 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 30]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 29, 64]), torch.Size([18, 12, 29, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,695 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,699 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:54,706 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,707 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:54,727 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,777 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:54,780 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,786 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 31]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:54,797 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,809 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,878 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,882 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:54,945 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:54,949 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,011 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,014 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,079 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,082 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,152 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,155 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,227 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,231 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,297 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,300 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,367 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,372 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,452 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,457 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,525 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,529 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,602 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,606 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 31]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 30, 64]), torch.Size([18, 12, 30, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,679 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,683 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:55,694 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,695 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:55,725 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,816 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:55,819 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,821 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 32]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:55,824 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,826 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:55,906 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:55,911 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,010 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,015 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,081 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,088 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,165 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,170 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,292 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,296 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,363 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,367 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,403 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,407 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,476 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,480 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,550 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,554 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,605 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,609 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,674 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,678 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 32]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 31, 64]), torch.Size([18, 12, 31, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,698 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,701 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:56,710 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,712 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:56,735 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,785 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:56,788 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,790 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 33]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:56,792 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,794 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,862 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,867 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,914 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,921 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:56,991 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:56,995 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,067 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,071 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,138 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,143 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,212 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,215 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,240 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,244 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,317 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,322 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,391 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,395 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,463 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,466 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,513 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,516 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 33]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 32, 64]), torch.Size([18, 12, 32, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,582 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,585 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:57,593 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,596 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:57,618 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,677 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:57,680 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,681 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 34]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:57,684 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,685 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,752 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,756 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,821 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,825 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,890 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,895 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:57,961 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:57,965 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,029 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,034 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,102 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,106 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,170 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,175 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,243 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,247 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,311 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,316 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,385 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,389 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,463 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,467 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 34]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 33, 64]), torch.Size([18, 12, 33, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,532 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,536 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:58,543 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,544 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:58,565 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,617 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:58,622 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,625 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 35]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:58,628 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,629 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,699 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,702 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,768 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,771 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,837 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,842 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,905 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,909 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:58,975 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:58,984 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,048 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,054 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,119 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,123 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,189 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,193 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,267 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,272 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,335 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,339 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,402 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,406 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 35]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 34, 64]), torch.Size([18, 12, 34, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,468 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,474 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:59,484 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,486 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:59,507 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,562 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:59,565 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,566 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 36]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:01:59,569 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,570 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,638 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,644 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,708 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,713 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,775 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,779 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,842 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,846 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,910 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,914 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,932 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,936 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:01:59,994 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:01:59,998 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,062 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,065 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,128 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,132 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,192 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,195 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,257 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,261 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 36]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 35, 64]), torch.Size([18, 12, 35, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,323 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,328 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:00,336 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,339 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:00,362 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,414 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:00,416 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,418 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 37]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:00,420 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,421 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,486 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,490 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,552 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,556 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,623 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,627 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,690 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,698 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,765 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,770 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,832 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,835 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,895 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,901 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:00,968 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:00,973 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:01,037 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,041 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:01,108 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,111 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:01,179 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,185 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 37]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 36, 64]), torch.Size([18, 12, 36, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:01,278 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,283 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:01,293 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,295 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:01,327 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,430 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:01,432 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,434 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 38]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:01,437 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,438 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:01,576 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,580 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:01,706 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,711 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:01,844 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,849 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:01,974 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:01,980 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:02,101 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,106 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:02,158 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,163 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:02,257 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,266 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:02,392 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,395 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:02,519 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,524 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:02,549 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,553 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:02,598 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,604 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 38]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 37, 64]), torch.Size([18, 12, 37, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:02,701 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,706 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:02,718 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,721 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:02,761 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,842 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_tokens forward,\n",
      " args:(torch.Size([18, 1]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:02,845 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_tokens forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,847 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.embed_positions forward,\n",
      " args:(torch.Size([18, 39]), <class 'int'>),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:02,851 [1792503362.py:86 in post_forward] INFO - post model.decoder.embed_positions forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,852 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.0 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:02,988 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.0 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:02,993 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.1 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,055 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.1 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,060 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.2 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,128 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.2 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,133 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.3 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,195 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.3 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,200 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.4 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,260 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.4 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,265 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.5 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,324 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.5 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,329 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.6 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,390 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.6 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,394 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.7 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,459 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.7 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,463 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.8 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,535 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.8 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,540 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.9 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,627 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.9 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,631 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.10 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,676 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.10 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,680 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.layers.11 forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {'attention_mask': torch.Size([18, 1, 1, 39]), 'layer_head_mask': <class 'NoneType'>, 'past_key_value': (torch.Size([18, 12, 38, 64]), torch.Size([18, 12, 38, 64])), 'output_attentions': <class 'bool'>, 'use_cache': <class 'bool'>}\n",
      "2023-09-24 12:02:03,689 [1792503362.py:86 in post_forward] INFO - post model.decoder.layers.11 forward, \n",
      "output: (torch.Size([18, 1, 768]), (torch.Size([18, 12, 39, 64]), torch.Size([18, 12, 39, 64])))\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,692 [1792503362.py:73 in pre_forward] INFO - pre model.decoder.final_layer_norm forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:03,698 [1792503362.py:86 in post_forward] INFO - post model.decoder.final_layer_norm forward, \n",
      "output: torch.Size([18, 1, 768])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,699 [1792503362.py:73 in pre_forward] INFO - pre lm_head forward,\n",
      " args:(torch.Size([18, 1, 768]),),\n",
      " kwargs: {}\n",
      "2023-09-24 12:02:03,721 [1792503362.py:86 in post_forward] INFO - post lm_head forward, \n",
      "output: torch.Size([18, 1, 50272])\n",
      "\n",
      "\n",
      "2023-09-24 12:02:03,795 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?_?  *I am a bot, and this action was performed automatically. Please [ Length ) if you have any questions or concerns.*\n",
      "2023-09-24 12:02:03,796 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,797 [2210540570.py:26 in <module>] INFO - Where is Deutschland?: A rich nation, a beautiful people, a place to live\n",
      "Where is Deutschland?\n",
      "It comes down to what is, to judge\n",
      "2023-09-24 12:02:03,798 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,799 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro?\n",
      "Huawei's Huawei Mate 60 Pro is an ambitious, cutting-edge device, but it's still going to be far from its best. Despite\n",
      "2023-09-24 12:02:03,799 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,800 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?—\n",
      "Haha, yes. I'm a therapist.    I should've said I don't know too much but I remember in undergrad\n",
      "2023-09-24 12:02:03,801 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,802 [2210540570.py:26 in <module>] INFO - Where is Deutschland?: a brand spanish\n",
      "Where's Mexico? I live in Mexico City\n",
      "2023-09-24 12:02:03,803 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,805 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro?1: Latest rumors suggest it is a 'budget' phone\n",
      "Huawei is reportedly looking highly into its smartphones, with the smartphone's price set at\n",
      "2023-09-24 12:02:03,806 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,808 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?�()\n",
      "I was.  I just feel like I don't have anything to remember to do other than to sleep.\n",
      "2023-09-24 12:02:03,809 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,809 [2210540570.py:26 in <module>] INFO - Where is Deutschland?><br /><br /><br />As I look out of the window at a mountain, and a mountain of trees across the garden, it\n",
      "2023-09-24 12:02:03,811 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,812 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro?1 leaked yet?\n",
      "\n",
      "A Huawei device is finally leaked which features the Huawei Mate 60 Pro with a 5.7-inch display display powered by\n",
      "2023-09-24 12:02:03,813 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,814 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?�\n",
      "Yes I'm, I'm not. I was just thinking because it seems to be really strange. Its not quite real either. But I\n",
      "2023-09-24 12:02:03,815 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,816 [2210540570.py:26 in <module>] INFO - Where is Deutschland? \"I am American and I have never been to a Nazi country. This is some anti-Semitic shit.\"\n",
      "There's been no Nazis in the\n",
      "2023-09-24 12:02:03,817 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,818 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro? _ZTE\n",
      "Huawei Mate 60 Pro is a great phone.\n",
      "Well the company is very good at marketing their phones at such high quality.\n",
      "2023-09-24 12:02:03,819 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,820 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?>(insert my own reaction)  (no sarcasm here)\n",
      "I do not belong in this sub because I am not very conscious. I\n",
      "2023-09-24 12:02:03,820 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,821 [2210540570.py:26 in <module>] INFO - Where is Deutschland?ook.  Edit: Looks like Germany. It ain't Germany.\n",
      "In the Netherlands. It doesn't seem like it, but the accent seems\n",
      "2023-09-24 12:02:03,822 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,823 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro?_ \n",
      "They are selling the Mate 60 Pro to buy now, and there's already litres of product being offered at a similar price to the Mate\n",
      "2023-09-24 12:02:03,825 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,826 [2210540570.py:26 in <module>] INFO - Who are you? Are you conscious?�\n",
      "The last person, in the middle, I am not conscious  the second person, in the middle   and no, that is not\n",
      "2023-09-24 12:02:03,827 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,828 [2210540570.py:26 in <module>] INFO - Where is Deutschland? (I saw that you guys were down to 1 person so it must really be a huge market.)\n",
      "There is no US mainland. The US is\n",
      "2023-09-24 12:02:03,830 [2210540570.py:27 in <module>] INFO - ----------\n",
      "2023-09-24 12:02:03,831 [2210540570.py:26 in <module>] INFO - How is Huawei Mate 60 Pro?4? Is this too far removed? The price is $1,500 for the Mate60 Pro5 and $1,599 for the Mate 60\n",
      "2023-09-24 12:02:03,831 [2210540570.py:27 in <module>] INFO - ----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate test\n",
    "\n",
    "prompts = [\n",
    "    'Who are you? Are you conscious?',\n",
    "    'Where is Deutschland?',\n",
    "    'How is Huawei Mate 60 Pro?'\n",
    "] * 6\n",
    "\n",
    "prompt_len = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompts, padding=\"max_length\", max_length=prompt_len, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    max_length=30 + prompt_len,\n",
    "    # num_beams=2,\n",
    "    # num_beam_groups=2,\n",
    "    # diversity_penalty=0.1,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "output_texts = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "for output_text in output_texts:\n",
    "    logging.info(output_text)\n",
    "    logging.info('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args_type_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m args_type_set, kwargs_type_set, output_type_set\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args_type_set' is not defined"
     ]
    }
   ],
   "source": [
    "args_type_set, kwargs_type_set, output_type_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LayerWeightHook at 0x7ff14e56fb80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_module._hf_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load / offload\n",
    "#     module object, .dat file path\n",
    "#     layer: pre / post forward hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_activation_assignment(num_layers, offload_config: Policy):\n",
    "#     logging.debug(f\"<compute_activation_assignment> enter\")\n",
    "#     gpu_batch_limit = int(offload_config.num_gpu_batches * offload_config.act_gpu_percent)\n",
    "#     cpu_batch_limit = int(offload_config.num_gpu_batches * (offload_config.act_gpu_percent + offload_config.act_cpu_percent))\n",
    "#     logging.debug(f\"<compute_activation_assignment> gpu_batch_limit: {gpu_batch_limit}, cpu_batch_limit: {cpu_batch_limit}\")\n",
    "    \n",
    "#     act_assign_dict = {}\n",
    "#     for l in range(num_layers):\n",
    "#         for i in range(offload_config.num_gpu_batches):\n",
    "#             act_key = f\"layer.{l}_index.{i}\"\n",
    "#             if i < gpu_batch_limit:\n",
    "#                 device = 'cuda'\n",
    "#             elif i < cpu_batch_limit:\n",
    "#                 device = 'cpu'\n",
    "#             else:\n",
    "#                 device = 'disk'\n",
    "#             act_assign_dict[act_key]= {'assigned_device': device}\n",
    "#     return act_assign_dict\n",
    "\n",
    "\n",
    "# def compute_kv_cache_assignment(num_layers, offload_config: OffloadConfig):\n",
    "#     logging.debug(f\"<compute_kv_cache_assignment> enter\")\n",
    "#     gpu_batch_limit = int(offload_config.num_gpu_batches * offload_config.cache_gpu_percent)\n",
    "#     cpu_batch_limit = int(offload_config.num_gpu_batches * (offload_config.cache_gpu_percent + offload_config.cache_cpu_percent))\n",
    "#     logging.debug(f\"<compute_kv_cache_assignment> gpu_batch_limit: {gpu_batch_limit}, cpu_batch_limit: {cpu_batch_limit}\")\n",
    "    \n",
    "#     act_assign_dict = {}\n",
    "#     for l in range(num_layers):\n",
    "#         for i in range(offload_config.num_gpu_batches):\n",
    "#             key_cache_key = f\"key_layer.{l}_index.{i}\"\n",
    "#             value_cache_key = f\"key_layer.{l}_index.{i}\"\n",
    "#             if i < gpu_batch_limit:\n",
    "#                 device = 'cuda'\n",
    "#             elif i < cpu_batch_limit:\n",
    "#                 device = 'cpu'\n",
    "#             else:\n",
    "#                 device = 'disk'\n",
    "#             act_assign_dict[key_cache_key] = {'assigned_device': device}\n",
    "#             act_assign_dict[value_cache_key] = {'assigned_device': device}\n",
    "#     return act_assign_dict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
