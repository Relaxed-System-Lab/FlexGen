{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, OPTForCausalLM, BloomForCausalLM, CodeGenForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.33.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "# model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 240/240 [00:00<00:00, 69.4kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 4.44MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 731kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 4.75MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 1.00k/1.00k [00:00<00:00, 867kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 86.5kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 999/999 [00:00<00:00, 956kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 797M/797M [00:14<00:00, 53.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "model = CodeGenForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeGenForCausalLM(\n",
       "  (transformer): CodeGenModel(\n",
       "    (wte): Embedding(51200, 1024)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-19): 20 x CodeGenBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CodeGenAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): CodeGenMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, <function __main__.model_pre_hook(module, args, kwargs)>)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_pre_hook(module, args, kwargs):\n",
    "    # print(args, kwargs)\n",
    "    display = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            display[k] = v.shape \n",
    "        elif k == 'past_key_values' and v is not None:\n",
    "            display[k] = f'{len(v)} * ' + str((v[0][0].shape, v[0][1].shape))\n",
    "        else:\n",
    "            display[k] = v\n",
    "    print(display)\n",
    "\n",
    "    return args, kwargs\n",
    "\n",
    "model._forward_pre_hooks.clear()\n",
    "model.register_forward_pre_hook(model_pre_hook, with_kwargs=True)\n",
    "model._forward_pre_hooks\n",
    "\n",
    "# def layer_pre_hook(module, args, kwargs):\n",
    "#     display = {}\n",
    "#     for k, v in kwargs.items():\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             display[k] = v.shape \n",
    "#         elif k in ['past_key_value', 'layer_past'] and v is not None:\n",
    "#             display[k] = str((v[0].shape, v[1].shape))\n",
    "#         else:\n",
    "#             display[k] = v\n",
    "#     print(display)\n",
    "\n",
    "#     return args, kwargs\n",
    "\n",
    "# layer = model.model.decoder.layers[2]\n",
    "\n",
    "# layer._forward_pre_hooks.clear()\n",
    "# layer.register_forward_pre_hook(layer_pre_hook, with_kwargs=True)\n",
    "# layer._forward_pre_hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([12, 20]), 'past_key_values': None, 'use_cache': True, 'position_ids': torch.Size([12, 20]), 'attention_mask': torch.Size([12, 20]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 20, 64]), torch.Size([12, 16, 20, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 21]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 21, 64]), torch.Size([12, 16, 21, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 22]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 22, 64]), torch.Size([12, 16, 22, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 23]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 23, 64]), torch.Size([12, 16, 23, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 24]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 24, 64]), torch.Size([12, 16, 24, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 25]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 25, 64]), torch.Size([12, 16, 25, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 26]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 26, 64]), torch.Size([12, 16, 26, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 27]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 27, 64]), torch.Size([12, 16, 27, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 28]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 28, 64]), torch.Size([12, 16, 28, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 29]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 29, 64]), torch.Size([12, 16, 29, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 30]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 30, 64]), torch.Size([12, 16, 30, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 31]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 31, 64]), torch.Size([12, 16, 31, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 32]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 32, 64]), torch.Size([12, 16, 32, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 33]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 33, 64]), torch.Size([12, 16, 33, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 34]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 34, 64]), torch.Size([12, 16, 34, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 35]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 35, 64]), torch.Size([12, 16, 35, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 36]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 36, 64]), torch.Size([12, 16, 36, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 37]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 37, 64]), torch.Size([12, 16, 37, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 38]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 38, 64]), torch.Size([12, 16, 38, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 39]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 39, 64]), torch.Size([12, 16, 39, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 40]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 40, 64]), torch.Size([12, 16, 40, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 41]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 41, 64]), torch.Size([12, 16, 41, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 42]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 42, 64]), torch.Size([12, 16, 42, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 43]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 43, 64]), torch.Size([12, 16, 43, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 44]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 44, 64]), torch.Size([12, 16, 44, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 45]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 45, 64]), torch.Size([12, 16, 45, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 46]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 46, 64]), torch.Size([12, 16, 46, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 47]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 47, 64]), torch.Size([12, 16, 47, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 48]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 48, 64]), torch.Size([12, 16, 48, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 49]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "Who are you? Are you conscious?#!/usr/bin/env python\n",
      "\n",
      "# Copyright (c) 2021 - present in the source code of this module\n",
      "\n",
      "import os\n",
      "\n",
      "----------\n",
      "Where is Deutschland?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "Who are you? Are you conscious?#!/usr/bin/env python\n",
      "\n",
      "# Copyright (c) 2021 - present in the source code of this module\n",
      "\n",
      "import os\n",
      "\n",
      "----------\n",
      "Where is Deutschland?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "Who are you? Are you conscious?#!/usr/bin/env python\n",
      "\n",
      "# Copyright (c) 2021 - present in the source code of this module\n",
      "\n",
      "import os\n",
      "\n",
      "----------\n",
      "Where is Deutschland?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "Who are you? Are you conscious?#!/usr/bin/env python\n",
      "\n",
      "# Copyright (c) 2021 - present in the source code of this module\n",
      "\n",
      "import os\n",
      "\n",
      "----------\n",
      "Where is Deutschland?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'Who are you? Are you conscious?',\n",
    "    'Where is Deutschland?',\n",
    "    'How is Huawei Mate 60 Pro?'\n",
    "] * 4\n",
    "\n",
    "prompt_len = 20\n",
    "\n",
    "inputs = tokenizer(prompts, padding=\"max_length\", max_length=prompt_len, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=30 + prompt_len,\n",
    "    # num_beams=2,\n",
    "    # num_beam_groups=2,\n",
    "    # diversity_penalty=0.1,\n",
    "    # do_sample=True,\n",
    ")\n",
    "\n",
    "output_texts = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "for output_text in output_texts:\n",
    "    print(output_text)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
