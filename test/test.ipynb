{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingfangyu/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, OPTForCausalLM, MistralForCausalLM\n",
    "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from accelerate.hooks import remove_hook_from_module\n",
    "from accelerate.utils import named_module_tensors, find_tied_parameters\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.format import open_memmap\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from threading import Thread\n",
    "from queue import Queue \n",
    "\n",
    "import functools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'facebook/opt-125m'\n",
    "# checkpoint = 'facebook/opt-13B'\n",
    "# checkpoint = 'mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "comp_device = 0\n",
    "torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingfangyu/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. get model parameter & buffer names\n",
    "2. find the transformer block module\n",
    "3. get a device map\n",
    "4. get offloaded weights np.memmap files\n",
    "\"\"\"\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights(): \n",
    "    e: OPTForCausalLM = AutoModelForCausalLM.from_config(config,)\n",
    "# don't run e.tie_weights() or the tied weights will not be in the device map\n",
    "# e.tie_weights()\n",
    "    \n",
    "def find_module_list(module: nn.Module):\n",
    "    def _find_module_list(module: nn.Module, prefix=''):\n",
    "        if isinstance(module, nn.ModuleList):\n",
    "            yield module, prefix\n",
    "        else:\n",
    "            for name, child in module.named_children():\n",
    "                yield from _find_module_list(child, prefix=prefix+'.'+name if prefix else name)\n",
    "    \n",
    "    g = _find_module_list(module)\n",
    "    try:\n",
    "        return next(iter(g))\n",
    "    except:\n",
    "        raise ValueError(f'{module.__class__.__name__} does not have a nn.ModuleList structure')\n",
    "\n",
    "layers, layers_name = find_module_list(e)\n",
    "# layers_name\n",
    "\n",
    "res = {}\n",
    "for n, t in named_module_tensors(e, recurse=True):\n",
    "    if isinstance(t, nn.Parameter) and layers_name in n:\n",
    "        res[n] = 'disk'\n",
    "    else:\n",
    "        res[n] = comp_device\n",
    "\n",
    "weights_offload_folder = f'_weights_offload/{checkpoint}/{torch_dtype}'\n",
    "\n",
    "# all parameters of the model will be offloaded as memory-mapped array in a given folder.\n",
    "if not os.path.exists(weights_offload_folder):\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=res, torch_dtype=torch_dtype, offload_folder=weights_offload_folder, use_safetensors=False) # use pytorch *.bin, as disk_offload have some bugs for safetensors\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=res, torch_dtype=torch_dtype, offload_folder=None) \n",
    "\n",
    "# remove accelerate disk_offload hooks\n",
    "model = remove_hook_from_module(model, recurse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0139, -0.0025, -0.0152,  ..., -0.0056, -0.0118, -0.0037])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiskWeightsLoader:\n",
    "    def __init__(self, weights_offload_folder) -> None:\n",
    "        self.weights_offload_folder = weights_offload_folder\n",
    "\n",
    "        with open(os.path.join(weights_offload_folder, \"index.json\"), \"r\") as f: \n",
    "            self.index = json.load(f)  \n",
    "\n",
    "    def open_memmap(self, key):\n",
    "        metadata = self.index[key]\n",
    "\n",
    "        f_name = os.path.join(weights_offload_folder, key + '.dat')\n",
    "\n",
    "        shape = tuple(metadata[\"shape\"])\n",
    "        if shape == ():\n",
    "            # NumPy memory-mapped arrays can't have 0 dims so it was saved as 1d tensor\n",
    "            shape = (1,)\n",
    "\n",
    "        dtype = metadata[\"dtype\"]\n",
    "        if dtype == \"bfloat16\":\n",
    "            # NumPy does not support bfloat16 so this was saved as a int16\n",
    "            dtype = \"int16\"\n",
    "\n",
    "        weight = np.memmap(f_name, dtype=dtype, shape=shape, mode=\"r\")\n",
    "\n",
    "        if len(metadata[\"shape\"]) == 0:\n",
    "            weight = weight[0]\n",
    "\n",
    "        weight = torch.from_numpy(weight) # no data movement\n",
    "\n",
    "        if metadata[\"dtype\"] == \"bfloat16\":\n",
    "            weight = weight.view(torch.bfloat16)\n",
    "\n",
    "        return weight\n",
    "    \n",
    "dl = DiskWeightsLoader(weights_offload_folder)\n",
    "dl.open_memmap(\"model.decoder.layers.0.fc1.bias\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from accelerate.utils import honor_type\n",
    "from typing import Mapping\n",
    "\n",
    "def get_info(obj, debug=False):\n",
    "    if isinstance(obj, (tuple, list)):\n",
    "        ret = honor_type(obj, (get_info(o) for o in obj))\n",
    "        if len(set(ret)) == 1 and len(ret) > 1:\n",
    "            return f\"{len(ret)} * {ret[0]}\"\n",
    "        else:\n",
    "            return ret \n",
    "    elif isinstance(obj, Mapping):\n",
    "        return type(obj)({k: get_info(v) for k, v in obj.items()})\n",
    "    elif isinstance(obj, (torch.Tensor)):\n",
    "        if debug:\n",
    "            return f\"{obj.__class__.__name__}(shape={tuple(obj.size())}, dtype={obj.dtype}, device={obj.device}, mem/elem/dtype={sys.getsizeof(obj.storage()) / obj.numel() / obj.element_size():.3f})\"\n",
    "        else:\n",
    "            return f\"{obj.__class__.__name__}(shape={tuple(obj.size())}, mem/elem/dtype={sys.getsizeof(obj.storage()) / obj.numel() / obj.element_size():.3f})\"\n",
    "    elif isinstance(obj, (int, bool, type(None))):\n",
    "        return f\"{obj}\"\n",
    "    else:\n",
    "        return f\"{obj.__class__.__name__}: {obj}\"\n",
    "\n",
    "from data_movement import Engine, Task\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    1. override forward functions\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_model, comp_device=0, **kwargs) -> None:\n",
    "        # self.checkpoint = kwargs.get('checkpoint')\n",
    "        # self.torch_dtype = kwargs.get('torch_dtype')\n",
    "        # self.config = AutoConfig.from_pretrained(self.checkpoint, torch_dtype=self.torch_dtype)\n",
    "        # with init_empty_weights(): # while buffers are not empty\n",
    "        #     self.hf_model = AutoModelForCausalLM.from_config(self.config, torch_dtype=self.torch_dtype)\n",
    "        \n",
    "        self.comp_device = comp_device\n",
    "\n",
    "        self.dm_engine = Engine(self.comp_device)\n",
    "\n",
    "        # init model \n",
    "        self.hf_model = hf_model#.to(comp_device)\n",
    "        self.layers, self.layers_name = self.get_layers()\n",
    "\n",
    "\n",
    "    def get_layers(self) -> tuple[nn.Module, str]:\n",
    "        if isinstance(self.hf_model, (OPTForCausalLM, )):\n",
    "            return self.hf_model.model.decoder.layers, 'model.decoder.layers'\n",
    "        else:\n",
    "            def find_module_list(module: nn.Module) -> tuple[nn.Module, str]:\n",
    "                def _find_module_list(module: nn.Module, prefix=''):\n",
    "                    if isinstance(module, nn.ModuleList):\n",
    "                        yield module, prefix\n",
    "                    else:\n",
    "                        for name, child in module.named_children():\n",
    "                            yield from _find_module_list(child, prefix=prefix+'.'+name if prefix else name)\n",
    "                \n",
    "                g = _find_module_list(module)\n",
    "                try:\n",
    "                    return next(iter(g))\n",
    "                except:\n",
    "                    raise ValueError(f'{module.__class__.__name__} does not have a nn.ModuleList structure')\n",
    "\n",
    "            return find_module_list(self.hf_model)\n",
    "    \n",
    "    def override_layer_forward(self, i: int):\n",
    "        layer = self.layers[i]\n",
    "        old_forward = layer.forward\n",
    "\n",
    "        @functools.wraps(old_forward)\n",
    "        def new_forward(*args, **kwargs):\n",
    "            print(f'\\t{i = }, {get_info(args) = }, \\n\\t{i = }, {get_info(kwargs) = }')\n",
    "\n",
    "            if isinstance(self.hf_model, (OPTForCausalLM, )):\n",
    "                actv_recomp = args[0] # b,1,h / bzh\n",
    "                kv_cache = kwargs.get('past_key_value') # b,n_kv_heads,s_cache,h_kv    x2\n",
    "                attn_mask = kwargs.get('attention_mask') # b,1,1,s_all  (bsz, 1, tgt_len, src_len)\n",
    "\n",
    "            # new to hf: args, kwargs\n",
    "            args_for_old = args\n",
    "            kwargs_for_old = kwargs\n",
    "\n",
    "            # hf execution\n",
    "            old_output = old_forward(*args_for_old, **kwargs_for_old) # h'=(b,z,h), kv=(b,n,s_all,h) x2\n",
    "            \n",
    "            # hf to new: output\n",
    "            output = old_output\n",
    "            print(f'\\t{i = }, {get_info(output) = }\\n')\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        layer.forward = new_forward\n",
    "        return layer\n",
    "\n",
    "    def override_hf_model_forward(self):\n",
    "        old_forward = self.hf_model.forward\n",
    "        @functools.wraps(old_forward)\n",
    "        def new_forward(*args, **kwargs):\n",
    "            print(f'hf_model {get_info(args) = }, \\nhf_model {get_info(kwargs) = }\\n')\n",
    "\n",
    "            # new to hf: args, kwargs\n",
    "            args_for_old = args\n",
    "            kwargs_for_old = kwargs\n",
    "\n",
    "            # hf execution\n",
    "            old_output = old_forward(*args_for_old, **kwargs_for_old) \n",
    "\n",
    "            # hf to new: output\n",
    "            output = old_output \n",
    "            print(f'hf_model {get_info(output) = }\\n')\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        self.hf_model.forward = new_forward\n",
    "        return self.hf_model\n",
    "\n",
    "    def build(self):\n",
    "        for i, _ in enumerate(self.layers):\n",
    "            self.override_layer_forward(i)\n",
    "        self.override_hf_model_forward()\n",
    "        return self.hf_model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/tmp/ipykernel_201893/1397479523.py:18: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return f\"{obj.__class__.__name__}(shape={tuple(obj.size())}, mem/elem/dtype={sys.getsizeof(obj.storage()) / obj.numel() / obj.element_size():.3f})\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_model get_info(args) = (), \n",
      "hf_model get_info(kwargs) = {'input_ids': 'Tensor(shape=(96, 50), mem/elem/dtype=1.001)', 'past_key_values': 'None', 'use_cache': 'True', 'attention_mask': 'Tensor(shape=(96, 50), mem/elem/dtype=1.001)', 'return_dict': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False'}\n",
      "\n",
      "\ti = 0, get_info(args) = ('Tensor(shape=(96, 50, 768), mem/elem/dtype=1.000)',), \n",
      "\ti = 0, get_info(kwargs) = {'attention_mask': 'Tensor(shape=(96, 1, 50, 50), mem/elem/dtype=1.000)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensor on device meta is not on the expected device cuda:0!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m\n\u001b[1;32m     31\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     32\u001b[0m     prompts,\n\u001b[1;32m     33\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[39m# padding=True,\u001b[39;00m\n\u001b[1;32m     37\u001b[0m )\u001b[39m.\u001b[39mto(comp_device)\n\u001b[1;32m     39\u001b[0m \u001b[39m# generate\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m generate_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     41\u001b[0m     inputs\u001b[39m.\u001b[39;49minput_ids,\n\u001b[1;32m     42\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49mgen_len,  \u001b[39m# max_lengths\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m     \n\u001b[1;32m     44\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, \u001b[39m#\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m     num_beam_groups\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m#\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m     diversity_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, \u001b[39m#\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m     \u001b[39m# do_sample=True, #\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[39m# outputs\u001b[39;00m\n\u001b[1;32m     51\u001b[0m output_texts \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(\n\u001b[1;32m     52\u001b[0m     generate_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     53\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1826\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1820\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1821\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1822\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1823\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1824\u001b[0m     )\n\u001b[1;32m   1825\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1826\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_group_beam_search(\n\u001b[1;32m   1827\u001b[0m         input_ids,\n\u001b[1;32m   1828\u001b[0m         beam_scorer,\n\u001b[1;32m   1829\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1830\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1831\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   1832\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1833\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1834\u001b[0m     )\n\u001b[1;32m   1836\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONSTRAINED_BEAM_SEARCH:\n\u001b[1;32m   1837\u001b[0m     final_constraints \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2930\u001b[0m, in \u001b[0;36mGenerationMixin._group_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2928\u001b[0m \u001b[39m# do one decoder step on all beams of all sentences in batch\u001b[39;00m\n\u001b[1;32m   2929\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2930\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2931\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2932\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2933\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2934\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2935\u001b[0m )\n\u001b[1;32m   2937\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2938\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 106\u001b[0m, in \u001b[0;36mModel.override_hf_model_forward.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m kwargs_for_old \u001b[39m=\u001b[39m kwargs\n\u001b[1;32m    105\u001b[0m \u001b[39m# hf execution\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m old_output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs_for_old, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_for_old) \n\u001b[1;32m    108\u001b[0m \u001b[39m# hf to new: output\u001b[39;00m\n\u001b[1;32m    109\u001b[0m output \u001b[39m=\u001b[39m old_output \n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/opt/modeling_opt.py:1117\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1114\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1116\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1118\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1119\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1120\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1121\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1122\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1123\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1124\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1125\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1126\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1127\u001b[0m )\n\u001b[1;32m   1129\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m   1131\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/opt/modeling_opt.py:883\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    873\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    874\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    875\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    880\u001b[0m         use_cache,\n\u001b[1;32m    881\u001b[0m     )\n\u001b[1;32m    882\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 883\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    884\u001b[0m         hidden_states,\n\u001b[1;32m    885\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[1;32m    886\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    887\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    888\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    889\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    890\u001b[0m     )\n\u001b[1;32m    892\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 84\u001b[0m, in \u001b[0;36mModel.override_layer_forward.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m kwargs_for_old \u001b[39m=\u001b[39m kwargs\n\u001b[1;32m     83\u001b[0m \u001b[39m# hf execution\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m old_output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs_for_old, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_for_old) \u001b[39m# h'=(b,z,h), kv=(b,n,s_all,h) x2\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m# hf to new: output\u001b[39;00m\n\u001b[1;32m     87\u001b[0m output \u001b[39m=\u001b[39m old_output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/opt/modeling_opt.py:521\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39m# 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_layer_norm_before:\n\u001b[0;32m--> 521\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    523\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m    524\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    525\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    526\u001b[0m     past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    529\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    530\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    202\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:2573\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2569\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2570\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2571\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2572\u001b[0m     )\n\u001b[0;32m-> 2573\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_decomp/__init__.py:83\u001b[0m, in \u001b[0;36m_convert_out_params.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m is_none \u001b[39m=\u001b[39m out_kwargs[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m((o \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m is_none \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m out_kwargs)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, out\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m is_none \u001b[39melse\u001b[39;49;00m out_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_prims_common/wrappers.py:252\u001b[0m, in \u001b[0;36mout_wrapper.<locals>._out_wrapper.<locals>._fn\u001b[0;34m(out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, is_out\u001b[39m=\u001b[39m(out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    253\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    254\u001b[0m     \u001b[39misinstance\u001b[39m(result, TensorLike)\n\u001b[1;32m    255\u001b[0m     \u001b[39mand\u001b[39;00m is_tensor\n\u001b[1;32m    256\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(result, Tuple)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(out_names)\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Naively you might expect this assert to be true, but\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39m# it's not:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[39m# be a normal meta tensor, but this is perfectly\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[39m# harmless.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_refs/__init__.py:3213\u001b[0m, in \u001b[0;36mnative_layer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   3211\u001b[0m     out \u001b[39m=\u001b[39m out \u001b[39m*\u001b[39m weight\n\u001b[1;32m   3212\u001b[0m \u001b[39melif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m bias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 3213\u001b[0m     out \u001b[39m=\u001b[39m out \u001b[39m*\u001b[39;49m weight \u001b[39m+\u001b[39m bias\n\u001b[1;32m   3215\u001b[0m out \u001b[39m=\u001b[39m _maybe_convert_to_dtype(out, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdtype)  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   3216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_prims_common/wrappers.py:252\u001b[0m, in \u001b[0;36mout_wrapper.<locals>._out_wrapper.<locals>._fn\u001b[0;34m(out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, is_out\u001b[39m=\u001b[39m(out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    253\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    254\u001b[0m     \u001b[39misinstance\u001b[39m(result, TensorLike)\n\u001b[1;32m    255\u001b[0m     \u001b[39mand\u001b[39;00m is_tensor\n\u001b[1;32m    256\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(result, Tuple)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(out_names)\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Naively you might expect this assert to be true, but\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39m# it's not:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[39m# be a normal meta tensor, but this is perfectly\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[39m# harmless.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_prims_common/wrappers.py:137\u001b[0m, in \u001b[0;36melementwise_type_promotion_wrapper.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m promoted_args \u001b[39m=\u001b[39m {\n\u001b[1;32m    131\u001b[0m     x: _maybe_convert_to_dtype(bound\u001b[39m.\u001b[39marguments[x], compute_dtype)\n\u001b[1;32m    132\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_promoting_arg_names  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m x \u001b[39min\u001b[39;00m bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m    134\u001b[0m }\n\u001b[1;32m    135\u001b[0m bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mupdate(promoted_args)\n\u001b[0;32m--> 137\u001b[0m result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbound\u001b[39m.\u001b[39;49marguments)\n\u001b[1;32m    139\u001b[0m \u001b[39m# Override the return_dtype if a dtype arg is present and not None\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m bound\u001b[39m.\u001b[39marguments:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_refs/__init__.py:1042\u001b[0m, in \u001b[0;36m_make_elementwise_binary_reference.<locals>.inner.<locals>._ref\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1036\u001b[0m torch\u001b[39m.\u001b[39m_check_value(\n\u001b[1;32m   1037\u001b[0m     supports_two_python_scalars\n\u001b[1;32m   1038\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(a, Number) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(b, Number)),\n\u001b[1;32m   1039\u001b[0m     \u001b[39mlambda\u001b[39;00m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m: Receive two Number inputs to an elementwise binary operation!\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1040\u001b[0m )\n\u001b[1;32m   1041\u001b[0m a, b \u001b[39m=\u001b[39m _maybe_broadcast(a, b)\n\u001b[0;32m-> 1042\u001b[0m output \u001b[39m=\u001b[39m prim(a, b)\n\u001b[1;32m   1043\u001b[0m \u001b[39mreturn\u001b[39;00m handle_noncontiguous_outputs([a, b], output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_refs/__init__.py:1653\u001b[0m, in \u001b[0;36mmul\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[39m@_make_elementwise_binary_reference\u001b[39m(\n\u001b[1;32m   1649\u001b[0m     type_promotion_kind\u001b[39m=\u001b[39mELEMENTWISE_TYPE_PROMOTION_KIND\u001b[39m.\u001b[39mDEFAULT,\n\u001b[1;32m   1650\u001b[0m     supports_two_python_scalars\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1651\u001b[0m )\n\u001b[1;32m   1652\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmul\u001b[39m(a: TensorLikeType, b: TensorLikeType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorLikeType:\n\u001b[0;32m-> 1653\u001b[0m     \u001b[39mreturn\u001b[39;00m prims\u001b[39m.\u001b[39;49mmul(a, b)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_ops.py:594\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(self_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     \u001b[39m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    593\u001b[0m     \u001b[39m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m self_\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_prims/__init__.py:359\u001b[0m, in \u001b[0;36m_prim_elementwise_meta\u001b[0;34m(type_promotion, args_with_fixed_dtypes, *args)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m args_with_fixed_dtypes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     args_ \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(args_with_fixed_dtypes) \u001b[39m+\u001b[39m args_\n\u001b[0;32m--> 359\u001b[0m utils\u001b[39m.\u001b[39;49mcheck_same_device(\u001b[39m*\u001b[39;49margs_, allow_cpu_scalar_tensors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    360\u001b[0m utils\u001b[39m.\u001b[39mcheck_same_shape(\u001b[39m*\u001b[39margs_, allow_cpu_scalar_tensors\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    362\u001b[0m l2p_perm \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcompute_elementwise_output_logical_to_physical_perm(\u001b[39m*\u001b[39margs_)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_prims_common/__init__.py:740\u001b[0m, in \u001b[0;36mcheck_same_device\u001b[0;34m(allow_cpu_scalar_tensors, *args)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m!=\u001b[39m arg\u001b[39m.\u001b[39mdevice:\n\u001b[1;32m    733\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    734\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTensor on device \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m             \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(arg\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m         )\n\u001b[0;32m--> 740\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    741\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    742\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    743\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnexpected type when checking for same device, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(arg)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    744\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor on device meta is not on the expected device cuda:0!"
     ]
    }
   ],
   "source": [
    "num_prompts = 16\n",
    "prompts = None\n",
    "prompt_len = 50\n",
    "comp_device = 0\n",
    "gen_len = 20\n",
    "\n",
    "\n",
    "# hf_model= OPTForCausalLM.from_pretrained(checkpoint)\n",
    "model = Model(m, comp_device=comp_device).build()\n",
    "\n",
    "# test\n",
    "if True:\n",
    "    if prompts is None:  # get default prompts\n",
    "        prompts = [\n",
    "            \"for i in range(10): \",\n",
    "            \"Who are you? Are you conscious?\",\n",
    "            \"Where is Deutschland?\",\n",
    "            \"How is Huawei Mate 60 Pro?\",\n",
    "        ]\n",
    "    prompts = (\n",
    "        prompts * (num_prompts // len(prompts))\n",
    "        + prompts[: (num_prompts % len(prompts))]\n",
    "    )\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint) # , padding_side=\"left\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # eos padding\n",
    "\n",
    "    # inputs\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=prompt_len,\n",
    "        return_tensors=\"pt\",\n",
    "        # padding=True,\n",
    "    ).to(comp_device)\n",
    "\n",
    "    # generate\n",
    "    generate_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=gen_len,  # max_lengths\n",
    "        \n",
    "        num_beams=6, #\n",
    "        num_beam_groups=2, #\n",
    "        diversity_penalty=0.1, #\n",
    "        # do_sample=True, #\n",
    "    )\n",
    "\n",
    "    # outputs\n",
    "    output_texts = tokenizer.batch_decode(\n",
    "        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    print(output_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
