{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class Vector:\n",
    "    def __init__(self, \n",
    "        shape: tuple[int], \n",
    "        dtype: torch.dtype, \n",
    "        device: torch.device | str | int, \n",
    "        s_dim: int, \n",
    "        cap: int | None = None \n",
    "    ):\n",
    "        \n",
    "        self._shape = shape\n",
    "        self.dtype = dtype\n",
    "        self.device = device \n",
    "        self.s_dim = s_dim\n",
    "        self.cap = cap if cap else shape[s_dim] * 2\n",
    "\n",
    "        self.rear = shape[s_dim]  # x[:rear] or x[:len(valid_data)]\n",
    "        \n",
    "        _storage_shape = list(shape)\n",
    "        _storage_shape[s_dim] = self.cap\n",
    "        _storage_shape = tuple(_storage_shape)\n",
    "        self.storage_shape = _storage_shape\n",
    "        self.storage = torch.zeros(self.storage_shape, dtype=dtype, device=device)\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def from_tensor(cls, tensor: torch.Tensor, s_dim: int):\n",
    "        return cls(tensor.shape, tensor.dtype, tensor.device, s_dim)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "    \n",
    "    def size(self):\n",
    "        return self._shape \n",
    "\n",
    "    def check_broadcastable(self, x: torch.Tensor):\n",
    "        x_shape = list(x.shape)\n",
    "        storage_shape = list(self.storage_shape)\n",
    "        assert len(x_shape) == len(storage_shape)\n",
    "\n",
    "        for dim, (x, s) in enumerate(zip(x_shape, storage_shape)):\n",
    "            if dim != self.s_dim:\n",
    "                if x != s and x != 1:\n",
    "                    return False \n",
    "        return True\n",
    "\n",
    "    def push_back(self, x: torch.Tensor):\n",
    "        assert check_broadcastable(x)\n",
    "\n",
    "        push_len = x.shape[self.s_dim]\n",
    "        if self.rear + push_len > self.cap:\n",
    "            self.cap *= 2\n",
    "            self.storage_shape[self.s_dim] = self.cap\n",
    "            tmp = torch.zeros(self.storage_shape, dtype=self.dtype, device=self.device) \n",
    "            tmp.copy_(self.storage)\n",
    "            self.storage = tmp\n",
    "            \n",
    "        storage_slice = [None for _ in range(len(self.storage_shape))]\n",
    "        storage_slice[self.s_dim] = slice(self.rear, self.rear + push_len)\n",
    "        self.storage[*storage_slice].copy_(x)\n",
    "\n",
    "        self.rear += push_len \n",
    "\n",
    "    def pop_back(self):\n",
    "        storage_slice = [None for _ in range(len(self.storage_shape))]\n",
    "        storage_slice[self.s_dim] = slice(self.rear - 1, self.rear)\n",
    "        ret = self.storage[*storage_slice]\n",
    "        self.rear -= 1 \n",
    "        return ret\n",
    "\n",
    "    @property\n",
    "    def attn_mask(self):\n",
    "        # (b, s, src_len, tgt_len)\n",
    "        ...\n",
    "\n",
    "    def log_sum_exp(self, q: torch.Tensor):\n",
    "        # q: (b, n, s_q, h_d)\n",
    "        ... "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
