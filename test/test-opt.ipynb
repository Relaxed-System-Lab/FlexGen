{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, OPTForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class Policy:\n",
    "    gpu_batch_size: int\n",
    "    num_gpu_batches: int\n",
    "\n",
    "    # percent = a means a%\n",
    "    w_gpu_percent: float\n",
    "    w_cpu_percent: float\n",
    "    cache_gpu_percent: float\n",
    "    cache_cpu_percent: float\n",
    "    act_gpu_percent: float\n",
    "    act_cpu_percent: float\n",
    "\n",
    "    # Whether to overlap the I/O and compute\n",
    "    overlap: bool\n",
    "\n",
    "    # Whether to separate attention and mlp as two layers\n",
    "    # sep_layer: bool\n",
    "\n",
    "    # Whether to use pinned memory for weights on CPU\n",
    "    pin_weight: bool\n",
    "\n",
    "    # Whether to compute attention on CPU\n",
    "    # cpu_cache_compute: bool\n",
    "\n",
    "    # Sparsity of attention weights\n",
    "    # attn_sparsity: float\n",
    "\n",
    "    # Compress weights with group-wise quantization\n",
    "    # compress_weight: bool\n",
    "    # comp_weight_config: CompressionConfig\n",
    "\n",
    "    # Compress KV cache with group-wise quantization\n",
    "    # compress_cache: bool\n",
    "    # comp_cache_config: CompressionConfig\n",
    "\n",
    "    @property\n",
    "    def w_disk_percent(self):\n",
    "        return 100 - self.w_gpu_percent - self.w_cpu_percent\n",
    "\n",
    "    @property\n",
    "    def cache_disk_percent(self):\n",
    "        return 100 - self.cache_gpu_percent - self.cache_cpu_percent\n",
    "\n",
    "    @property\n",
    "    def act_disk_percent(self):\n",
    "        return 100 - self.act_gpu_percent - self.act_cpu_percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(\n",
    "    gpu_batch_size=4, \n",
    "    num_gpu_batches=12, \n",
    "    w_gpu_percent=20, \n",
    "    w_cpu_percent=30, \n",
    "    cache_gpu_percent=20, \n",
    "    cache_cpu_percent=30, \n",
    "    act_gpu_percent=0, \n",
    "    act_cpu_percent=100, \n",
    "    overlap=True,\n",
    "    pin_weight=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, <function __main__.model_pre_hook(module, args, kwargs)>)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_pre_hook(module, args, kwargs):\n",
    "    # print(args, kwargs)\n",
    "    display = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            display[k] = v.shape \n",
    "        elif k == 'past_key_values' and v is not None:\n",
    "            display[k] = f'{len(v)} * ' + str((v[0][0].shape, v[0][1].shape))\n",
    "        else:\n",
    "            display[k] = v\n",
    "    print(display)\n",
    "\n",
    "    return args, kwargs\n",
    "\n",
    "model._forward_pre_hooks.clear()\n",
    "model.register_forward_pre_hook(model_pre_hook, with_kwargs=True)\n",
    "model._forward_pre_hooks\n",
    "\n",
    "# def layer_pre_hook(module, args, kwargs):\n",
    "#     display = {}\n",
    "#     for k, v in kwargs.items():\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             display[k] = v.shape \n",
    "#         elif k in ['past_key_value', 'layer_past'] and v is not None:\n",
    "#             display[k] = str((v[0].shape, v[1].shape))\n",
    "#         else:\n",
    "#             display[k] = v\n",
    "#     print(display)\n",
    "\n",
    "#     return args, kwargs\n",
    "\n",
    "# layer = model.model.decoder.layers[2]\n",
    "\n",
    "# layer._forward_pre_hooks.clear()\n",
    "# layer.register_forward_pre_hook(layer_pre_hook, with_kwargs=True)\n",
    "# layer._forward_pre_hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([12, 20]), 'past_key_values': None, 'use_cache': True, 'position_ids': torch.Size([12, 20]), 'attention_mask': torch.Size([12, 20]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 20, 64]), torch.Size([12, 16, 20, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 21]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 21, 64]), torch.Size([12, 16, 21, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 22]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 22, 64]), torch.Size([12, 16, 22, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 23]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 23, 64]), torch.Size([12, 16, 23, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 24]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 24, 64]), torch.Size([12, 16, 24, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 25]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 25, 64]), torch.Size([12, 16, 25, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 26]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 26, 64]), torch.Size([12, 16, 26, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 27]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 27, 64]), torch.Size([12, 16, 27, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 28]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 28, 64]), torch.Size([12, 16, 28, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 29]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 29, 64]), torch.Size([12, 16, 29, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 30]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 30, 64]), torch.Size([12, 16, 30, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 31]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 31, 64]), torch.Size([12, 16, 31, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 32]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 32, 64]), torch.Size([12, 16, 32, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 33]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 33, 64]), torch.Size([12, 16, 33, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 34]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 34, 64]), torch.Size([12, 16, 34, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 35]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 35, 64]), torch.Size([12, 16, 35, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 36]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 36, 64]), torch.Size([12, 16, 36, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 37]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 37, 64]), torch.Size([12, 16, 37, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 38]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 38, 64]), torch.Size([12, 16, 38, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 39]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 39, 64]), torch.Size([12, 16, 39, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 40]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 40, 64]), torch.Size([12, 16, 40, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 41]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 41, 64]), torch.Size([12, 16, 41, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 42]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 42, 64]), torch.Size([12, 16, 42, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 43]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 43, 64]), torch.Size([12, 16, 43, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 44]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 44, 64]), torch.Size([12, 16, 44, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 45]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 45, 64]), torch.Size([12, 16, 45, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 46]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 46, 64]), torch.Size([12, 16, 46, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 47]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 47, 64]), torch.Size([12, 16, 47, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 48]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([12, 1]), 'past_key_values': '20 * (torch.Size([12, 16, 48, 64]), torch.Size([12, 16, 48, 64]))', 'use_cache': True, 'position_ids': torch.Size([12, 1]), 'attention_mask': torch.Size([12, 49]), 'token_type_ids': None, 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "Who are you? Are you conscious?#!/usr/bin/env python\n",
      "\n",
      "# Copyright (c) 2021 - present in the source code of this module\n",
      "\n",
      "import os\n",
      "\n",
      "----------\n",
      "Where is Deutschland?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "Who are you? Are you conscious?#!/usr/bin/env python\n",
      "\n",
      "# Copyright (c) 2021 - present in the source code of this module\n",
      "\n",
      "import os\n",
      "\n",
      "----------\n",
      "Where is Deutschland?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "Who are you? Are you conscious?#!/usr/bin/env python\n",
      "\n",
      "# Copyright (c) 2021 - present in the source code of this module\n",
      "\n",
      "import os\n",
      "\n",
      "----------\n",
      "Where is Deutschland?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "Who are you? Are you conscious?#!/usr/bin/env python\n",
      "\n",
      "# Copyright (c) 2021 - present in the source code of this module\n",
      "\n",
      "import os\n",
      "\n",
      "----------\n",
      "Where is Deutschland?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?#!/usr/bin/env python\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright (c)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'Who are you? Are you conscious?',\n",
    "    'Where is Deutschland?',\n",
    "    'How is Huawei Mate 60 Pro?'\n",
    "] * 4\n",
    "\n",
    "prompt_len = 20\n",
    "\n",
    "inputs = tokenizer(prompts, padding=\"max_length\", max_length=prompt_len, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=30 + prompt_len,\n",
    "    # num_beams=2,\n",
    "    # num_beam_groups=2,\n",
    "    # diversity_penalty=0.1,\n",
    "    # do_sample=True,\n",
    ")\n",
    "\n",
    "output_texts = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "for output_text in output_texts:\n",
    "    print(output_text)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
